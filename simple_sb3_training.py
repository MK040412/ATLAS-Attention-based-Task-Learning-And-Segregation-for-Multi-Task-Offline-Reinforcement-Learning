#!/usr/bin/env python3
"""
ê°€ì¥ ê°„ë‹¨í•œ SB3 í•™ìŠµ
"""

import argparse
import os
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser(description="Simple SB3 Training")
    parser.add_argument("--timesteps", type=int, default=10000)
    parser.add_argument("--save_path", type=str, default="./simple_sac_model")
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    try:
        # SB3 import
        from stable_baselines3 import SAC
        from stable_baselines3.common.env_checker import check_env
        from fixed_simple_gym_wrapper import SimpleAntPushWrapper

        print(f"ğŸš€ Simple SB3 Training")
        print(f"  Timesteps: {args.timesteps}")
        print(f"  Save path: {args.save_path}")
        
        # í™˜ê²½ ìƒì„±
        env = SimpleAntPushWrapper(custom_cfg={
            'cube_position': [2.0, 0.0, 0.1],
            'goal_position': [3.0, 0.0, 0.1]
        })
        
        # í™˜ê²½ ê²€ì¦
        print(f"ğŸ” Checking environment compatibility...")
        try:
            check_env(env)
            print(f"âœ… Environment is SB3 compatible!")
        except Exception as e:
            print(f"âš ï¸ Environment check warning: {e}")
        
        # SAC ì—ì´ì „íŠ¸ ìƒì„± (ìµœì†Œ ì„¤ì •)
        print(f"ğŸ¤– Creating SAC agent...")
        model = SAC(
            "MlpPolicy",
            env,
            learning_rate=3e-4,
            buffer_size=10000,      # ì‘ì€ ë²„í¼
            learning_starts=100,    # ë¹ ë¥¸ ì‹œì‘
            batch_size=64,          # ì‘ì€ ë°°ì¹˜
            tau=0.005,
            gamma=0.99,
            train_freq=1,
            gradient_steps=1,
            policy_kwargs=dict(
                net_arch=[64, 64]   # ì‘ì€ ë„¤íŠ¸ì›Œí¬
            ),
            verbose=1,
            device="cuda"
        )
        
        print(f"ğŸ“Š Model info:")
        print(f"  Policy: {model.policy}")
        print(f"  Device: {model.device}")
        print(f"  Buffer size: {model.buffer_size}")
        
        # í•™ìŠµ ì‹œì‘
        print(f"ğŸ“ Starting training...")
        model.learn(
            total_timesteps=args.timesteps,
            log_interval=10,
            progress_bar=True
        )
        
        # ëª¨ë¸ ì €ì¥
        model.save(args.save_path)
        print(f"ğŸ’¾ Model saved to: {args.save_path}")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print(f"ğŸ§ª Testing model...")
        obs, _ = env.reset()
        total_reward = 0
        
        for step in range(100):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env.step(action)
            total_reward += reward
            
            if step % 20 == 0:
                print(f"  Step {step}: reward={reward:.3f}, total={total_reward:.3f}")
            
            if done or truncated:
                print(f"  Episode finished at step {step}")
                break
        
        print(f"âœ… Test completed! Total reward: {total_reward:.2f}")
        
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            env.close()
        except:
            pass
        sim.close()

if __name__ == "__main__":
    main()