#!/usr/bin/env python3
"""
SB3 ê¸°ë°˜ KL-DPMM MoE SAC ë³‘ë ¬ í•™ìŠµ (DPMM ë¬¸ì œ í•´ê²° ë²„ì „)
"""

import argparse
import torch
import numpy as np
import yaml
import os
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser(description="SB3-based KL-DPMM MoE SAC Training")
    parser.add_argument("--tasks", type=str, default="tasks.yaml")
    parser.add_argument("--save_dir", type=str, default="./sb3_outputs")
    parser.add_argument("--num_envs", type=int, default=256)  # ë³‘ë ¬ í™˜ê²½
    parser.add_argument("--episodes", type=int, default=100)
    parser.add_argument("--tau_birth", type=float, default=1e-3)
    parser.add_argument("--tau_merge_kl", type=float, default=0.1)  # KL divergence ì„ê³„ê°’
    parser.add_argument("--latent_dim", type=int, default=16)
    parser.add_argument("--checkpoint_freq", type=int, default=20)
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    try:
        # SB3 ê¸°ë°˜ ëª¨ë“ˆë“¤ import
        from fixed_simple_gym_wrapper import SimpleAntPushWrapper  # SB3 í˜¸í™˜ í™˜ê²½
        from sb3_moe_agent import SB3MoEAgent
        from language_processor import LanguageProcessor
        from config import CONFIG

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(args.save_dir, exist_ok=True)

        # íƒœìŠ¤í¬ ë¡œë“œ (ê¸°ë³¸ íƒœìŠ¤í¬ ìƒì„±)
        if os.path.exists(args.tasks):
            tasks = yaml.safe_load(open(args.tasks))
        else:
            # ê¸°ë³¸ íƒœìŠ¤í¬ë“¤
            tasks = [
                {
                    'name': 'push_cube_basic',
                    'instruction': 'Push the blue cube to the goal position',
                    'env_cfg': {'cube_position': [2.0, 0.0, 0.1], 'goal_position': [3.0, 0.0, 0.1]}
                },
                {
                    'name': 'push_cube_far',
                    'instruction': 'Move the cube to the distant target',
                    'env_cfg': {'cube_position': [2.0, 0.0, 0.1], 'goal_position': [4.0, 0.0, 0.1]}
                },
                {
                    'name': 'push_cube_left',
                    'instruction': 'Push the cube to the left side',
                    'env_cfg': {'cube_position': [2.0, 0.0, 0.1], 'goal_position': [2.0, 2.0, 0.1]}
                }
            ]
        
        # ì–¸ì–´ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        lang_processor = LanguageProcessor(device=CONFIG.device)
        
        # í†µí•© ì—ì´ì „íŠ¸ (ì•„ì§ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ)
        agent = None
        # í•™ìŠµ í†µê³„
        all_training_stats = {
            'tasks': [],
            'episode_rewards': [],
            'cluster_counts': [],
            'expert_counts': [],
            'dpmm_events': []
        }
        print(f"ğŸš€ Starting SB3-based KL-DPMM MoE-SAC Training")
        print(f"  Tasks: {len(tasks)}")
        print(f"  Episodes per task: {args.episodes}")
        print(f"  KL merge threshold: {args.tau_merge_kl}")
        print(f"  Birth threshold: {args.tau_birth}")
        print(f"  Parallel environments: {args.num_envs}")

        # íƒœìŠ¤í¬ë³„ í•™ìŠµ
        for task_idx, task in enumerate(tasks):
            print(f"\n{'='*60}")
            print(f"ğŸ¯ Task {task_idx + 1}/{len(tasks)}: {task['name']}")
            print(f"ğŸ“ Instruction: {task['instruction']}")
            print(f"{'='*60}")
            
            # SB3 í˜¸í™˜ í™˜ê²½ ìƒì„± (ë‹¨ì¼ í™˜ê²½ìœ¼ë¡œ ì‹œì‘)
            env = SimpleAntPushWrapper(custom_cfg=task['env_cfg'])
            
            # ì°¨ì› í™•ì¸
            state_dim = env.observation_space.shape[0]
            action_dim = env.action_space.shape[0]
            
            print(f"ğŸ”§ Environment Setup:")
            print(f"  State dim: {state_dim}")
            print(f"  Action dim: {action_dim}")
            
            # ì§€ì‹œì‚¬í•­ ì„ë² ë”©
            instr_emb = lang_processor.encode(task['instruction'])
            instr_dim = instr_emb.shape[-1]
            
            print(f"  Instruction embedding dim: {instr_dim}")
            
            # ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (ì²« ë²ˆì§¸ íƒœìŠ¤í¬ì—ì„œë§Œ)
            if agent is None:
                agent = SB3MoEAgent(
                    env=env,
                    instr_dim=instr_dim,
                    latent_dim=args.latent_dim,
                    cluster_threshold=args.tau_merge_kl  # KL divergence ì„ê³„ê°’
                )
                
                # DPMM íŒŒë¼ë¯¸í„° ì„¤ì • (DPMM ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì•ˆì „í•œ ì„¤ì •)
                if hasattr(agent, 'set_dpmm_params'):
                    agent.set_dpmm_params(
                        tau_birth=args.tau_birth,
                        tau_merge=args.tau_merge_kl
                    )
                
                print(f"ğŸ¤– SB3 Agent initialized with KL-DPMM")
            
            # íƒœìŠ¤í¬ë³„ í•™ìŠµ í†µê³„
            task_stats = {
                'task_name': task['name'],
                'episode_rewards': [],
                'cluster_counts': [],
                'expert_counts': [],
                'dpmm_events': []
            }
            
            # ì—í”¼ì†Œë“œ í•™ìŠµ (DPMM ì•ˆì •ì„±ì„ ìœ„í•œ ë³´ìˆ˜ì  ì ‘ê·¼)
            for episode in range(args.episodes):
                if episode % 10 == 0:
                    print(f"\n--- Episode {episode + 1}/{args.episodes} ---")
                
                # í™˜ê²½ ë¦¬ì…‹
                state, _ = env.reset()
                episode_reward = 0
                step_count = 0
                max_steps = 500
                
                while step_count < max_steps:
                    try:
                        # í–‰ë™ ì„ íƒ
                        action, cluster_id = agent.select_action(
                            state, 
                            instr_emb.squeeze(0).detach().cpu().numpy(),
                            deterministic=False
                        )
                        
                        if step_count == 0 and episode % 10 == 0:
                            print(f"  Assigned to cluster/expert: {cluster_id}")
                        
                        # í™˜ê²½ ìŠ¤í…
                        next_state, reward, done, truncated, info = env.step(action)
                        episode_reward += reward
                        
                        # ê²½í—˜ ì €ì¥
                        agent.store_experience(state, action, reward, next_state, done, cluster_id)
                        
                        state = next_state
                        step_count += 1
                        
                        if done or truncated:
                            break
                            
                    except Exception as e:
                        print(f"  âš ï¸ Environment step error: {e}")
                        break
                
                # ì—í”¼ì†Œë“œ í›„ ì—…ë°ì´íŠ¸ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
                if episode % 10 == 0 and episode > 0:  # 10 ì—í”¼ì†Œë“œë§ˆë‹¤ ì—…ë°ì´íŠ¸
                    try:
                        update_info = agent.update_experts(batch_size=32)
                        if update_info and episode % 20 == 0:
                            print(f"  ğŸ“ˆ Updated experts: {list(update_info.keys())}")
                    except Exception as e:
                        print(f"  âš ï¸ Update error: {e}")
                
                # KL ê¸°ë°˜ ë³‘í•© (DPMM ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë” ì‹ ì¤‘í•˜ê²Œ)
                if episode % 20 == 0 and episode > 0:  # 20 ì—í”¼ì†Œë“œë§ˆë‹¤ ë³‘í•© ì‹œë„
                    try:
                        merged = safe_merge_clusters(agent, verbose=(episode % 40 == 0))
                        if merged:
                            task_stats['dpmm_events'].append({
                                'episode': episode,
                                'type': 'merge',
                                'info': merged
                            })
                    except Exception as e:
                        print(f"  âš ï¸ Merge error: {e}")
                
                # í†µê³„ ìˆ˜ì§‘ (ì•ˆì „í•˜ê²Œ)
                task_stats['episode_rewards'].append(episode_reward)
                
                try:
                    stats = agent.get_info()
                    task_stats['cluster_counts'].append(stats.get('dpmm', {}).get('num_clusters', 0))
                    task_stats['expert_counts'].append(stats.get('num_experts', 0))
                except:
                    task_stats['cluster_counts'].append(0)
                    task_stats['expert_counts'].append(0)
                
                # ì£¼ê¸°ì  ì¶œë ¥
                if episode % 20 == 0 and episode > 0:
                    try:
                        stats = agent.get_info()
                        recent_rewards = task_stats['episode_rewards'][-10:]
                        print(f"  ğŸ“Š Episode {episode + 1} Summary:")
                        print(f"    Recent avg reward: {np.mean(recent_rewards):.2f}")
                        print(f"    Episode reward: {episode_reward:.2f}")
                        print(f"    Steps: {step_count}")
                        print(f"    Clusters: {stats.get('dpmm', {}).get('num_clusters', 'N/A')}")
                        print(f"    Experts: {stats.get('num_experts', 'N/A')}")
                    except Exception as e:
                        print(f"  âš ï¸ Stats error: {e}")
            
            # íƒœìŠ¤í¬ ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if (task_idx + 1) % args.checkpoint_freq == 0:
                checkpoint_path = os.path.join(args.save_dir, f"sb3_checkpoint_task_{task_idx + 1}.pt")
                try:
                    agent.save(checkpoint_path)
                    print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
                except Exception as e:
                    print(f"âš ï¸ Checkpoint save error: {e}")
            
            # íƒœìŠ¤í¬ í†µê³„ ì €ì¥
            all_training_stats['tasks'].append(task_stats)
            all_training_stats['episode_rewards'].extend(task_stats['episode_rewards'])
            all_training_stats['cluster_counts'].extend(task_stats['cluster_counts'])
            all_training_stats['expert_counts'].extend(task_stats['expert_counts'])
            all_training_stats['dpmm_events'].extend(task_stats['dpmm_events'])
            
            # íƒœìŠ¤í¬ ìš”ì•½
            print(f"\nğŸ Task '{task['name']}' completed!")
            print(f"  Average reward: {np.mean(task_stats['episode_rewards']):.2f}")
            print(f"  Final clusters: {task_stats['cluster_counts'][-1] if task_stats['cluster_counts'] else 0}")
            print(f"  Final experts: {task_stats['expert_counts'][-1] if task_stats['expert_counts'] else 0}")
            print(f"  DPMM events: {len(task_stats['dpmm_events'])}")
            
            env.close()
        
        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        final_checkpoint_path = os.path.join(args.save_dir, "final_sb3_checkpoint.pt")
        try:
            agent.save(final_checkpoint_path)
            print(f"ğŸ’¾ Final checkpoint saved: {final_checkpoint_path}")
        except Exception as e:
            print(f"âš ï¸ Final checkpoint save error: {e}")
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        stats_path = os.path.join(args.save_dir, "sb3_training_stats.npz")
        try:
            np.savez(stats_path, **all_training_stats)
            print(f"ğŸ“Š Training stats saved: {stats_path}")
        except Exception as e:
            print(f"âš ï¸ Stats save error: {e}")
        
        # ìµœì¢… ìš”ì•½
        print(f"\nğŸ‰ All tasks completed!")
        print(f"  Total episodes: {len(all_training_stats['episode_rewards'])}")
        if all_training_stats['episode_rewards']:
            print(f"  Average reward: {np.mean(all_training_stats['episode_rewards']):.2f}")
        print(f"  Final clusters: {all_training_stats['cluster_counts'][-1] if all_training_stats['cluster_counts'] else 0}")
        print(f"  Final experts: {all_training_stats['expert_counts'][-1] if all_training_stats['expert_counts'] else 0}")
        print(f"  Total DPMM events: {len(all_training_stats['dpmm_events'])}")
        print(f"  Checkpoints saved to: {args.save_dir}")

    except Exception as e:
        print(f"âŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        sim.close()

def safe_merge_clusters(agent, verbose=False):
    """DPMM ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì•ˆì „í•œ í´ëŸ¬ìŠ¤í„° ë³‘í•©"""
    try:
        if hasattr(agent, 'merge_clusters'):
            return agent.merge_clusters(verbose=verbose)
        elif hasattr(agent, 'dpmm') and hasattr(agent.dpmm, 'merge'):
            agent.dpmm.merge(verbose=verbose)
            return True
        else:
            if verbose:
                print("  âš ï¸ No merge method available")
            return False
    except Exception as e:
        if verbose:
            print(f"  âš ï¸ Safe merge failed: {e}")
        return False

def create_default_tasks():
    """ê¸°ë³¸ íƒœìŠ¤í¬ ìƒì„±"""
    return [
        {
            'name': 'push_cube_basic',
            'instruction': 'Push the blue cube to the goal position',
            'env_cfg': {
                'cube_position': [2.0, 0.0, 0.1],
                'goal_position': [3.0, 0.0, 0.1]
            }
        },
        {
            'name': 'push_cube_far',
            'instruction': 'Move the cube to the distant target location',
            'env_cfg': {
                'cube_position': [2.0, 0.0, 0.1],
                'goal_position': [4.0, 0.0, 0.1]
            }
        },
        {
            'name': 'push_cube_left',
            'instruction': 'Push the cube to the left side target',
            'env_cfg': {
                'cube_position': [2.0, 0.0, 0.1],
                'goal_position': [2.0, 2.0, 0.1]
            }
        },
        {
            'name': 'push_cube_right',
            'instruction': 'Move the object to the right side goal',
            'env_cfg': {
                'cube_position': [2.0, 0.0, 0.1],
                'goal_position': [2.0, -2.0, 0.1]
            }
        }
    ]

if __name__ == "__main__":
    main()
