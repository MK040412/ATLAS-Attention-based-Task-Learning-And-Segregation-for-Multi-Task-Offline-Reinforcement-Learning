"""
UMAP ì‹œê°í™”ë¥¼ ìœ„í•œ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ë„êµ¬
"""

import numpy as np
import matplotlib.pyplot as plt
import umap
import seaborn as sns
from collections import defaultdict
import torch
import pickle
from typing import Dict, List, Tuple
import os

# ê¸°ì¡´ ëª¨ë“ˆ import
from Ant_CEMP import *

class ClusteringVisualizer:
    """í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì •ì„ ì‹œê°í™”í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, save_dir: str = "clustering_analysis"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # ì‹œê°í™” ë°ì´í„° ì €ì¥
        self.latent_snapshots = []  # [(step, latents, clusters, tasks)]
        self.clustering_history = []
        
        # UMAP ì„¤ì •
        self.umap_reducer = umap.UMAP(
            n_neighbors=15,
            min_dist=0.1,
            metric='euclidean',
            random_state=42
        )
        
        print(f"ğŸ¨ ClusteringVisualizer initialized, saving to: {save_dir}")
    
    def add_snapshot(self, step: int, latents: np.ndarray, clusters: np.ndarray, tasks: np.ndarray):
        """í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤ëƒ…ìƒ· ì¶”ê°€"""
        self.latent_snapshots.append({
            'step': step,
            'latents': latents.copy(),
            'clusters': clusters.copy(), 
            'tasks': tasks.copy(),
            'num_clusters': len(np.unique(clusters))
        })
        
        print(f"ğŸ“¸ Snapshot saved at step {step}: {len(latents)} points, {len(np.unique(clusters))} clusters")
    
    def create_umap_visualization(self, snapshot_indices: List[int] = None):
        """UMAPìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™”"""
        if not self.latent_snapshots:
            print("âŒ No snapshots available for visualization")
            return
        
        if snapshot_indices is None:
            # ì „ì²´ ìŠ¤ëƒ…ìƒ· ì¤‘ì—ì„œ ê· ë“±í•˜ê²Œ ì„ íƒ
            total_snapshots = len(self.latent_snapshots)
            snapshot_indices = [0, total_snapshots//3, 2*total_snapshots//3, total_snapshots-1]
            snapshot_indices = [i for i in snapshot_indices if i < total_snapshots]
        
        # ì„œë¸Œí”Œë¡¯ ì„¤ì •
        n_snapshots = len(snapshot_indices)
        fig, axes = plt.subplots(2, n_snapshots, figsize=(5*n_snapshots, 10))
        if n_snapshots == 1:
            axes = axes.reshape(2, 1)
        
        plt.suptitle('UMAP Clustering Visualization During Training', fontsize=16, fontweight='bold')
        
        for i, snapshot_idx in enumerate(snapshot_indices):
            snapshot = self.latent_snapshots[snapshot_idx]
            
            # UMAP ì°¨ì› ì¶•ì†Œ
            latents_2d = self.umap_reducer.fit_transform(snapshot['latents'])
            
            # ìƒë‹¨: í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ
            ax1 = axes[0, i]
            scatter1 = ax1.scatter(
                latents_2d[:, 0], latents_2d[:, 1], 
                c=snapshot['clusters'], 
                cmap='tab10', 
                alpha=0.7,
                s=50
            )
            ax1.set_title(f'Step {snapshot["step"]:,}\nClusters: {snapshot["num_clusters"]}')
            ax1.set_xlabel('UMAP 1')
            ax1.set_ylabel('UMAP 2')
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  í‘œì‹œ
            unique_clusters = np.unique(snapshot['clusters'])
            for cluster_id in unique_clusters:
                mask = snapshot['clusters'] == cluster_id
                if np.any(mask):
                    center = latents_2d[mask].mean(axis=0)
                    ax1.scatter(center[0], center[1], c='black', s=200, marker='x', linewidths=3)
                    ax1.annotate(f'C{cluster_id}', (center[0], center[1]), 
                               xytext=(5, 5), textcoords='offset points', fontweight='bold')
            
            # í•˜ë‹¨: íƒœìŠ¤í¬ë³„ ìƒ‰ìƒ
            ax2 = axes[1, i]
            scatter2 = ax2.scatter(
                latents_2d[:, 0], latents_2d[:, 1], 
                c=snapshot['tasks'], 
                cmap='viridis', 
                alpha=0.7,
                s=50
            )
            ax2.set_title(f'Tasks Distribution')
            ax2.set_xlabel('UMAP 1')
            ax2.set_ylabel('UMAP 2')
            
            # ì»¬ëŸ¬ë°” ì¶”ê°€
            if i == n_snapshots - 1:
                plt.colorbar(scatter1, ax=ax1, label='Cluster ID')
                plt.colorbar(scatter2, ax=ax2, label='Task ID')
        
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/umap_clustering_evolution.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_cluster_evolution_plot(self):
        """í´ëŸ¬ìŠ¤í„° ìˆ˜ ë³€í™” ì‹œê°í™”"""
        if not self.latent_snapshots:
            return
        
        steps = [s['step'] for s in self.latent_snapshots]
        num_clusters = [s['num_clusters'] for s in self.latent_snapshots]
        
        plt.figure(figsize=(12, 6))
        plt.plot(steps, num_clusters, 'o-', linewidth=2, markersize=8)
        plt.xlabel('Training Steps')
        plt.ylabel('Number of Clusters')
        plt.title('Cluster Count Evolution During Training')
        plt.grid(True, alpha=0.3)
        
        # ì£¼ìš” ë³€í™”ì  í‘œì‹œ
        for i in range(1, len(num_clusters)):
            if num_clusters[i] != num_clusters[i-1]:
                plt.axvline(x=steps[i], color='red', linestyle='--', alpha=0.5)
                plt.annotate(f'{num_clusters[i]}', 
                           (steps[i], num_clusters[i]), 
                           xytext=(10, 10), textcoords='offset points')
        
        plt.savefig(f'{self.save_dir}/cluster_evolution.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def create_task_cluster_heatmap(self):
        """íƒœìŠ¤í¬-í´ëŸ¬ìŠ¤í„° ê´€ê³„ íˆíŠ¸ë§µ"""
        if not self.latent_snapshots:
            return
        
        # ìµœì¢… ìŠ¤ëƒ…ìƒ· ì‚¬ìš©
        final_snapshot = self.latent_snapshots[-1]
        
        # íƒœìŠ¤í¬-í´ëŸ¬ìŠ¤í„° ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        unique_tasks = np.unique(final_snapshot['tasks'])
        unique_clusters = np.unique(final_snapshot['clusters'])
        
        matrix = np.zeros((len(unique_tasks), len(unique_clusters)))
        
        for i, task in enumerate(unique_tasks):
            for j, cluster in enumerate(unique_clusters):
                mask = (final_snapshot['tasks'] == task) & (final_snapshot['clusters'] == cluster)
                matrix[i, j] = np.sum(mask)
        
        # íˆíŠ¸ë§µ ìƒì„±
        plt.figure(figsize=(10, 6))
        sns.heatmap(matrix, 
                   xticklabels=[f'Cluster {c}' for c in unique_clusters],
                   yticklabels=[f'Task {t}' for t in unique_tasks],
                   annot=True, fmt='g', cmap='Blues')
        plt.title('Task-Cluster Assignment Matrix (Final State)')
        plt.xlabel('Clusters')
        plt.ylabel('Tasks')
        plt.tight_layout()
        plt.savefig(f'{self.save_dir}/task_cluster_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def save_data(self, filename: str = "clustering_data.pkl"):
        """ì‹œê°í™” ë°ì´í„° ì €ì¥"""
        filepath = os.path.join(self.save_dir, filename)
        with open(filepath, 'wb') as f:
            pickle.dump(self.latent_snapshots, f)
        print(f"ğŸ’¾ Data saved to: {filepath}")
    
    def load_data(self, filename: str = "clustering_data.pkl"):
        """ì‹œê°í™” ë°ì´í„° ë¡œë“œ"""
        filepath = os.path.join(self.save_dir, filename)
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                self.latent_snapshots = pickle.load(f)
            print(f"ğŸ“‚ Data loaded from: {filepath}")
        else:
            print(f"âŒ File not found: {filepath}")


class ModifiedTrainer(Trainer):
    """ì‹œê°í™” ê¸°ëŠ¥ì´ ì¶”ê°€ëœ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: Dict, visualizer: ClusteringVisualizer = None):
        super().__init__(config)
        self.visualizer = visualizer or ClusteringVisualizer()
        
        # ìŠ¤ëƒ…ìƒ· ì €ì¥ ê°„ê²©
        self.snapshot_freq = config.get('snapshot_freq', 2000)
        
        print(f"ğŸ“Š Modified trainer with visualization enabled")
    
    def _collect_latent_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ëª¨ë“  expert bufferì—ì„œ latent ë°ì´í„° ìˆ˜ì§‘"""
        all_latents = []
        all_clusters = []
        all_tasks = []
        
        for buffer in self.agent.expert_buffers:
            if buffer.size > 0:
                # ìµœê·¼ ë°ì´í„° ìƒ˜í”Œë§
                sample_size = min(buffer.size, 200)
                indices = np.random.choice(buffer.size, sample_size, replace=False)
                
                latents = buffer.latents[indices]
                tasks = buffer.task_ids[indices]
                
                # í´ëŸ¬ìŠ¤í„° í• ë‹¹
                clusters = []
                for latent in latents:
                    latent_tensor = torch.FloatTensor(latent)
                    cluster_id = self.agent.dpmm.infer_cluster(latent_tensor)
                    clusters.append(cluster_id)
                
                all_latents.extend(latents)
                all_clusters.extend(clusters)
                all_tasks.extend(tasks)
        
        return (np.array(all_latents), 
                np.array(all_clusters), 
                np.array(all_tasks))
    
    def train(self):
        """ì‹œê°í™” ê¸°ëŠ¥ì´ ì¶”ê°€ëœ í›ˆë ¨ ë£¨í”„"""
        step = 0
        episode_reward = 0
        episode_length = 0
        
        obs, _ = self.env.reset()
        
        print("ğŸš€ Starting training with visualization...")
        
        while step < self.config['total_steps']:
            # ê¸°ì¡´ í›ˆë ¨ ë¡œì§
            action, latent, cluster_id, expert_id = self.agent.select_action(obs, eval_mode=False)
            next_obs, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            
            self.agent.add_transition(
                obs, action, reward, next_obs, done, 
                latent, cluster_id, expert_id, self.env.current_task
            )
            
            episode_reward += reward
            episode_length += 1
            step += 1
            
            # Episode end
            if done:
                self.history['episode_rewards'].append(episode_reward)
                self.history['episode_lengths'].append(episode_length)
                self.task_performance_history[self.env.current_task].append(episode_reward)
                
                obs, _ = self.env.reset()
                episode_reward = 0
                episode_length = 0
            else:
                obs = next_obs
            
            # Training
            if step % self.config['train_freq'] == 0:
                losses = self.agent.train_step()
                
                if losses and step % 5000 == 0:
                    self._log_training_progress(step, losses)
            
            # DPMM update
            if step % self.config['dpmm_freq'] == 0:
                self._update_dpmm_statistics(step)
            
            # ìŠ¤ëƒ…ìƒ· ì €ì¥
            if step % self.snapshot_freq == 0 and step > 0:
                try:
                    latents, clusters, tasks = self._collect_latent_data()
                    if len(latents) > 0:
                        self.visualizer.add_snapshot(step, latents, clusters, tasks)
                except Exception as e:
                    print(f"âš ï¸ Snapshot collection failed at step {step}: {e}")
            
            # Evaluation
            if step % self.config['eval_freq'] == 0:
                self._comprehensive_evaluation(step)
        
        # ìµœì¢… ìŠ¤ëƒ…ìƒ·
        try:
            latents, clusters, tasks = self._collect_latent_data()
            if len(latents) > 0:
                self.visualizer.add_snapshot(step, latents, clusters, tasks)
        except Exception as e:
            print(f"âš ï¸ Final snapshot failed: {e}")
        
        print("âœ… Training completed!")
        self._final_evaluation()
        
        # ì‹œê°í™” ìƒì„±
        self.create_visualizations()
    
    def create_visualizations(self):
        """ëª¨ë“  ì‹œê°í™” ìƒì„±"""
        print("\nğŸ¨ Creating visualizations...")
        
        try:
            # 1. UMAP í´ëŸ¬ìŠ¤í„°ë§ ì§„í™”
            self.visualizer.create_umap_visualization()
            
            # 2. í´ëŸ¬ìŠ¤í„° ìˆ˜ ë³€í™”
            self.visualizer.create_cluster_evolution_plot()
            
            # 3. íƒœìŠ¤í¬-í´ëŸ¬ìŠ¤í„° íˆíŠ¸ë§µ
            self.visualizer.create_task_cluster_heatmap()
            
            # 4. ë°ì´í„° ì €ì¥
            self.visualizer.save_data()
            
            print("âœ… All visualizations created successfully!")
            
        except Exception as e:
            print(f"âŒ Visualization creation failed: {e}")
            import traceback
            traceback.print_exc()


def create_animated_clustering(visualizer: ClusteringVisualizer, output_file: str = "clustering_animation.gif"):
    """í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì •ì˜ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±"""
    import matplotlib.animation as animation
    
    if not visualizer.latent_snapshots:
        print("âŒ No snapshots available for animation")
        return
    
    # ëª¨ë“  latent ë°ì´í„°ë¥¼ í•©ì³ì„œ ì „ì²´ UMAP ê³µê°„ ìƒì„±
    all_latents = np.vstack([s['latents'] for s in visualizer.latent_snapshots])
    umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    all_latents_2d = umap_reducer.fit_transform(all_latents)
    
    # ê° ìŠ¤ëƒ…ìƒ·ì˜ ì‹œì‘ ì¸ë±ìŠ¤ ê³„ì‚°
    start_indices = [0]
    for snapshot in visualizer.latent_snapshots[:-1]:
        start_indices.append(start_indices[-1] + len(snapshot['latents']))
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    def animate(frame):
        if frame >= len(visualizer.latent_snapshots):
            return
        
        ax1.clear()
        ax2.clear()
        
        snapshot = visualizer.latent_snapshots[frame]
        start_idx = start_indices[frame]
        end_idx = start_idx + len(snapshot['latents'])
        
        latents_2d = all_latents_2d[start_idx:end_idx]
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ì‹œê°í™”
        scatter1 = ax1.scatter(
            latents_2d[:, 0], latents_2d[:, 1],
            c=snapshot['clusters'],
            cmap='tab10',
            alpha=0.7,
            s=50
        )
        ax1.set_title(f'Step {snapshot["step"]:,} - Clusters: {snapshot["num_clusters"]}')
        ax1.set_xlabel('UMAP 1')
        ax1.set_ylabel('UMAP 2')
        
        # íƒœìŠ¤í¬ë³„ ì‹œê°í™”
        scatter2 = ax2.scatter(
            latents_2d[:, 0], latents_2d[:, 1],
            c=snapshot['tasks'],
            cmap='viridis',
            alpha=0.7,
            s=50
        )
        ax2.set_title(f'Tasks Distribution')
        ax2.set_xlabel('UMAP 1')
        ax2.set_ylabel('UMAP 2')
        
        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  í‘œì‹œ
        unique_clusters = np.unique(snapshot['clusters'])
        for cluster_id in unique_clusters:
            mask = snapshot['clusters'] == cluster_id
            if np.any(mask):
                center = latents_2d[mask].mean(axis=0)
                ax1.scatter(center[0], center[1], c='black', s=200, marker='x', linewidths=3)
    
    anim = animation.FuncAnimation(
        fig, animate, frames=len(visualizer.latent_snapshots),
        interval=1000, repeat=True
    )
    
    # GIFë¡œ ì €ì¥
    output_path = os.path.join(visualizer.save_dir, output_file)
    anim.save(output_path, writer='pillow', fps=1)
    print(f"ğŸ¬ Animation saved to: {output_path}")
    
    plt.show()


def analyze_cluster_stability(visualizer: ClusteringVisualizer):
    """í´ëŸ¬ìŠ¤í„° ì•ˆì •ì„± ë¶„ì„"""
    if len(visualizer.latent_snapshots) < 2:
        print("âŒ Need at least 2 snapshots for stability analysis")
        return
    
    print("\nğŸ“Š Cluster Stability Analysis")
    print("=" * 50)
    
    stability_scores = []
    
    for i in range(1, len(visualizer.latent_snapshots)):
        prev_snapshot = visualizer.latent_snapshots[i-1]
        curr_snapshot = visualizer.latent_snapshots[i]
        
        # ë™ì¼í•œ latent pointsì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„° í• ë‹¹ ë¹„êµ (ê°„ë‹¨í™”ëœ ë²„ì „)
        stability = np.mean(prev_snapshot['clusters'] == curr_snapshot['clusters'][:len(prev_snapshot['clusters'])])
        stability_scores.append(stability)
        
        print(f"Step {prev_snapshot['step']} â†’ {curr_snapshot['step']}: {stability:.3f}")
    
    avg_stability = np.mean(stability_scores)
    print(f"\nAverage Stability: {avg_stability:.3f}")
    
    # ì•ˆì •ì„± ê·¸ë˜í”„
    plt.figure(figsize=(10, 6))
    steps = [visualizer.latent_snapshots[i]['step'] for i in range(1, len(visualizer.latent_snapshots))]
    plt.plot(steps, stability_scores, 'o-', linewidth=2, markersize=8)
    plt.xlabel('Training Steps')
    plt.ylabel('Cluster Stability')
    plt.title('Cluster Assignment Stability Over Time')
    plt.grid(True, alpha=0.3)
    plt.axhline(y=avg_stability, color='red', linestyle='--', label=f'Average: {avg_stability:.3f}')
    plt.legend()
    plt.savefig(f'{visualizer.save_dir}/cluster_stability.png', dpi=300, bbox_inches='tight')
    plt.show()


def main_with_visualization():
    """ì‹œê°í™”ê°€ í¬í•¨ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Lifelong RL with Clustering Visualization")
    print("=" * 80)
    
    # ì„¤ì •
    config = get_default_config()
    config['total_steps'] = 50000  # ğŸ”¥ 5ê°œ íƒœìŠ¤í¬ì´ë¯€ë¡œ ë” ê¸¸ê²Œ
    config['snapshot_freq'] = 5000  # ğŸ”¥ ìŠ¤ëƒ…ìƒ· ì£¼ê¸°
    config['task_count'] = 5        # ğŸ”¥ ëª…ì‹œì ìœ¼ë¡œ 5ê°œ íƒœìŠ¤í¬ ì„¤ì •
    
    print("ğŸ“‹ Configuration (with 5 tasks):")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # ì‹œê°í™”ê¸° ìƒì„±
    visualizer = ClusteringVisualizer(save_dir="clustering_results_5tasks")
    
    try:
        # ìˆ˜ì •ëœ íŠ¸ë ˆì´ë„ˆë¡œ í›ˆë ¨
        trainer = ModifiedTrainer(config, visualizer)
        trainer.train()
        
        print("\nâœ… Training completed successfully!")
        
        # ì¶”ê°€ ë¶„ì„
        print("\nğŸ” Additional Analysis...")
        
        # 1. ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        try:
            create_animated_clustering(visualizer)
        except Exception as e:
            print(f"âš ï¸ Animation creation failed: {e}")
        
        # 2. í´ëŸ¬ìŠ¤í„° ì•ˆì •ì„± ë¶„ì„
        try:
            analyze_cluster_stability(visualizer)
        except Exception as e:
            print(f"âš ï¸ Stability analysis failed: {e}")
        
        # 3. ìƒì„¸ í†µê³„
        print(f"\nğŸ“ˆ Final Statistics:")
        print(f"   Total snapshots: {len(visualizer.latent_snapshots)}")
        if visualizer.latent_snapshots:
            final_snapshot = visualizer.latent_snapshots[-1]
            print(f"   Final clusters: {final_snapshot['num_clusters']}")
            print(f"   Final data points: {len(final_snapshot['latents'])}")
        
        print(f"\nğŸ’¾ All results saved to: {visualizer.save_dir}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Training interrupted by user")
        # ì¤‘ê°„ ê²°ê³¼ë¼ë„ ì‹œê°í™”
        if len(visualizer.latent_snapshots) > 0:
            print("ğŸ¨ Creating visualizations with available data...")
            visualizer.create_umap_visualization()
            visualizer.create_cluster_evolution_plot()
            
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()


def load_and_visualize(data_path: str = "clustering_results/clustering_data.pkl"):
    """ì €ì¥ëœ ë°ì´í„°ë¡œë¶€í„° ì‹œê°í™” ìƒì„±"""
    print(f"ğŸ“‚ Loading data from: {data_path}")
    
    visualizer = ClusteringVisualizer()
    visualizer.load_data(data_path)
    
    if visualizer.latent_snapshots:
        print("ğŸ¨ Creating visualizations from loaded data...")
        visualizer.create_umap_visualization()
        visualizer.create_cluster_evolution_plot()
        visualizer.create_task_cluster_heatmap()
        
        # ì¶”ê°€ ë¶„ì„
        analyze_cluster_stability(visualizer)
        
        try:
            create_animated_clustering(visualizer)
        except Exception as e:
            print(f"âš ï¸ Animation creation failed: {e}")
    else:
        print("âŒ No data found in the file")


if __name__ == "__main__":
    # ì‹¤í–‰ ì˜µì…˜ ì„ íƒ
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "load":
        # ì €ì¥ëœ ë°ì´í„°ë¡œë¶€í„° ì‹œê°í™”
        data_path = sys.argv[2] if len(sys.argv) > 2 else "clustering_results/clustering_data.pkl"
        load_and_visualize(data_path)
    else:
        # ìƒˆë¡œìš´ í›ˆë ¨ + ì‹œê°í™”
        main_with_visualization()
