import argparse
import torch
import yaml
import numpy as np
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser(description="Ant MoE Agent Curriculum with Vision")
    parser.add_argument("--tasks", type=str, default="tasks.yaml")
    parser.add_argument("--save_dir", type=str, default="./outputs")
    parser.add_argument("--num_envs", type=int, default=4)  # ë””ë²„ê¹…ì„ ìœ„í•´ ì ê²Œ
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    from ant_push_env import IsaacAntPushEnv
    from config import CONFIG
    from language_processor import LanguageProcessor
    from corrected_fusion_encoder import FusionEncoder
    from ant_moe_agent import DPMMMoESACAgent

    # ì „ì—­ ë³€ìˆ˜ë“¤
    tasks = yaml.safe_load(open(args.tasks))
    lang = LanguageProcessor(device=CONFIG.device)
    agent = None
    fusion_encoder = None

    # ë©”ì¸ í•™ìŠµ ë£¨í”„
    for task_idx, task in enumerate(tasks):
        print(f"\n{'='*60}")
        print(f"Task {task_idx + 1}/{len(tasks)}: {task['name']}")
        print(f"Instruction: {task['instruction']}")
        print(f"{'='*60}")
        
        # í™˜ê²½ ìƒì„±
        env = IsaacAntPushEnv(custom_cfg=task['env_cfg'], num_envs=args.num_envs)
        
        # --- ë™ì  ìƒíƒœ ì°¨ì› ì²˜ë¦¬ ---
        # ì´ˆê¸° ë¦¬ì…‹ìœ¼ë¡œ ì‹¤ì œ ìƒíƒœ ë²¡í„° ì–»ê¸°
        state, _ = env.reset()
        
        # ìƒíƒœ ë° ì•¡ì…˜ ì°¨ì› í™•ì¸
        if state.ndim == 2:  # ë‹¤ì¤‘ í™˜ê²½
            actual_state_dim = state.shape[1]
            single_env_state = state[0]
        else:  # ë‹¨ì¼ í™˜ê²½
            actual_state_dim = state.shape[0]
            single_env_state = state
        
        action_space_shape = env.action_space.shape
        if len(action_space_shape) == 2:
            action_dim = action_space_shape[1]
        else:
            action_dim = action_space_shape[0]
        
        print(f"Actual state dim: {actual_state_dim}, Action dim: {action_dim}")
        print(f"State shape: {state.shape}")
        print(f"Action space shape: {action_space_shape}")
        
        # CONFIG ì—…ë°ì´íŠ¸
        if CONFIG.base_state_dim is None:
            CONFIG.base_state_dim = actual_state_dim
            print(f"Updated CONFIG.base_state_dim to {actual_state_dim}")
        
        # --- Fusion Encoder ì´ˆê¸°í™” (ë™ì  ìƒíƒœ ì°¨ì› ì‚¬ìš©) ---
        if fusion_encoder is None:
            fusion_encoder = FusionEncoder(
                instr_dim=CONFIG.instr_emb_dim,
                state_dim=actual_state_dim,  # ì‹¤ì œ ì¸¡ì •ëœ ìƒíƒœ ì°¨ì› ì‚¬ìš©
                latent_dim=CONFIG.latent_dim,
                hidden_dim=CONFIG.hidden_dim
            ).to(CONFIG.device)
            print(f"Initialized FusionEncoder with state_dim={actual_state_dim}")
        
        # --- ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ) ---
        if agent is None:
            agent = DPMMMoESACAgent(
                state_dim=actual_state_dim,
                action_dim=action_dim,
                instr_dim=CONFIG.instr_emb_dim,
                latent_dim=CONFIG.latent_dim,
                fusion_hidden_dim=CONFIG.hidden_dim
            )
            # ì—ì´ì „íŠ¸ì˜ fusion_encoderë¥¼ ìš°ë¦¬ê°€ ë§Œë“  ê²ƒìœ¼ë¡œ êµì²´
            agent.fusion_encoder = fusion_encoder
            print(f"Initialized DPMMMoESACAgent with updated FusionEncoder")
        
        # ì§€ì‹œì‚¬í•­ ì„ë² ë”©
        instr_emb = lang.encode(task['instruction'])
        print(f"Instruction embedding shape: {instr_emb.shape}")
        
        # ì—í”¼ì†Œë“œ ë£¨í”„
        for episode in range(task['episodes']):
            print(f"\n--- Episode {episode + 1}/{task['episodes']} ---")
            
            # í™˜ê²½ ë¦¬ì…‹
            state, _ = env.reset()
            done = False
            step_count = 0
            max_steps = 1000
            episode_reward = 0
            episode_experiences = []
            
            # ë¹„ì „ ê¸°ë°˜ ìƒíƒœ ê²€ì¦
            if step_count == 0:
                print(f"  Initial state shape: {state.shape}")
                if state.ndim == 2:
                    sample_state = state[0]
                else:
                    sample_state = state
                
                # ìƒíƒœ êµ¬ì„± ë¶„ì„
                base_dim = actual_state_dim - 6  # vision_cube(3) + goal(3) ì œì™¸
                proprioception = sample_state[:base_dim]
                vision_cube_pos = sample_state[base_dim:base_dim+3]
                goal_pos = sample_state[base_dim+3:base_dim+6]
                
                print(f"  State composition:")
                print(f"    Proprioception: {proprioception.shape} = {proprioception[:5]}...")
                print(f"    Vision cube pos: {vision_cube_pos}")
                print(f"    Goal pos: {goal_pos}")
            
            while not done and step_count < max_steps:
                # ë‹¤ì¤‘ í™˜ê²½ ì²˜ë¦¬
                if state.ndim == 2:
                    # ì²« ë²ˆì§¸ í™˜ê²½ë§Œ ì‚¬ìš© (ê°„ë‹¨í™”)
                    current_state = state[0]
                    
                    # í–‰ë™ ì„ íƒ
                    action, cluster_id, latent_state = agent.select_action(
                        current_state, instr_emb, deterministic=False
                    )
                    
                    # ëª¨ë“  í™˜ê²½ì— ê°™ì€ í–‰ë™ ì ìš©
                    full_action = np.tile(action, (args.num_envs, 1))
                    
                else:
                    current_state = state
                    action, cluster_id, latent_state = agent.select_action(
                        current_state, instr_emb, deterministic=False
                    )
                    full_action = action
                
                if step_count == 0:
                    print(f"  Action shape: {action.shape}")
                    print(f"  Full action shape: {full_action.shape}")
                    print(f"  Assigned to cluster: {cluster_id}")
                
                # í™˜ê²½ ìŠ¤í…
                next_state, reward, done_info, info = env.step(full_action)
                
                # ë³´ìƒ ë° ì¢…ë£Œ ì²˜ë¦¬
                if isinstance(reward, np.ndarray):
                    reward = reward[0]  # ì²« ë²ˆì§¸ í™˜ê²½ì˜ ë³´ìƒ
                if isinstance(done_info, np.ndarray):
                    done = done_info[0]  # ì²« ë²ˆì§¸ í™˜ê²½ì˜ ì¢…ë£Œ ìƒíƒœ
                else:
                    done = done_info
                
                episode_reward += reward
                
                # ê²½í—˜ ì €ì¥
                if next_state.ndim == 2:
                    next_state_single = next_state[0]
                else:
                    next_state_single = next_state
                
                episode_experiences.append({
                    'state': current_state,
                    'action': action,
                    'reward': reward,
                    'next_state': next_state_single,
                    'done': done,
                    'instruction_emb': instr_emb,
                    'cluster_id': cluster_id,
                    'latent_state': latent_state
                })
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state = next_state
                step_count += 1
                
                # ì£¼ê¸°ì  ì •ë³´ ì¶œë ¥ (ë¹„ì „ ì •ë³´ í¬í•¨)
                if step_count % 200 == 0:
                    # í˜„ì¬ ìƒíƒœì—ì„œ ë¹„ì „ ì •ë³´ ì¶”ì¶œ
                    if state.ndim == 2:
                        current_obs = state[0]
                    else:
                        current_obs = state
                    
                    base_dim = actual_state_dim - 6
                    vision_cube = current_obs[base_dim:base_dim+3]
                    goal_pos = current_obs[base_dim+3:base_dim+6]
                    cube_goal_dist = np.linalg.norm(vision_cube - goal_pos)
                    
                    print(f"    Step {step_count}: Reward={reward:.3f}, Total={episode_reward:.3f}")
                    print(f"    Vision cube pos: [{vision_cube[0]:.2f}, {vision_cube[1]:.2f}, {vision_cube[2]:.2f}]")
                    print(f"    Goal pos: [{goal_pos[0]:.2f}, {goal_pos[1]:.2f}, {goal_pos[2]:.2f}]")
                    print(f"    Cube-Goal distance: {cube_goal_dist:.3f}")
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ê²½í—˜ ì €ì¥
            for exp in episode_experiences:
                agent.store_experience(**exp)
            
            # ì „ë¬¸ê°€ ì—…ë°ì´íŠ¸
            if episode % CONFIG.update_frequency == 0:
                losses = agent.update_experts(batch_size=CONFIG.batch_size)
                if losses:
                    print(f"  Updated experts: {list(losses.keys())}")
                    # í‰ê·  ì†ì‹¤ ì¶œë ¥
                    avg_losses = {}
                    for expert_key, expert_losses in losses.items():
                        if expert_losses:
                            avg_losses[expert_key] = {
                                k: v for k, v in expert_losses.items() 
                                if isinstance(v, (int, float))
                            }
                    if avg_losses:
                        print(f"  Average losses: {avg_losses}")
            
            # í´ëŸ¬ìŠ¤í„° ë³‘í•©
            if episode % CONFIG.merge_frequency == 0:
                agent.merge_clusters(threshold=CONFIG.merge_threshold)
            
            print(f"Episode {episode + 1} Summary:")
            print(f"  Steps: {step_count}, Total Reward: {episode_reward:.3f}")
            print(f"  Average Reward: {episode_reward/step_count:.3f}")
            
            # ì—ì´ì „íŠ¸ ì •ë³´ ì¶œë ¥
            if episode % 10 == 0:
                agent_info = agent.get_info()
                print(f"  Agent Info: {agent_info['num_experts']} experts, "
                      f"{agent_info['cluster_info']['num_clusters']} clusters")
                print(f"  Buffer sizes: {agent_info['buffer_sizes']}")
        
        # íƒœìŠ¤í¬ ì™„ë£Œ ì •ë³´
        final_info = agent.get_info()
        print(f"\nTask '{task['name']}' completed!")
        print(f"Final state: {final_info['num_experts']} experts, "
              f"{final_info['cluster_info']['num_clusters']} clusters")
        print(f"Cluster counts: {final_info['cluster_info']['cluster_counts']}")
        
        env.close()

    sim.close()
    print("\nğŸ‰ Vision-enhanced training completed!")

if __name__ == "__main__":
    main()