import argparse
import torch
import yaml
import numpy as np
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser(description="Simplified Ant MoE Agent")
    parser.add_argument("--tasks", type=str, default="tasks.yaml")
    parser.add_argument("--save_dir", type=str, default="./outputs")
    parser.add_argument("--num_envs", type=int, default=1)  # ë‹¨ì¼ í™˜ê²½ìœ¼ë¡œ ì‹œì‘
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    from ant_push_env import IsaacAntPushEnv
    from config import CONFIG
    from language_processor import LanguageProcessor
    from simplified_moe_agent import SimplifiedMoEAgent

    # ì „ì—­ ë³€ìˆ˜ë“¤
    tasks = yaml.safe_load(open(args.tasks))
    lang = LanguageProcessor(device=CONFIG.device)
    agent = None

    # ë©”ì¸ í•™ìŠµ ë£¨í”„
    for task_idx, task in enumerate(tasks):
        print(f"\n{'='*60}")
        print(f"Task {task_idx + 1}/{len(tasks)}: {task['name']}")
        print(f"Instruction: {task['instruction']}")
        print(f"{'='*60}")
        
        # í™˜ê²½ ìƒì„±
        env = IsaacAntPushEnv(custom_cfg=task['env_cfg'], num_envs=args.num_envs)
        
        # ì´ˆê¸° ë¦¬ì…‹ìœ¼ë¡œ ìƒíƒœ ì°¨ì› í™•ì¸
        state, _ = env.reset()
        
        if state.ndim == 2:  # ë‹¤ì¤‘ í™˜ê²½
            actual_state_dim = state.shape[1]
            single_env_state = state[0]
        else:  # ë‹¨ì¼ í™˜ê²½
            actual_state_dim = state.shape[0]
            single_env_state = state
        
        action_space_shape = env.action_space.shape
        if len(action_space_shape) == 2:
            action_dim = action_space_shape[1]
        else:
            action_dim = action_space_shape[0]
        
        print(f"State dim: {actual_state_dim}, Action dim: {action_dim}")
        print(f"State shape: {state.shape}")
        
        # CONFIG ì—…ë°ì´íŠ¸
        if CONFIG.base_state_dim is None:
            CONFIG.base_state_dim = actual_state_dim
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)
        if agent is None:
            agent = SimplifiedMoEAgent(
                state_dim=actual_state_dim,
                action_dim=action_dim,
                instr_dim=CONFIG.instr_emb_dim,
                latent_dim=CONFIG.latent_dim,
                fusion_hidden_dim=CONFIG.hidden_dim
            )
            print(f"Initialized SimplifiedMoEAgent")
        
        # ì§€ì‹œì‚¬í•­ ì„ë² ë”©
        instr_emb = lang.encode(task['instruction'])
        print(f"Instruction embedding shape: {instr_emb.shape}")
        
        # ì—í”¼ì†Œë“œ ë£¨í”„
        for episode in range(task['episodes']):
            print(f"\n--- Episode {episode + 1}/{task['episodes']} ---")
            
            # í™˜ê²½ ë¦¬ì…‹
            state, _ = env.reset()
            done = False
            step_count = 0
            max_steps = 1000
            episode_reward = 0
            episode_experiences = []
            
            # ì´ˆê¸° ìƒíƒœ ì •ë³´
            if episode == 0:
                print(f"  Initial state shape: {state.shape}")
                if state.ndim == 2:
                    sample_state = state[0]
                else:
                    sample_state = state
                
                # ìƒíƒœ êµ¬ì„± ë¶„ì„
                base_dim = actual_state_dim - 6  # vision_cube(3) + goal(3) ì œì™¸
                proprioception = sample_state[:base_dim]
                vision_cube_pos = sample_state[base_dim:base_dim+3]
                goal_pos = sample_state[base_dim+3:base_dim+6]
                
                print(f"  State composition:")
                print(f"    Proprioception dim: {proprioception.shape[0]}")
                print(f"    Vision cube pos: {vision_cube_pos}")
                print(f"    Goal pos: {goal_pos}")
            
            while not done and step_count < max_steps:
                # ë‹¤ì¤‘ í™˜ê²½ ì²˜ë¦¬
                if state.ndim == 2:
                    current_state = state[0]  # ì²« ë²ˆì§¸ í™˜ê²½ë§Œ ì‚¬ìš©
                    
                    # í–‰ë™ ì„ íƒ
                    action, cluster_id, latent_state = agent.select_action(
                        current_state, instr_emb, deterministic=False
                    )
                    
                    # ëª¨ë“  í™˜ê²½ì— ê°™ì€ í–‰ë™ ì ìš©
                    full_action = np.tile(action, (args.num_envs, 1))
                else:
                    current_state = state
                    action, cluster_id, latent_state = agent.select_action(
                        current_state, instr_emb, deterministic=False
                    )
                    full_action = action
                
                if step_count == 0:
                    print(f"  Action shape: {action.shape}")
                    print(f"  Assigned to cluster: {cluster_id}")
                
                # í™˜ê²½ ìŠ¤í…
                try:
                    next_state, reward, done_info, info = env.step(full_action)
                except Exception as e:
                    print(f"Warning: Environment step failed: {e}")
                    break
                
                # ë³´ìƒ ë° ì¢…ë£Œ ì²˜ë¦¬
                if isinstance(reward, np.ndarray):
                    reward = reward[0] if len(reward) > 0 else 0.0
                elif torch.is_tensor(reward):
                    reward = reward.item() if reward.numel() == 1 else reward[0].item()
                else:
                    reward = float(reward)
                
                if isinstance(done_info, np.ndarray):
                    done = done_info[0] if len(done_info) > 0 else False
                elif torch.is_tensor(done_info):
                    done = done_info.item() if done_info.numel() == 1 else done_info[0].item()
                else:
                    done = bool(done_info)
                
                episode_reward += reward
                
                # ê²½í—˜ ì €ì¥
                if next_state.ndim == 2:
                    next_state_single = next_state[0]
                else:
                    next_state_single = next_state
                
                episode_experiences.append({
                    'state': current_state,
                    'action': action,
                    'reward': reward,
                    'next_state': next_state_single,
                    'done': done,
                    'instruction_emb': instr_emb,
                    'cluster_id': cluster_id,
                    'latent_state': latent_state
                })
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state = next_state
                step_count += 1
                
                # ì£¼ê¸°ì  ì •ë³´ ì¶œë ¥
                if step_count % 200 == 0:
                    # í˜„ì¬ ìƒíƒœì—ì„œ ì •ë³´ ì¶”ì¶œ
                    if state.ndim == 2:
                        current_obs = state[0]
                    else:
                        current_obs = state
                    
                    base_dim = actual_state_dim - 6
                    vision_cube = current_obs[base_dim:base_dim+3]
                    goal_pos = current_obs[base_dim+3:base_dim+6]
                    cube_goal_dist = np.linalg.norm(vision_cube - goal_pos)
                    
                    print(f"    Step {step_count}: Reward={reward:.3f}, Total={episode_reward:.3f}")
                    print(f"    Vision cube pos: [{vision_cube[0]:.2f}, {vision_cube[1]:.2f}, {vision_cube[2]:.2f}]")
                    print(f"    Goal pos: [{goal_pos[0]:.2f}, {goal_pos[1]:.2f}, {goal_pos[2]:.2f}]")
                    print(f"    Cube-Goal distance: {cube_goal_dist:.3f}")
                    print(f"    Current cluster: {cluster_id}")
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ê²½í—˜ ì €ì¥
            for exp in episode_experiences:
                agent.store_experience(**exp)
            
            # ì „ë¬¸ê°€ ì—…ë°ì´íŠ¸
            if episode % CONFIG.update_frequency == 0 and episode > 0:
                losses = agent.update_experts(batch_size=CONFIG.batch_size)
                if losses:
                    print(f"  Updated experts: {list(losses.keys())}")
                    # í‰ê·  ì†ì‹¤ ì¶œë ¥
                    valid_losses = {k: v for k, v in losses.items() if v is not None}
                    if valid_losses:
                        print(f"  Loss summary: {len(valid_losses)} experts updated")
            
            # í´ëŸ¬ìŠ¤í„° ë³‘í•©
            if episode % CONFIG.merge_frequency == 0 and episode > 0:
                agent.merge_clusters(threshold=CONFIG.merge_threshold)
            
            print(f"Episode {episode + 1} Summary:")
            print(f"  Steps: {step_count}, Total Reward: {episode_reward:.3f}")
            if step_count > 0:
                print(f"  Average Reward: {episode_reward/step_count:.3f}")
            
            # ì—ì´ì „íŠ¸ ì •ë³´ ì¶œë ¥
            if episode % 10 == 0:
                agent_info = agent.get_info()
                print(f"  Agent Info: {agent_info['num_experts']} experts, "
                      f"{agent_info['cluster_info']['num_clusters']} clusters")
                if agent_info['buffer_sizes']:
                    total_experiences = sum(agent_info['buffer_sizes'])
                    print(f"  Total experiences: {total_experiences}")
        
        # íƒœìŠ¤í¬ ì™„ë£Œ ì •ë³´
        final_info = agent.get_info()
        print(f"\nTask '{task['name']}' completed!")
        print(f"Final state: {final_info['num_experts']} experts, "
              f"{final_info['cluster_info']['num_clusters']} clusters")
        if final_info['cluster_info']['cluster_counts']:
            print(f"Cluster counts: {final_info['cluster_info']['cluster_counts']}")
        
        # ëª¨ë¸ ì €ì¥
        save_path = f"{args.save_dir}/agent_after_task_{task_idx}.pt"
        try:
            agent.save(save_path)
            print(f"Agent saved to {save_path}")
        except Exception as e:
            print(f"Warning: Failed to save agent: {e}")
        
        env.close()

    sim.close()
    print("\nğŸ‰ Simplified training completed!")

if __name__ == "__main__":
    main()