import argparse
import torch
import numpy as np
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_envs", type=int, default=1)
    parser.add_argument("--episodes", type=int, default=3)
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    try:
        from ant_push_env_with_glip import IsaacAntPushEnvWithGLIP
        from integrated_moe_agent import IntegratedMoEAgent
        from language_processor import LanguageProcessor
        from config import CONFIG
        
        print("ğŸ§ª Step 4 Test: Integrated MoE Agent")
        
        # ì–¸ì–´ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        lang_processor = LanguageProcessor(device=CONFIG.device)
        
        # í™˜ê²½ ìƒì„±
        env = IsaacAntPushEnvWithGLIP(
            custom_cfg={
                'cube_position': [2.0, 0.0, 0.1],
                'goal_position': [3.0, 0.0, 0.1]
            },
            num_envs=1
        )
        
        # ì´ˆê¸° ë¦¬ì…‹ìœ¼ë¡œ ìƒíƒœ ì°¨ì› í™•ì¸
        state, _ = env.reset()
        state_dim = state.shape[0]
        action_dim = env.get_action_dim()
        
        print(f"ğŸ“Š Environment Info:")
        print(f"  State dim: {state_dim}")
        print(f"  Action dim: {action_dim}")
        print(f"  Instruction embedding dim: {CONFIG.instr_emb_dim}")
        
        # MoE ì—ì´ì „íŠ¸ ìƒì„±
        agent = IntegratedMoEAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            instr_dim=CONFIG.instr_emb_dim,
            latent_dim=CONFIG.latent_dim,
            hidden_dim=CONFIG.hidden_dim
        )
        
        # í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´ë“¤
        test_instructions = [
            "Push the blue cube to the goal position",
            "Move the cube towards the target",
            "Navigate to the blue object and push it"
        ]
        
        print(f"\nğŸ¯ Testing with {len(test_instructions)} different instructions:")
        
        for episode in range(args.episodes):
            instruction = test_instructions[episode % len(test_instructions)]
            print(f"\n=== Episode {episode + 1}: '{instruction}' ===")
            
            # ëª…ë ¹ì–´ ì„ë² ë”©
            instr_emb = lang_processor.encode(instruction)
            print(f"  Instruction embedding shape: {instr_emb.shape}")
            
            # ì—í”¼ì†Œë“œ ì‹¤í–‰
            state, _ = env.reset()
            total_reward = 0
            steps = 0
            max_steps = 200
            
            cluster_history = []
            
            while steps < max_steps:
                # í–‰ë™ ì„ íƒ
                action, cluster_id = agent.select_action(
                    state=state,
                    instruction_emb=instr_emb.squeeze(0).cpu().numpy(),
                    deterministic=False
                )
                
                cluster_history.append(cluster_id)
                
                # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
                next_state, reward, done, info = env.step(action)
                total_reward += reward
                
                # ê²½í—˜ ì €ì¥
                agent.store_experience(
                    state=state,
                    action=action,
                    reward=reward,
                    next_state=next_state,
                    done=done,
                    instruction_emb=instr_emb.squeeze(0).cpu().numpy(),
                    cluster_id=cluster_id
                )
                
                # ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸ (ë§¤ 10ìŠ¤í…ë§ˆë‹¤)
                if steps % 10 == 0 and steps > 0:
                    update_results = agent.update_experts(batch_size=32)
                    if update_results and steps % 50 == 0:
                        print(f"    Step {steps}: Updated {len(update_results)} experts")
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if steps % 50 == 0:
                    ant_pos = next_state[:3]
                    cube_pos = next_state[-6:-3]
                    goal_pos = next_state[-3:]
                    dist_to_goal = np.linalg.norm(cube_pos - goal_pos)
                    
                    print(f"    Step {steps}: Reward={reward:.3f}, Total={total_reward:.1f}")
                    print(f"      Cluster: {cluster_id}, Dist to goal: {dist_to_goal:.3f}")
                    print(f"      Ant: {ant_pos}, Cube: {cube_pos}")
                
                state = next_state
                steps += 1
                
                if done:
                    break
            
            # ì—í”¼ì†Œë“œ í†µê³„
            unique_clusters = len(set(cluster_history))
            most_used_cluster = max(set(cluster_history), key=cluster_history.count)
            
            print(f"  Episode Summary:")
            print(f"    Steps: {steps}, Total Reward: {total_reward:.2f}")
            print(f"    Unique clusters used: {unique_clusters}")
            print(f"    Most used cluster: {most_used_cluster}")
            
            agent.episode_count += 1
        
        # ìµœì¢… í†µê³„
        final_stats = agent.get_statistics()
        print(f"\nğŸ“ˆ Final Statistics:")
        print(f"  Total experts created: {final_stats['num_experts']}")
        print(f"  Total clusters: {final_stats['num_clusters']}")
        print(f"  Cluster counts: {final_stats['cluster_counts']}")
        print(f"  Expert buffer sizes: {final_stats['expert_buffer_sizes']}")
        print(f"  Total steps: {final_stats['total_steps']}")
        print(f"  Total episodes: {final_stats['total_episodes']}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í…ŒìŠ¤íŠ¸
        checkpoint_path = "./test_checkpoint.pth"
        agent.save_checkpoint(checkpoint_path)
        
        print("âœ… Step 4 test passed!")
        env.close()
        
    except Exception as e:
        print(f"âŒ Step 4 test failed: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        sim.close()

if __name__ == "__main__":
    main()