import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple
from collections import defaultdict, deque
import itertools
from scipy.stats import multivariate_normal
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings("ignore")

class RTX4060OptimizedFusionEncoder(nn.Module):
    """RTX 4060 ë©”ëª¨ë¦¬ ìµœì í™”ëœ Fusion Encoder"""
    
    def __init__(self, instr_dim: int, state_dim: int, latent_dim: int = 32):
        super().__init__()
        self.instr_dim = instr_dim
        self.state_dim = state_dim
        self.latent_dim = latent_dim
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‘ì€ ë„¤íŠ¸ì›Œí¬
        input_dim = instr_dim + state_dim
        hidden_dim = 128  # 256ì—ì„œ 128ë¡œ ì¶•ì†Œ
        
        self.fusion_net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(inplace=True),  # inplaceë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim // 2, latent_dim)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        for layer in self.fusion_net:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight, gain=0.5)  # ì‘ì€ ì´ˆê¸°ê°’
                nn.init.zeros_(layer.bias)
    
    def forward(self, instr_emb: torch.Tensor, state: torch.Tensor) -> torch.Tensor:
        # ì°¨ì› ì •ë¦¬
        if instr_emb.dim() > 2:
            instr_emb = instr_emb.mean(dim=-2)  # ì‹œí€€ìŠ¤ ì°¨ì› í‰ê· 
        if instr_emb.dim() == 2 and instr_emb.size(0) == 1:
            instr_emb = instr_emb.squeeze(0)
            
        if state.dim() == 2 and state.size(0) == 1:
            state = state.squeeze(0)
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if instr_emb.dim() == 1:
            instr_emb = instr_emb.unsqueeze(0)
        if state.dim() == 1:
            state = state.unsqueeze(0)
            
        # ìœµí•©
        combined = torch.cat([instr_emb, state], dim=-1)
        z = self.fusion_net(combined)
        
        return z.squeeze(0) if z.size(0) == 1 else z

class KLDivergenceDPMM:
    """KL Divergence ê¸°ë°˜ DPMM - ë©”ëª¨ë¦¬ ìµœì í™”"""
    
    def __init__(self, latent_dim: int = 32, alpha: float = 1.0, 
                 tau_birth: float = 1e-3, tau_merge_kl: float = 0.1):
        self.latent_dim = latent_dim
        self.alpha = alpha
        self.tau_birth = tau_birth
        self.tau_merge_kl = tau_merge_kl
        
        self.clusters = []
        self.cluster_counts = []
        self.total_points = 0
        self.total_births = 0
        self.total_merges = 0
        
    def _create_cluster(self, z: np.ndarray) -> int:
        """ìƒˆ í´ëŸ¬ìŠ¤í„° ìƒì„±"""
        cluster_id = len(self.clusters)
        
        # ì´ˆê¸° ê³µë¶„ì‚°ì€ ë‹¨ìœ„ í–‰ë ¬ì˜ ì‘ì€ ë°°ìˆ˜
        initial_cov = np.eye(self.latent_dim) * 0.1
        
        cluster = {
            'mu': z.copy(),
            'cov': initial_cov,
            'count': 1,
            'sum_z': z.copy(),
            'sum_zz': np.outer(z, z)
        }
        
        self.clusters.append(cluster)
        self.cluster_counts.append(1)
        self.total_births += 1
        
        return cluster_id
    
    def _update_cluster(self, cluster_id: int, z: np.ndarray):
        """í´ëŸ¬ìŠ¤í„° í†µê³„ ì—…ë°ì´íŠ¸"""
        cluster = self.clusters[cluster_id]
        
        # ì˜¨ë¼ì¸ ì—…ë°ì´íŠ¸
        cluster['count'] += 1
        cluster['sum_z'] += z
        cluster['sum_zz'] += np.outer(z, z)
        
        # í‰ê·  ì—…ë°ì´íŠ¸
        cluster['mu'] = cluster['sum_z'] / cluster['count']
        
        # ê³µë¶„ì‚° ì—…ë°ì´íŠ¸ (ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê³ ë ¤)
        if cluster['count'] > 1:
            mean_outer = np.outer(cluster['mu'], cluster['mu'])
            cov = (cluster['sum_zz'] / cluster['count']) - mean_outer
            
            # ì •ê·œí™” í•­ ì¶”ê°€ (ìˆ˜ì¹˜ì  ì•ˆì •ì„±)
            cov += np.eye(self.latent_dim) * 1e-6
            cluster['cov'] = cov
        
        self.cluster_counts[cluster_id] = cluster['count']
    
    def _compute_log_likelihood(self, z: np.ndarray, cluster: dict) -> float:
        """ê°€ìš°ì‹œì•ˆ ë¡œê·¸ ìš°ë„ ê³„ì‚°"""
        try:
            return multivariate_normal.logpdf(z, cluster['mu'], cluster['cov'])
        except:
            # ìˆ˜ì¹˜ì  ë¬¸ì œ ì‹œ ë‚®ì€ ìš°ë„ ë°˜í™˜
            return -1e6
    
    def assign_and_update(self, z: np.ndarray) -> int:
        """í´ëŸ¬ìŠ¤í„° í• ë‹¹ ë° ì—…ë°ì´íŠ¸"""
        z = np.array(z, dtype=np.float32)
        self.total_points += 1
        
        if len(self.clusters) == 0:
            return self._create_cluster(z)
        
        # ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ë¡œê·¸ ìš°ë„ ê³„ì‚°
        log_likelihoods = []
        for cluster in self.clusters:
            ll = self._compute_log_likelihood(z, cluster)
            log_likelihoods.append(ll)
        
        max_log_likelihood = max(log_likelihoods)
        
        # Birth criterion: ìµœëŒ€ ìš°ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ìƒˆ í´ëŸ¬ìŠ¤í„° ìƒì„±
        if max_log_likelihood < np.log(self.tau_birth):
            return self._create_cluster(z)
        
        # ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹
        best_cluster = np.argmax(log_likelihoods)
        self._update_cluster(best_cluster, z)
        
        return best_cluster
    
    def _symmetric_kl_divergence(self, mu1, cov1, mu2, cov2) -> float:
        """ëŒ€ì¹­ KL divergence ê³„ì‚°"""
        try:
            # KL(P||Q) + KL(Q||P) / 2
            inv_cov1 = np.linalg.inv(cov1)
            inv_cov2 = np.linalg.inv(cov2)
            
            diff = mu1 - mu2
            
            # KL(P||Q)
            kl_pq = 0.5 * (
                np.trace(inv_cov2 @ cov1) + 
                diff.T @ inv_cov2 @ diff - 
                self.latent_dim + 
                np.log(np.linalg.det(cov2) / np.linalg.det(cov1))
            )
            
            # KL(Q||P)  
            kl_qp = 0.5 * (
                np.trace(inv_cov1 @ cov2) + 
                diff.T @ inv_cov1 @ diff - 
                self.latent_dim + 
                np.log(np.linalg.det(cov1) / np.linalg.det(cov2))
            )
            
            return (kl_pq + kl_qp) / 2
        except:
            return np.inf
    
    def merge_clusters(self, verbose: bool = False) -> bool:
        """KL divergence ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ë³‘í•©"""
        if len(self.clusters) < 2:
            return False
        
        min_kl = np.inf
        merge_pair = None
        
        # ëª¨ë“  í´ëŸ¬ìŠ¤í„° ìŒì— ëŒ€í•´ KL divergence ê³„ì‚°
        for i, j in itertools.combinations(range(len(self.clusters)), 2):
            kl_div = self._symmetric_kl_divergence(
                self.clusters[i]['mu'], self.clusters[i]['cov'],
                self.clusters[j]['mu'], self.clusters[j]['cov']
            )
            
            if kl_div < min_kl:
                min_kl = kl_div
                merge_pair = (i, j)
        
        # ë³‘í•© ì¡°ê±´ í™•ì¸
        if min_kl < self.tau_merge_kl:
            i, j = merge_pair
            if verbose:
                print(f"  Merging clusters {i} and {j} (KL divergence: {min_kl:.4f})")
            
            self._merge_clusters(i, j)
            self.total_merges += 1
            return True
        
        return False
    
    def _merge_clusters(self, i: int, j: int):
        """ë‘ í´ëŸ¬ìŠ¤í„° ë³‘í•©"""
        # jë¥¼ ië¡œ ë³‘í•© (i < j ê°€ì •)
        if i > j:
            i, j = j, i
        
        cluster_i = self.clusters[i]
        cluster_j = self.clusters[j]
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë³‘í•©
        total_count = cluster_i['count'] + cluster_j['count']
        
        # ìƒˆë¡œìš´ í‰ê· 
        new_mu = (cluster_i['count'] * cluster_i['mu'] + 
                  cluster_j['count'] * cluster_j['mu']) / total_count
        
        # ìƒˆë¡œìš´ ê³µë¶„ì‚° (í’€ë§ëœ ê³µë¶„ì‚°)
        new_cov = ((cluster_i['count'] - 1) * cluster_i['cov'] + 
                   (cluster_j['count'] - 1) * cluster_j['cov']) / (total_count - 2)
        
        # í´ëŸ¬ìŠ¤í„° i ì—…ë°ì´íŠ¸
        cluster_i['mu'] = new_mu
        cluster_i['cov'] = new_cov
        cluster_i['count'] = total_count
        cluster_i['sum_z'] = cluster_i['sum_z'] + cluster_j['sum_z']
        cluster_i['sum_zz'] = cluster_i['sum_zz'] + cluster_j['sum_zz']
        
        # í´ëŸ¬ìŠ¤í„° j ì œê±°
        del self.clusters[j]
        del self.cluster_counts[j]
    
    def get_statistics(self) -> Dict:
        """DPMM í†µê³„ ë°˜í™˜"""
        return {
            'num_clusters': len(self.clusters),
            'cluster_counts': self.cluster_counts.copy(),
            'total_points': self.total_points,
            'total_births': self.total_births,
            'total_merges': self.total_merges,
            'avg_cluster_size': np.mean(self.cluster_counts) if self.cluster_counts else 0
        }

class RTX4060SACExpert:
    """RTX 4060 ìµœì í™”ëœ SAC Expert"""
    
    def __init__(self, state_dim: int, action_dim: int, expert_id: int):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.expert_id = expert_id
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‘ì€ ë„¤íŠ¸ì›Œí¬
        hidden_dim = 128  # 256ì—ì„œ 128ë¡œ ì¶•ì†Œ
        
        # Actor network
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, action_dim * 2)  # mean + log_std
        ).to(self.device)
        
        # Twin critic networks (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì‘ê²Œ)
        self.critic1 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1)
        ).to(self.device)
        
        self.critic2 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1)
        ).to(self.device)
        
        # Target networks
        self.target_critic1 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1)
        ).to(self.device)
        
        self.target_critic2 = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1)
        ).to(self.device)
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())
        
        # Entropy coefficient
        self.log_alpha = nn.Parameter(torch.zeros(1, device=self.device))
        self.target_entropy = -action_dim  # SAC ê¸°ë³¸ê°’
        
        # Optimizers (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)
        self.critic1_optimizer = torch.optim.Adam(self.critic1.parameters(), lr=1e-4)
        self.critic2_optimizer = torch.optim.Adam(self.critic2.parameters(), lr=1e-4)
        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=1e-4)
        
        # ì‘ì€ replay buffer (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.replay_buffer = deque(maxlen=5000)
        self.tau = 0.005  # soft update coefficient
        
    @property
    def alpha(self):
        return self.log_alpha.exp()
    
    def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:
        """í–‰ë™ ì„ íƒ"""
        with torch.no_grad():
            if not torch.is_tensor(state):
                state = torch.FloatTensor(state).to(self.device)
            
            if state.dim() == 1:
                state = state.unsqueeze(0)
            
            # Actor forward pass
            actor_output = self.actor(state)
            mean, log_std = actor_output.chunk(2, dim=-1)
            
            # ì•ˆì •ì„±ì„ ìœ„í•œ í´ë¦¬í•‘
            log_std = torch.clamp(log_std, -20, 2)
            std = log_std.exp()
            
            if deterministic:
                action = torch.tanh(mean)
            else:
                # ì¬ë§¤ê°œí™” íŠ¸ë¦­
                normal = torch.distributions.Normal(mean, std)
                x_t = normal.rsample()
                action = torch.tanh(x_t)
            
            return action.squeeze(0).cpu().numpy()
    
    def store_experience(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        experience = {
            'state': np.array(state, dtype=np.float32),
            'action': np.array(action, dtype=np.float32),
            'reward': float(reward),
            'next_state': np.array(next_state, dtype=np.float32),
            'done': bool(done)
        }
        self.replay_buffer.append(experience)
    
    def sample_batch(self, batch_size: int = 32):
        """ë°°ì¹˜ ìƒ˜í”Œë§"""
        if len(self.replay_buffer) < batch_size:
            return None
        
        import random
        batch = random.sample(self.replay_buffer, batch_size)
        
        states = torch.FloatTensor([e['state'] for e in batch]).to(self.device)
        actions = torch.FloatTensor([e['action'] for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e['reward'] for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e['next_state'] for e in batch]).to(self.device)
        dones = torch.BoolTensor([e['done'] for e in batch]).to(self.device)
        
        return states, actions, rewards, next_states, dones
    
    def update(self, batch_size: int = 32) -> Optional[Dict]:
        """SAC ì—…ë°ì´íŠ¸"""
        batch = self.sample_batch(batch_size)
        if batch is None:
            return None
        
        states, actions, rewards, next_states, dones = batch
        
        # Critic ì—…ë°ì´íŠ¸
        with torch.no_grad():
            # ë‹¤ìŒ ìƒíƒœì—ì„œì˜ í–‰ë™ ìƒ˜í”Œë§
            next_actor_output = self.actor(next_states)
            next_mean, next_log_std = next_actor_output.chunk(2, dim=-1)
            next_log_std = torch.clamp(next_log_std, -20, 2)
            next_std = next_log_std.exp()
            
            next_normal = torch.distributions.Normal(next_mean, next_std)
            next_x_t = next_normal.rsample()
            next_actions = torch.tanh(next_x_t)
            next_log_probs = next_normal.log_prob(next_x_t) - torch.log(1 - next_actions.pow(2) + 1e-6)
            next_log_probs = next_log_probs.sum(dim=-1, keepdim=True)
            
            # íƒ€ê²Ÿ Q ê°’
            target_q1 = self.target_critic1(torch.cat([next_states, next_actions], dim=-1))
            target_q2 = self.target_critic2(torch.cat([next_states, next_actions], dim=-1))
            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
            
            target_q = rewards.unsqueeze(-1) + 0.99 * target_q * (~dones).unsqueeze(-1)
        
        # í˜„ì¬ Q ê°’
        current_q1 = self.critic1(torch.cat([states, actions], dim=-1))
        current_q2 = self.critic2(torch.cat([states, actions], dim=-1))
        
        # Critic ì†ì‹¤
        critic1_loss = F.mse_loss(current_q1, target_q)
        critic2_loss = F.mse_loss(current_q2, target_q)
        
        # Critic ì—…ë°ì´íŠ¸
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), 1.0)
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), 1.0)
        self.critic2_optimizer.step()
        
        # Actor ì—…ë°ì´íŠ¸
        actor_output = self.actor(states)
        mean, log_std = actor_output.chunk(2, dim=-1)
        log_std = torch.clamp(log_std, -20, 2)
        std = log_std.exp()
        
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()
        actions_pred = torch.tanh(x_t)
        log_probs = normal.log_prob(x_t) - torch.log(1 - actions_pred.pow(2) + 1e-6)
        log_probs = log_probs.sum(dim=-1, keepdim=True)
        
        q1_pred = self.critic1(torch.cat([states, actions_pred], dim=-1))
        q2_pred = self.critic2(torch.cat([states, actions_pred], dim=-1))
        q_pred = torch.min(q1_pred, q2_pred)
        
        actor_loss = (self.alpha * log_probs - q_pred).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
        self.actor_optimizer.step()
        
        # Alpha ì—…ë°ì´íŠ¸
        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
        
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()
        
        # Soft update target networks
        self._soft_update(self.target_critic1, self.critic1)
        self._soft_update(self.target_critic2, self.critic2)
        
        return {
            'critic1_loss': critic1_loss.item(),
            'critic2_loss': critic2_loss.item(),
            'actor_loss': actor_loss.item(),
            'alpha_loss': alpha_loss.item(),
            'alpha': self.alpha.item()
        }
    
    def _soft_update(self, target, source):
        """ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸"""
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)

class IntegratedKLDPMMMoESAC:
    """í†µí•©ëœ KL-DPMM MoE-SAC ì—ì´ì „íŠ¸ - RTX 4060 ìµœì í™”"""
    
    def __init__(self, state_dim: int, action_dim: int, instr_dim: int = 384,
                 latent_dim: int = 32, tau_birth: float = 1e-3, 
                 tau_merge_kl: float = 0.1, device: str = "cuda"):
        
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.instr_dim = instr_dim
        self.latent_dim = latent_dim
        
        print(f"ğŸ¤– Initializing Integrated KL-DPMM MoE-SAC Agent")
        print(f"  Device: {self.device}")
        print(f"  State dim: {state_dim}, Action dim: {action_dim}")
        print(f"  Latent dim: {latent_dim}")
        print(f"  Birth threshold: {tau_birth}, Merge threshold: {tau_merge_kl}")
        
        # Pre-trained language model (ê²½ëŸ‰í™”)
        print("ğŸ“ Loading pre-trained sentence transformer...")
        self.sentence_transformer = SentenceTransformer(
            'sentence-transformers/all-MiniLM-L6-v2',
            device=self.device
        )
        self.sentence_transformer.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ê³ ì •
        
        # Fusion encoder
        self.fusion_encoder = RTX4060OptimizedFusionEncoder(
            instr_dim=instr_dim,
            state_dim=state_dim,
            latent_dim=latent_dim
        ).to(self.device)
        
        # KL-Divergence DPMM
        self.dpmm = KLDivergenceDPMM(
            latent_dim=latent_dim,
            tau_birth=tau_birth,
            tau_merge_kl=tau_merge_kl
        )
        
        # SAC Experts
        self.experts: Dict[int, RTX4060SACExpert] = {}
        self.max_experts = 8  # RTX 4060 ë©”ëª¨ë¦¬ ì œí•œ
        
        # í•™ìŠµ í†µê³„
        self.step_count = 0
        self.episode_count = 0
        
        # Fusion encoder optimizer
        self.fusion_optimizer = torch.optim.Adam(
            self.fusion_encoder.parameters(), lr=1e-4
        )
        
        print(f"âœ… Agent initialized successfully!")
    
    def encode_instruction(self, instruction: str) -> torch.Tensor:
        """ìì—°ì–´ ì§€ì‹œì‚¬í•­ ì¸ì½”ë”©"""
        with torch.no_grad():
            embedding = self.sentence_transformer.encode(
                instruction, 
                convert_to_tensor=True,
                device=self.device
            )
            return embedding
    
    def select_action(self, state: np.ndarray, instruction: str, 
                     deterministic: bool = False) -> Tuple[np.ndarray, int]:
        """í–‰ë™ ì„ íƒ"""
        with torch.no_grad():
            # ì§€ì‹œì‚¬í•­ ì¸ì½”ë”©
            instr_emb = self.encode_instruction(instruction)
            
            # ìƒíƒœ í…ì„œ ë³€í™˜
            if not torch.is_tensor(state):
                state_tensor = torch.FloatTensor(state).to(self.device)
            else:
                state_tensor = state.to(self.device)
            
            # ìœµí•©ëœ ì ì¬ í‘œí˜„
            latent_state = self.fusion_encoder(instr_emb, state_tensor)
            
            # DPMM í´ëŸ¬ìŠ¤í„° í• ë‹¹
            cluster_id = self.dpmm.assign_and_update(latent_state.cpu().numpy())
            
            # ì „ë¬¸ê°€ ìƒì„± (í•„ìš”ì‹œ)
            if cluster_id not in self.experts:
                if len(self.experts) >= self.max_experts:
                    print(f"âš ï¸ Maximum experts ({self.max_experts}) reached!")
                    cluster_id = min(self.experts.keys())  # ê°€ì¥ ì˜¤ë˜ëœ ì „ë¬¸ê°€ ì‚¬ìš©
                else:
                    self._create_expert(cluster_id)
            
            # í–‰ë™ ì„ íƒ
            expert = self.experts[cluster_id]
            action = expert.select_action(state, deterministic)
            
            return action, cluster_id
    
    def _create_expert(self, expert_id: int):
        """ìƒˆ ì „ë¬¸ê°€ ìƒì„±"""
        print(f"  ğŸ§  Creating new expert {expert_id}")
        expert = RTX4060SACExpert(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            expert_id=expert_id
        )
        self.experts[expert_id] = expert
    
    def store_experience(self, state, action, reward, next_state, done, 
                        instruction: str, cluster_id: int):
        """ê²½í—˜ ì €ì¥"""
        if cluster_id in self.experts:
            expert = self.experts[cluster_id]
            expert.store_experience(state, action, reward, next_state, done)
    
    def update_experts(self, batch_size: int = 32) -> Dict:
        """ëª¨ë“  ì „ë¬¸ê°€ ì—…ë°ì´íŠ¸"""
        losses = {}
        updated_count = 0
        
        for expert_id, expert in self.experts.items():
            if len(expert.replay_buffer) >= batch_size:
                try:
                    expert_losses = expert.update(batch_size)
                    if expert_losses:
                        losses[f'expert_{expert_id}'] = expert_losses
                        updated_count += 1
                except Exception as e:
                    print(f"âš ï¸ Expert {expert_id} update failed: {e}")
        
        # Fusion encoder ì—…ë°ì´íŠ¸ (ì„ íƒì )
        if updated_count > 0 and self.step_count % 10 == 0:
            self._update_fusion_encoder()
        
        self.step_count += 1
        
        return {
            'updated_experts': updated_count,
            'total_experts': len(self.experts),
            'losses': losses
        }
    
    def _update_fusion_encoder(self):
        """Fusion encoder ì—…ë°ì´íŠ¸ (ê°„ë‹¨í•œ ë²„ì „)"""
        # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë©”íƒ€ í•™ìŠµ ë¡œì§ì´ í•„ìš”í•˜ì§€ë§Œ,
        # RTX 4060 ì œì•½ìœ¼ë¡œ ì¸í•´ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„
        pass  # í˜„ì¬ëŠ” ê³ ì •ëœ fusion encoder ì‚¬ìš©
    
    def merge_clusters(self, verbose: bool = False) -> bool:
        """KL divergence ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ë³‘í•©"""
        merged = self.dpmm.merge_clusters(verbose)
        
        if merged and verbose:
            # ì „ë¬¸ê°€ ë³‘í•©ì€ ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ ìœ ì§€
            # TODO: ì‹¤ì œ ì „ë¬¸ê°€ ë„¤íŠ¸ì›Œí¬ ë³‘í•© ë¡œì§ ì¶”ê°€
            print(f"  ğŸ“Š Clusters merged, but experts maintained for stability")
        
        return merged
    
    def get_statistics(self) -> Dict:
        """ì—ì´ì „íŠ¸ í†µê³„"""
        dpmm_stats = self.dpmm.get_statistics()
        
        expert_stats = {
            'total_experts': len(self.experts),
            'buffer_sizes': {
                expert_id: len(expert.replay_buffer) 
                for expert_id, expert in self.experts.items()
            },
            'expert_ids': list(self.experts.keys())
        }
        
        return {
            'dpmm': dpmm_stats,
            'experts': expert_stats,
            'step_count': self.step_count,
            'episode_count': self.episode_count
        }
    
    def save_checkpoint(self, filepath: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'fusion_encoder': self.fusion_encoder.state_dict(),
            'experts': {
                expert_id: {
                    'actor': expert.actor.state_dict(),
                    'critic1': expert.critic1.state_dict(),
                    'critic2': expert.critic2.state_dict(),
                    'target_critic1': expert.target_critic1.state_dict(),
                    'target_critic2': expert.target_critic2.state_dict(),
                    'log_alpha': expert.log_alpha.item(),
                    'replay_buffer': list(expert.replay_buffer)
                }
                for expert_id, expert in self.experts.items()
            },
            'dpmm_stats': self.dpmm.get_statistics(),
            'step_count': self.step_count,
            'episode_count': self.episode_count,
            'config': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'instr_dim': self.instr_dim,
                'latent_dim': self.latent_dim
            }
        }
        
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ Checkpoint saved to {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # Fusion encoder ë¡œë“œ
        self.fusion_encoder.load_state_dict(checkpoint['fusion_encoder'])
        
        # ì „ë¬¸ê°€ë“¤ ë¡œë“œ
        self.experts = {}
        for expert_id, expert_data in checkpoint['experts'].items():
            expert_id = int(expert_id)
            expert = RTX4060SACExpert(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                expert_id=expert_id
            )
            
            expert.actor.load_state_dict(expert_data['actor'])
            expert.critic1.load_state_dict(expert_data['critic1'])
            expert.critic2.load_state_dict(expert_data['critic2'])
            expert.target_critic1.load_state_dict(expert_data['target_critic1'])
            expert.target_critic2.load_state_dict(expert_data['target_critic2'])
            expert.log_alpha.data.fill_(expert_data['log_alpha'])
            
            # Replay buffer ë³µì›
            expert.replay_buffer.extend(expert_data['replay_buffer'])
            
            self.experts[expert_id] = expert
        
        # ê¸°íƒ€ ìƒíƒœ ë³µì›
        self.step_count = checkpoint['step_count']
        self.episode_count = checkpoint['episode_count']
        
        print(f"ğŸ“‚ Checkpoint loaded from {filepath}")
        print(f"  Loaded {len(self.experts)} experts")
