import argparse
import torch
import numpy as np
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser(description="KL-DPMM MoE SAC Training")
    parser.add_argument("--tasks", type=str, default="tasks.yaml")
    parser.add_argument("--save_dir", type=str, default="./outputs")
    parser.add_argument("--num_envs", type=int, default=256)
    parser.add_argument("--episodes", type=int, default=100)
    parser.add_argument("--tau_birth", type=float, default=1e-3)
    parser.add_argument("--tau_merge_kl", type=float, default=0.1)  # KL divergence ì„ê³„ê°’
    parser.add_argument("--latent_dim", type=int, default=16)
    parser.add_argument("--checkpoint_freq", type=int, default=20)
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    import yaml
    import os
    from ant_push_env import IsaacAntPushEnv
    from config import CONFIG
    from language_processor import LanguageProcessor
    from integrated_dpmm_moe_sac import IntegratedKLDPMMMoESAC  # KL-DPMM ì‚¬ìš©

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.save_dir, exist_ok=True)

    # íƒœìŠ¤í¬ ë¡œë“œ
    tasks = yaml.safe_load(open(args.tasks))
    
    # ì–¸ì–´ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    lang = LanguageProcessor(device=CONFIG.device)
    
    # í†µí•© ì—ì´ì „íŠ¸ (ì•„ì§ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ)
    agent = None
    
    # í•™ìŠµ í†µê³„
    all_training_stats = {
        'tasks': [],
        'episode_rewards': [],
        'cluster_counts': [],
        'expert_counts': [],
        'merge_events': []
    }

    print(f"ğŸš€ Starting KL-DPMM MoE-SAC Training")
    print(f"  Tasks: {len(tasks)}")
    print(f"  Episodes per task: {args.episodes}")
    print(f"  KL merge threshold: {args.tau_merge_kl}")
    print(f"  Birth threshold: {args.tau_birth}")

    # íƒœìŠ¤í¬ë³„ í•™ìŠµ
    for task_idx, task in enumerate(tasks):
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Task {task_idx + 1}/{len(tasks)}: {task['name']}")
        print(f"ğŸ“ Instruction: {task['instruction']}")
        print(f"{'='*60}")
        
        # í™˜ê²½ ìƒì„±
        env = IsaacAntPushEnv(custom_cfg=task['env_cfg'], num_envs=args.num_envs)
        
        # ì´ˆê¸° ìƒíƒœë¡œ ì°¨ì› í™•ì¸
        state, _ = env.reset()
        state_dim = state.shape[0]
        action_dim = env.action_space.shape[1] if len(env.action_space.shape) == 2 else env.action_space.shape[0]
        
        print(f"ğŸ”§ Environment Setup:")
        print(f"  State dim: {state_dim}")
        print(f"  Action dim: {action_dim}")
        print(f"  Action space: {env.action_space.shape}")
        
        # ì§€ì‹œì‚¬í•­ ì„ë² ë”©
        instr_emb = lang.encode(task['instruction'])
        instr_dim = instr_emb.shape[-1]
        
        print(f"  Instruction embedding dim: {instr_dim}")
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (ì²« ë²ˆì§¸ íƒœìŠ¤í¬ì—ì„œë§Œ)
        if agent is None:
            agent = IntegratedKLDPMMMoESAC(
                state_dim=state_dim,
                action_dim=action_dim,
                instr_dim=instr_dim,
                latent_dim=args.latent_dim,
                tau_birth=args.tau_birth,
                tau_merge_kl=args.tau_merge_kl,  # KL divergence ì„ê³„ê°’
                device=str(CONFIG.device)
            )
            print(f"ğŸ¤– Agent initialized with KL-DPMM")
        
        # íƒœìŠ¤í¬ë³„ í•™ìŠµ í†µê³„
        task_stats = {
            'task_name': task['name'],
            'episode_rewards': [],
            'cluster_counts': [],
            'expert_counts': [],
            'kl_merge_events': []
        }
        
        # ì—í”¼ì†Œë“œ í•™ìŠµ
        for episode in range(args.episodes):
            print(f"\n--- Episode {episode + 1}/{args.episodes} ---")
            
            # í™˜ê²½ ë¦¬ì…‹
            state, _ = env.reset()
            episode_reward = 0
            step_count = 0
            max_steps = 500
            
            while step_count < max_steps:
                # í–‰ë™ ì„ íƒ
                action, cluster_id = agent.select_action(
                    state, 
                    instr_emb.squeeze(0).detach().cpu().numpy(),
                    deterministic=False
                )
                
                if step_count == 0:
                    print(f"  Assigned to cluster/expert: {cluster_id}")
                
                # í™˜ê²½ ìŠ¤í…
                try:
                    next_state, reward, done, info = env.step(action)
                    episode_reward += reward
                    
                    # ê²½í—˜ ì €ì¥
                    agent.store_experience(state, action, reward, next_state, done, cluster_id)
                    
                    state = next_state
                    step_count += 1
                    
                    if done:
                        break
                        
                except Exception as e:
                    print(f"  âš ï¸ Environment step error: {e}")
                    break
            
            # ì—í”¼ì†Œë“œ í›„ ì—…ë°ì´íŠ¸
            if episode % 5 == 0:  # 5 ì—í”¼ì†Œë“œë§ˆë‹¤ ì—…ë°ì´íŠ¸
                update_info = agent.update_experts()
                print(f"  ğŸ“ˆ Updated {update_info['updated_experts']}/{update_info['total_experts']} experts")
            
            # KL ê¸°ë°˜ ë³‘í•© (ë” ìì£¼ ì‹œë„)
            if episode % 3 == 0:  # 3 ì—í”¼ì†Œë“œë§ˆë‹¤ ë³‘í•© ì‹œë„
                merged = agent.merge_clusters(verbose=(episode % 10 == 0))
                if merged:
                    task_stats['kl_merge_events'].append(episode)
            
            # í†µê³„ ìˆ˜ì§‘
            stats = agent.get_statistics()
            task_stats['episode_rewards'].append(episode_reward)
            task_stats['cluster_counts'].append(stats['dpmm']['num_clusters'])
            task_stats['expert_counts'].append(stats['experts']['total_experts'])
            
            # ì£¼ê¸°ì  ì¶œë ¥
            if episode % 10 == 0 or episode == args.episodes - 1:
                print(f"  ğŸ“Š Episode {episode + 1} Summary:")
                print(f"    Reward: {episode_reward:.2f}")
                print(f"    Steps: {step_count}")
                print(f"    Clusters: {stats['dpmm']['num_clusters']}")
                print(f"    Experts: {stats['experts']['total_experts']}")
                print(f"    Total births: {stats['dpmm']['total_births']}")
                print(f"    Total KL-merges: {stats['dpmm']['total_merges']}")
                
                # ìµœê·¼ í‰ê·  ë³´ìƒ
                recent_rewards = task_stats['episode_rewards'][-10:]
                if recent_rewards:
                    print(f"    Recent avg reward: {np.mean(recent_rewards):.2f}")
        
        # íƒœìŠ¤í¬ ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (task_idx + 1) % args.checkpoint_freq == 0:
            checkpoint_path = os.path.join(args.save_dir, f"checkpoint_task_{task_idx + 1}.pt")
            agent.save_checkpoint(checkpoint_path)
        
        # íƒœìŠ¤í¬ í†µê³„ ì €ì¥
        all_training_stats['tasks'].append(task_stats)
        all_training_stats['episode_rewards'].extend(task_stats['episode_rewards'])
        all_training_stats['cluster_counts'].extend(task_stats['cluster_counts'])
        all_training_stats['expert_counts'].extend(task_stats['expert_counts'])
        all_training_stats['merge_events'].extend(task_stats['kl_merge_events'])
        
        # íƒœìŠ¤í¬ ìš”ì•½
        print(f"\nğŸ Task '{task['name']}' completed!")
        print(f"  Average reward: {np.mean(task_stats['episode_rewards']):.2f}")
        print(f"  Final clusters: {task_stats['cluster_counts'][-1]}")
        print(f"  Final experts: {task_stats['expert_counts'][-1]}")
        print(f"  KL-merge events: {len(task_stats['kl_merge_events'])}")
        
        env.close()
    
    # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    final_checkpoint_path = os.path.join(args.save_dir, "final_checkpoint.pt")
    agent.save_checkpoint(final_checkpoint_path)
    
    # í•™ìŠµ í†µê³„ ì €ì¥
    stats_path = os.path.join(args.save_dir, "training_stats.npz")
    np.savez(stats_path, **all_training_stats)
    
    # ìµœì¢… ìš”ì•½
    print(f"\nğŸ‰ All tasks completed!")
    print(f"  Total episodes: {len(all_training_stats['episode_rewards'])}")
    print(f"  Average reward: {np.mean(all_training_stats['episode_rewards']):.2f}")
    print(f"  Final clusters: {all_training_stats['cluster_counts'][-1]}")
    print(f"  Final experts: {all_training_stats['expert_counts'][-1]}")
    print(f"  Total KL-merge events: {len(all_training_stats['merge_events'])}")
    print(f"  Checkpoints saved to: {args.save_dir}")

    sim.close()

if __name__ == "__main__":
    main()