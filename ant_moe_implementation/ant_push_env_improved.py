import numpy as np
import torch
from gymnasium.spaces import Box
from isaaclab.envs import ManagerBasedRLEnv
from config import CONFIG
import isaaclab.sim as sim_utils
from isaaclab.assets import RigidObject, RigidObjectCfg
from isaaclab_tasks.manager_based.classic.ant.ant_env_cfg import AntEnvCfg, MySceneCfg
from glip_cube_detector import GLIPCubeDetector
from improved_reward_system import ImprovedRewardSystem

class IsaacAntPushEnvImproved(ManagerBasedRLEnv):
    """ê°œì„ ëœ ë³´ìƒ ì‹œìŠ¤í…œì„ ê°€ì§„ Ant Push í™˜ê²½"""
    
    def __init__(self, custom_cfg=None, num_envs: int = 1):
        print(f"ğŸ—ï¸ Creating Improved Ant Push Environment")
        
        # Build base IsaacLab Ant config
        cfg = AntEnvCfg()
        cfg.scene = MySceneCfg(num_envs=num_envs, env_spacing=cfg.scene.env_spacing)
        cfg.sim.device = str(CONFIG.device)
        super().__init__(cfg=cfg)

        # Store custom params
        c = custom_cfg or {}
        self.cube_position = np.array(c.get('cube_position', [2.0, 0.0, 0.1]), dtype=np.float32)
        self.goal_position = np.array(c.get('goal_position', [3.0, 0.0, 0.1]), dtype=np.float32)
        self.success_distance = c.get('success_distance', 0.3)
        self.max_steps = c.get('max_steps', 500)
        
        print(f"  Cube position: {self.cube_position}")
        print(f"  Goal position: {self.goal_position}")
        print(f"  Success distance: {self.success_distance}")
        print(f"  Max steps: {self.max_steps}")

        # ê°œì„ ëœ ë³´ìƒ ì‹œìŠ¤í…œ
        self.reward_system = ImprovedRewardSystem()
        
        # ìŠ¤í… ì¹´ìš´í„°
        self.current_step = 0

        # --- BLUE CUBE CREATION ---
        try:
            spawn_cfg = sim_utils.CuboidCfg(
                size=(0.2, 0.2, 0.2),
                rigid_props=sim_utils.RigidBodyPropertiesCfg(),
                mass_props=sim_utils.MassPropertiesCfg(mass=1.0),
                collision_props=sim_utils.CollisionPropertiesCfg(),
                visual_material=sim_utils.PreviewSurfaceCfg(
                    diffuse_color=(0.0, 0.0, 1.0),  # Blue color
                    metallic=0.1
                )
            )
            
            cube_cfg = RigidObjectCfg(
                prim_path="/World/envs/env_.*/Cube",
                spawn=spawn_cfg,
                init_state=RigidObjectCfg.InitialStateCfg(
                    pos=(*self.cube_position,),
                ),
            )
            
            self.cube = RigidObject(cfg=cube_cfg)
            print("âœ… Blue cube created successfully")
            
        except Exception as e:
            print(f"Warning: Cube creation failed: {e}")
            self.cube = None

        # --- GLIP DETECTOR ---
        try:
            self.glip_detector = GLIPCubeDetector(
                model_name="IDEA-Research/grounding-dino-tiny",
                confidence_threshold=0.3,
                device=CONFIG.device
            )
            
            # ì¹´ë©”ë¼ ë‚´ì¬ íŒŒë¼ë¯¸í„° (Isaac Lab ê¸°ë³¸ê°’)
            self.camera_intrinsics = {
                'fx': 554.3, 'fy': 554.3,
                'cx': 320.0, 'cy': 240.0,
                'width': 640, 'height': 480
            }
            print(f"  Camera intrinsics: fx={self.camera_intrinsics['fx']}, fy={self.camera_intrinsics['fy']}, cx={self.camera_intrinsics['cx']}, cy={self.camera_intrinsics['cy']}")
            
        except Exception as e:
            print(f"Warning: GLIP detector initialization failed: {e}")
            self.glip_detector = None

        # Redefine observation space: policy obs + cube_pos(3) + goal_pos(3)
        base_shape = self.observation_space['policy'].shape
        base_dim = int(np.prod(base_shape))
        full_dim = base_dim + 3 + 3
        self.observation_space = Box(
            low=-np.inf, high=np.inf, shape=(full_dim,), dtype=np.float32
        )
        print(f"âœ… Observation space: ({full_dim},)")

    def _tensor_to_numpy(self, tensor):
        """í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        if torch.is_tensor(tensor):
            return tensor.detach().cpu().numpy()
        elif isinstance(tensor, np.ndarray):
            return tensor
        else:
            return np.array(tensor)

    def _get_cube_position(self):
        """ì‹¤ì œ íë¸Œ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°"""
        if self.cube is not None:
            try:
                cube_pos_tensor = self.cube.data.root_pos_w[0]
                cube_pos = self._tensor_to_numpy(cube_pos_tensor)[:3]
                return np.array(cube_pos, dtype=np.float32)
            except:
                pass
        return np.array(self.cube_position, dtype=np.float32)

    def _get_ant_position(self):
        """Antì˜ í˜„ì¬ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°"""
        try:
            ant_pos_tensor = self.scene["robot"].data.root_pos_w[0]
            ant_pos = self._tensor_to_numpy(ant_pos_tensor)[:3]
            return np.array(ant_pos, dtype=np.float32)
        except:
            return np.array([0.5, 0.0, 0.0], dtype=np.float32)

    def get_camera_obs(self):
        """ì¹´ë©”ë¼ ê´€ì¸¡ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë”ë¯¸ êµ¬í˜„)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Isaac Labì˜ ì¹´ë©”ë¼ ì„¼ì„œë¥¼ ì‚¬ìš©
        rgb = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        depth = np.random.rand(480, 640).astype(np.float32)
        return rgb, depth

    def _get_glip_cube_position(self):
        """GLIPìœ¼ë¡œ ê°ì§€ëœ íë¸Œ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°"""
        if self.glip_detector is None:
            return self._get_cube_position()
        
        try:
            rgb, depth = self.get_camera_obs()
            
            # GLIPìœ¼ë¡œ 3D ìœ„ì¹˜ ê°ì§€
            world_pos = self.glip_detector.get_best_detection(
                rgb, depth, self.camera_intrinsics
            )
            
            if world_pos is not None:
                # ì¹´ë©”ë¼ ì¢Œí‘œê³„ì—ì„œ ì›”ë“œ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                ant_pos = self._get_ant_position()
                world_x = world_pos[0] + ant_pos[0]
                world_y = world_pos[1] + ant_pos[1] 
                world_z = 0.1  # íë¸ŒëŠ” ì§€ë©´ì—ì„œ 0.1m ë†’ì´
                
                return np.array([world_x, world_y, world_z], dtype=np.float32)
        except Exception as e:
            pass
        
        # GLIP ê°ì§€ ì‹¤íŒ¨ ì‹œ ì‹¤ì œ íë¸Œ ìœ„ì¹˜ ë°˜í™˜
        return self._get_cube_position()

    def _prepare_action(self, action):
        """ì•¡ì…˜ì„ í™˜ê²½ì— ë§ê²Œ ì¤€ë¹„"""
        if isinstance(action, np.ndarray):
            if action.ndim == 1:
                action = action.reshape(1, -1)
            action_tensor = torch.FloatTensor(action).to(self.device)
        elif torch.is_tensor(action):
            if action.dim() == 1:
                action = action.unsqueeze(0)
            action_tensor = action.to(self.device)
        else:
            action_tensor = torch.FloatTensor([[action]]).to(self.device)
        
        return action_tensor

    def _merge_obs(self, obs_dict):
        """ê´€ì¸¡ê°’ ë³‘í•©: [policy_obs, glip_cube_pos, goal_pos]"""
        base = obs_dict['policy']
        
        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
        if torch.is_tensor(base):
            base = self._tensor_to_numpy(base)
        
        # GLIPìœ¼ë¡œ íë¸Œ ìœ„ì¹˜ ê°ì§€
        glip_cube_pos = self._get_glip_cube_position()
        
        # ëª¨ë“  ê²ƒì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        base = np.array(base, dtype=np.float32).flatten()
        glip_cube_pos = np.array(glip_cube_pos, dtype=np.float32)
        goal_pos = np.array(self.goal_position, dtype=np.float32)
        
        return np.concatenate([base, glip_cube_pos, goal_pos], axis=0)

    def reset(self):
        """í™˜ê²½ ë¦¬ì…‹"""
        obs_dict, info = super().reset()
        merged = self._merge_obs(obs_dict)
        
        # ë³´ìƒ ì‹œìŠ¤í…œ ë¦¬ì…‹
        self.reward_system.reset()
        self.current_step = 0
        
        print(f"ğŸ”„ Environment reset - State shape: {merged.shape}")
        return merged, info

    def step(self, action):
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        self.current_step += 1
        
        try:
            action_tensor = self._prepare_action(action)
            obs_dict, _, term, trunc, info = super().step(action_tensor)
            
        except Exception as e:
            print(f"Environment step error: {e}")
            obs_dict = {'policy': np.zeros(self.observation_space.shape[0] - 6)}
            term = True
            trunc = False
            info = {}
        
        obs = self._merge_obs(obs_dict)
        
        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
        base_obs_dim = len(obs) - 6
        ant_pos = self._get_ant_position()
        glip_cube_pos = obs[base_obs_dim:base_obs_dim+3]
        goal_pos = obs[base_obs_dim+3:base_obs_dim+6]
        actual_cube_pos = self._get_cube_position()
        
        # ê°œì„ ëœ ë³´ìƒ ê³„ì‚°
        reward, reward_info = self.reward_system.compute_reward(
            ant_pos=ant_pos,
            cube_pos=actual_cube_pos,  # ì‹¤ì œ íë¸Œ ìœ„ì¹˜ ì‚¬ìš©
            goal_pos=goal_pos,
            action=action if isinstance(action, np.ndarray) else np.array(action)
        )
        
        # ì„±ê³µ ì¡°ê±´ í™•ì¸
        cube_to_goal_dist = np.linalg.norm(actual_cube_pos - goal_pos)
        success = cube_to_goal_dist < self.success_distance
        
        # ì¢…ë£Œ ì¡°ê±´
        done = bool(term or trunc or success or self.current_step >= self.max_steps)
        
        # ì¶”ê°€ ì •ë³´
        info.update({
            'success': success,
            'cube_to_goal_distance': cube_to_goal_dist,
            'ant_to_cube_distance': np.linalg.norm(ant_pos - actual_cube_pos),
            'steps': self.current_step,
            'reward_info': reward_info,
            'glip_detection_error': np.linalg.norm(glip_cube_pos - actual_cube_pos)
        })
        
        return obs, reward, done, info

    def get_action_dim(self):
        """ì•¡ì…˜ ì°¨ì› ë°˜í™˜"""
        if hasattr(self.action_space, 'shape'):
            return self.action_space.shape[1] if len(self.action_space.shape) == 2 else self.action_space.shape[0]
        return 8

    def update_task_config(self, task_config):
        """íƒœìŠ¤í¬ ì„¤ì • ì—…ë°ì´íŠ¸ (ì»¤ë¦¬í˜ëŸ¼ìš©)"""
        self.cube_position = np.array(task_config['cube_position'], dtype=np.float32)
        self.goal_position = np.array(task_config['goal_position'], dtype=np.float32)
        self.success_distance = task_config.get('success_distance', 0.3)
        self.max_steps = task_config.get('max_steps', 500)
        
        print(f"ğŸ“ Task config updated:")
        print(f"  Cube: {self.cube_position} -> Goal: {self.goal_position}")
        print(f"  Success distance: {self.success_distance}, Max steps: {self.max_steps}")

    def close(self):
        """í™˜ê²½ ì •ë¦¬"""
        try:
            super().close()
            print("âœ… Environment closed safely")
        except Exception as e:
            print(f"Warning during close: {e}")