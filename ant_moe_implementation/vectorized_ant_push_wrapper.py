#!/usr/bin/env python3
"""
ë³‘ë ¬ í™˜ê²½ ì§€ì› Ant Push ë˜í¼ (IsaacLab ë³‘ë ¬í™” í™œìš©)
"""

import gymnasium as gym
import numpy as np
import torch
from gymnasium.spaces import Box
from typing import Dict, Any, Tuple, Optional, List
from isaaclab.envs import ManagerBasedRLEnv
from isaaclab_tasks.manager_based.classic.ant.ant_env_cfg import AntEnvCfg, MySceneCfg
from config import CONFIG

class VectorizedAntPushWrapper(gym.Env):
    """ë³‘ë ¬ í™˜ê²½ì„ ì§€ì›í•˜ëŠ” Ant Push ë˜í¼"""
    
    def __init__(self, custom_cfg: Dict[str, Any] = None, num_envs: int = 64):
        super().__init__()
        
        self.num_envs = num_envs
        self.custom_cfg = custom_cfg or {}
        
        # IsaacLab ë³‘ë ¬ í™˜ê²½ ìƒì„±
        cfg = AntEnvCfg()
        cfg.scene = MySceneCfg(num_envs=num_envs, env_spacing=cfg.scene.env_spacing)
        cfg.sim.device = str(CONFIG.device)
        cfg.seed = 42
        
        self.isaac_env = ManagerBasedRLEnv(cfg=cfg)
        
        # ì´ˆê¸° ë¦¬ì…‹ìœ¼ë¡œ ì°¨ì› í™•ì¸
        obs_dict, _ = self.isaac_env.reset()
        base_obs = obs_dict['policy']
        
        # ë‹¨ì¼ í™˜ê²½ ì°¨ì› ê³„ì‚°
        if len(base_obs.shape) == 2:  # [num_envs, obs_dim]
            single_obs_dim = base_obs.shape[1]
        else:
            single_obs_dim = base_obs.shape[0]
        
        # íë¸Œ/ëª©í‘œ ìœ„ì¹˜ ì¶”ê°€
        self.single_obs_dim = single_obs_dim + 6
        
        # Gymnasium ê³µê°„ ì •ì˜ (ë‹¨ì¼ í™˜ê²½ ê¸°ì¤€)
        self.observation_space = Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(self.single_obs_dim,), 
            dtype=np.float32
        )
        
        # ì•¡ì…˜ ê³µê°„
        action_space = self.isaac_env.action_space
        if len(action_space.shape) == 2:  # [num_envs, action_dim]
            single_action_dim = action_space.shape[1]
        else:
            single_action_dim = action_space.shape[0]
        
        self.action_space = Box(
            low=-1.0, 
            high=1.0, 
            shape=(single_action_dim,), 
            dtype=np.float32
        )
        
        # í™˜ê²½ ìƒíƒœ
        self.current_step = 0
        self.max_episode_steps = 500
        self.active_env_idx = 0  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í™˜ê²½ ì¸ë±ìŠ¤
        
        # íë¸Œ/ëª©í‘œ ìœ„ì¹˜ (ëª¨ë“  í™˜ê²½ì— ë™ì¼í•˜ê²Œ ì ìš©)
        self.cube_pos = np.array(self.custom_cfg.get('cube_position', [2.0, 0.0, 0.1]), dtype=np.float32)
        self.goal_pos = np.array(self.custom_cfg.get('goal_position', [3.0, 0.0, 0.1]), dtype=np.float32)
        
        print(f"ğŸ—ï¸ VectorizedAntPushWrapper created:")
        print(f"  â€¢ Parallel environments: {num_envs}")
        print(f"  â€¢ Single obs dim: {self.single_obs_dim}")
        print(f"  â€¢ Single action dim: {single_action_dim}")
        print(f"  â€¢ Using environment rotation for diversity")
    
    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """í™˜ê²½ ë¦¬ì…‹ (ë³‘ë ¬ í™˜ê²½ì—ì„œ í•˜ë‚˜ ì„ íƒ)"""
        if seed is not None:
            np.random.seed(seed)
        
        self.current_step = 0
        
        # ì „ì²´ ë³‘ë ¬ í™˜ê²½ ë¦¬ì…‹
        obs_dict, info = self.isaac_env.reset()
        
        # ì‚¬ìš©í•  í™˜ê²½ ì¸ë±ìŠ¤ ì„ íƒ (ë¼ìš´ë“œ ë¡œë¹ˆ ë˜ëŠ” ëœë¤)
        self.active_env_idx = np.random.randint(0, self.num_envs)
        
        # ì„ íƒëœ í™˜ê²½ì˜ ê´€ì¸¡ê°’ ì¶”ì¶œ
        single_obs = self._extract_single_obs(obs_dict, self.active_env_idx)
        
        info_single = {
            'cube_position': self.cube_pos.tolist(),
            'goal_position': self.goal_pos.tolist(),
            'episode_step': self.current_step,
            'active_env_idx': self.active_env_idx,
            'total_parallel_envs': self.num_envs
        }
        
        return single_obs, info_single
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… (ë³‘ë ¬ í™˜ê²½ í™œìš©)"""
        self.current_step += 1
        
        # ì•¡ì…˜ì„ ë³‘ë ¬ í™˜ê²½ í˜•íƒœë¡œ í™•ì¥
        parallel_actions = self._expand_action_to_parallel(action)
        
        try:
            # ë³‘ë ¬ í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            obs_dict, rewards, terminated, truncated, info = self.isaac_env.step(parallel_actions)
            
            # í™œì„± í™˜ê²½ì˜ ê²°ê³¼ë§Œ ì¶”ì¶œ
            single_obs = self._extract_single_obs(obs_dict, self.active_env_idx)
            single_reward = self._extract_single_value(rewards, self.active_env_idx)
            single_terminated = self._extract_single_value(terminated, self.active_env_idx)
            single_truncated = self._extract_single_value(truncated, self.active_env_idx)
            
            # ì»¤ìŠ¤í…€ ë³´ìƒ ì¶”ê°€
            custom_reward = self._calculate_reward(single_obs)
            total_reward = float(single_reward) + custom_reward
            
            # ì—í”¼ì†Œë“œ ê¸¸ì´ ì œí•œ
            if self.current_step >= self.max_episode_steps:
                single_truncated = True
            
            info_single = {
                'episode_step': self.current_step,
                'active_env_idx': self.active_env_idx,
                'parallel_rewards_mean': float(rewards.mean()) if hasattr(rewards, 'mean') else single_reward,
                'max_steps_reached': self.current_step >= self.max_episode_steps
            }
            
            return single_obs, float(total_reward), bool(single_terminated), bool(single_truncated), info_single
            
        except Exception as e:
            print(f"âš ï¸ Vectorized step error: {e}")
            # í´ë°±
            dummy_obs = np.random.randn(self.single_obs_dim).astype(np.float32)
            return dummy_obs, 0.0, False, False, {'error': str(e)}
    
    def _extract_single_obs(self, obs_dict: Dict, env_idx: int) -> np.ndarray:
        """ë³‘ë ¬ ê´€ì¸¡ê°’ì—ì„œ ë‹¨ì¼ í™˜ê²½ ê´€ì¸¡ê°’ ì¶”ì¶œ"""
        base_obs = obs_dict['policy']
        
        # Tensorë¥¼ numpyë¡œ ë³€í™˜
        if hasattr(base_obs, 'cpu'):
            base_obs = base_obs.cpu().numpy()
        
        # íŠ¹ì • í™˜ê²½ì˜ ê´€ì¸¡ê°’ ì¶”ì¶œ
        if len(base_obs.shape) == 2:  # [num_envs, obs_dim]
            single_base_obs = base_obs[env_idx]
        else:
            single_base_obs = base_obs
        
        # íë¸Œ/ëª©í‘œ ìœ„ì¹˜ ì¶”ê°€
        full_obs = np.concatenate([single_base_obs.flatten(), self.cube_pos, self.goal_pos])
        
        return full_obs.astype(np.float32)
    
    def _extract_single_value(self, tensor_values, env_idx: int):
        """ë³‘ë ¬ í…ì„œì—ì„œ ë‹¨ì¼ ê°’ ì¶”ì¶œ"""
        if hasattr(tensor_values, 'cpu'):
            values = tensor_values.cpu().numpy()
        elif isinstance(tensor_values, torch.Tensor):
            values = tensor_values.detach().numpy()
        else:
            values = np.array(tensor_values)
        
        # ìŠ¤ì¹¼ë¼ì¸ ê²½ìš°
        if values.ndim == 0:
            return values.item()
        
        # ë²¡í„°ì¸ ê²½ìš°
        if values.ndim == 1 and len(values) > env_idx:
            return values[env_idx]
        
        # ê¸°ë³¸ê°’
        return values.flatten()[0] if len(values.flatten()) > 0 else 0
    
    def _expand_action_to_parallel(self, action: np.ndarray) -> torch.Tensor:
        """ë‹¨ì¼ ì•¡ì…˜ì„ ë³‘ë ¬ í™˜ê²½ìš©ìœ¼ë¡œ í™•ì¥"""
        if isinstance(action, np.ndarray):
            action = torch.FloatTensor(action).to(CONFIG.device)
        
        # ì°¨ì› ì •ë¦¬
        if action.dim() == 1:
            action = action.unsqueeze(0)  # [action_dim] -> [1, action_dim]
        
        # ëª¨ë“  í™˜ê²½ì— ë™ì¼í•œ ì•¡ì…˜ ì ìš© (ë˜ëŠ” ë‹¤ì–‘í™” ê°€ëŠ¥)
        parallel_actions = action.repeat(self.num_envs, 1)  # [num_envs, action_dim]
        
        return parallel_actions
    
    def _calculate_reward(self, obs: np.ndarray) -> float:
        """ë³´ìƒ ê³„ì‚°"""
        if len(obs) < 6:
            return 0.0
        
        cube_pos = obs[-6:-3]
        goal_pos = obs[-3:]
        
        distance = np.linalg.norm(cube_pos - goal_pos)
        distance_reward = -distance
        
        if distance < 0.3:
            goal_reward = 10.0
        else:
            goal_reward = 0.0
        
        return distance_reward + goal_reward + 0.1
    
    def close(self):
        """í™˜ê²½ ì¢…ë£Œ"""
        if hasattr(self, 'isaac_env'):
            self.isaac_env.close()

# ğŸ”„ í™˜ê²½ ë¡œí…Œì´ì…˜ì„ í†µí•œ ë‹¤ì–‘ì„± ì¦ëŒ€
class RotatingVectorizedWrapper(VectorizedAntPushWrapper):
    """í™˜ê²½ ë¡œí…Œì´ì…˜ìœ¼ë¡œ ë‹¤ì–‘ì„±ì„ ì¦ëŒ€í•˜ëŠ” ë˜í¼"""
    
    def __init__(self, custom_cfg: Dict[str, Any] = None, num_envs: int = 64, 
                 rotation_frequency: int = 10):
        super().__init__(custom_cfg, num_envs)
        self.rotation_frequency = rotation_frequency
        self.episode_count = 0
    
    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """ë¦¬ì…‹ ì‹œ í™˜ê²½ ë¡œí…Œì´ì…˜"""
        self.episode_count += 1
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ë‹¤ë¥¸ í™˜ê²½ ì‚¬ìš©
        if self.episode_count % self.rotation_frequency == 0:
            self.active_env_idx = (self.active_env_idx + 1) % self.num_envs
            print(f"ğŸ”„ Rotated to environment {self.active_env_idx}")
        
        return super().reset(seed, options)

# ğŸ¯ MoE ì—ì´ì „íŠ¸ì™€ì˜ í†µí•©
class ParallelMoETraining:
    """ë³‘ë ¬ í™˜ê²½ì„ í™œìš©í•œ MoE í•™ìŠµ"""
    
    def __init__(self, num_envs: int = 64, num_experts: int = 8):
        self.num_envs = num_envs
        self.num_experts = num_experts
        
        # ë³‘ë ¬ í™˜ê²½ ìƒì„±
        self.env = RotatingVectorizedWrapper(num_envs=num_envs)
        
        # ê° ì „ë¬¸ê°€ë³„ë¡œ ë‹¤ë¥¸ í™˜ê²½ ì¸ë±ìŠ¤ í• ë‹¹
        self.expert_env_mapping = {}
        for i in range(num_experts):
            self.expert_env_mapping[i] = list(range(i, num_envs, num_experts))
    
    def collect_parallel_experience(self, agent, instruction_emb, steps_per_env: int = 100):
        """ë³‘ë ¬ë¡œ ê²½í—˜ ìˆ˜ì§‘"""
        all_experiences = []
        
        for expert_id in range(self.num_experts):
            # í•´ë‹¹ ì „ë¬¸ê°€ê°€ ì‚¬ìš©í•  í™˜ê²½ë“¤
            env_indices = self.expert_env_mapping[expert_id]
            
            for env_idx in env_indices[:4]:  # ê° ì „ë¬¸ê°€ë‹¹ 4ê°œ í™˜ê²½ ì‚¬ìš©
                self.env.active_env_idx = env_idx
                
                # ì—í”¼ì†Œë“œ ì‹¤í–‰
                state, _ = self.env.reset()
                
                for step in range(steps_per_env):
                    action, cluster_id = agent.select_action(state, instruction_emb)
                    next_state, reward, done, truncated, info = self.env.step(action)
                    
                    all_experiences.append({
                        'state': state,
                        'action': action,
                        'reward': reward,
                        'next_state': next_state,
                        'done': done or truncated,
                        'cluster_id': cluster_id,
                        'env_idx': env_idx
                    })
                    
                    state = next_state
                    
                    if done or truncated:
                        state, _ = self.env.reset()
        
        return all_experiences