import torch
import os

class CONFIG:
    # RTX 4060 μµμ ν™” μ„¤μ •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # λ©”λ¨λ¦¬ μµμ ν™”
    use_mixed_precision = True
    max_batch_size = 32
    gradient_accumulation_steps = 2
    
    # Pre-trained λ¨λΈ μ„¤μ •
    text_model_name = "sentence-transformers/all-MiniLM-L6-v2"  # 22MB
    instr_emb_dim = 384
    
    # λ„¤νΈμ›ν¬ ν¬κΈ° (λ©”λ¨λ¦¬ μ μ•½)
    latent_dim = 32
    hidden_dim = 128
    
    # DPMM νλΌλ―Έν„°
    dpmm_alpha = 1.0
    dpmm_base_var = 1.0
    tau_birth = 1e-3
    tau_merge_kl = 0.1
    
    # SAC νλΌλ―Έν„°
    gamma = 0.99
    tau = 0.005
    actor_lr = 1e-4
    critic_lr = 1e-4
    alpha_lr = 1e-4
    
    # ν•™μµ νλΌλ―Έν„° (RTX 4060 μµμ ν™”)
    batch_size = 32
    buffer_capacity = 5000
    update_frequency = 10
    merge_frequency = 15
    
    # λ©”λ¨λ¦¬ κ΄€λ¦¬
    max_experts = 8
    cleanup_frequency = 10
    
    # ν™κ²½ μ„¤μ •
    max_episode_length = 300
    num_envs = 64  # RTX 4060μ— λ§κ² μ¶•μ†
    
    @classmethod
    def setup_rtx4060_optimization(cls):
        """RTX 4060 νΉν™” μµμ ν™” μ„¤μ •"""
        if torch.cuda.is_available():
            # CUDA λ©”λ¨λ¦¬ μµμ ν™”
            torch.backends.cudnn.benchmark = False
            torch.backends.cudnn.deterministic = True
            
            # λ©”λ¨λ¦¬ λ¶„ν•  μµμ ν™”
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
            # λ©”λ¨λ¦¬ μ •λ¦¬
            torch.cuda.empty_cache()
            
            print(f"π”§ RTX 4060 optimizations applied")
            print(f"  Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
