import argparse
import torch
import numpy as np
import yaml
import os
from datetime import datetime
import matplotlib.pyplot as plt
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser(description="Full Curriculum Training")
    parser.add_argument("--num_envs", type=int, default=1)
    parser.add_argument("--save_dir", type=str, default="./curriculum_results")
    parser.add_argument("--checkpoint_freq", type=int, default=50)
    parser.add_argument("--eval_freq", type=int, default=20)
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    try:
        from ant_push_env_improved import IsaacAntPushEnvImproved
        from integrated_moe_agent import IntegratedMoEAgent
        from language_processor import LanguageProcessor
        from improved_reward_system import CurriculumManager
        from config import CONFIG
        
        print("ğŸ“ Full Curriculum Training Started")
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = os.path.join(args.save_dir, f"curriculum_{timestamp}")
        os.makedirs(save_dir, exist_ok=True)
        
        # ì–¸ì–´ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        lang_processor = LanguageProcessor(device=CONFIG.device)
        
        # ì»¤ë¦¬í˜ëŸ¼ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        curriculum = CurriculumManager()
        
        # í™˜ê²½ ìƒì„± (ì´ˆê¸° ë ˆë²¨ë¡œ)
        initial_level = curriculum.get_current_level()
        env = IsaacAntPushEnvImproved(
            custom_cfg=initial_level,
            num_envs=args.num_envs
        )
        
        # ìƒíƒœ ë° ì•¡ì…˜ ì°¨ì› í™•ì¸
        state, _ = env.reset()
        state_dim = state.shape[0]
        action_dim = env.get_action_dim()
        
        print(f"ğŸ“Š Training Setup:")
        print(f"  State dim: {state_dim}")
        print(f"  Action dim: {action_dim}")
        print(f"  Save directory: {save_dir}")
        
        # MoE ì—ì´ì „íŠ¸ ìƒì„±
        agent = IntegratedMoEAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            instr_dim=CONFIG.instr_emb_dim,
            latent_dim=CONFIG.latent_dim,
            hidden_dim=CONFIG.hidden_dim
        )
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        training_stats = {
            'episodes': [],
            'rewards': [],
            'success_rates': [],
            'steps': [],
            'levels': [],
            'expert_counts': [],
            'cluster_counts': []
        }
        
        # ëª…ë ¹ì–´ í…œí”Œë¦¿
        instruction_templates = [
            "Push the blue cube to the goal position",
            "Move the cube towards the target location", 
            "Navigate to the blue object and push it to the goal",
            "Guide the cube to reach the destination",
            "Push the blue block to the target area"
        ]
        
        episode = 0
        total_episodes = 1000  # ì´ ì—í”¼ì†Œë“œ ìˆ˜
        
        print(f"\nğŸš€ Starting curriculum training for {total_episodes} episodes")
        
        while episode < total_episodes:
            # í˜„ì¬ ë ˆë²¨ ì •ë³´
            current_level = curriculum.get_current_level()
            
            # í™˜ê²½ íƒœìŠ¤í¬ ì„¤ì • ì—…ë°ì´íŠ¸
            env.update_task_config(current_level)
            
            # ëª…ë ¹ì–´ ì„ íƒ (ë‹¤ì–‘ì„±ì„ ìœ„í•´ ëœë¤)
            instruction = np.random.choice(instruction_templates)
            instr_emb = lang_processor.encode(instruction)
            
            # ì—í”¼ì†Œë“œ ì‹¤í–‰
            episode_reward, episode_steps, episode_success, episode_info = run_episode(
                env=env,
                agent=agent,
                instruction_emb=instr_emb,
                max_steps=current_level['max_steps'],
                episode_num=episode
            )
            
            # ì»¤ë¦¬í˜ëŸ¼ì— ê²°ê³¼ ì¶”ê°€
            final_distance = episode_info.get('final_cube_to_goal_distance', float('inf'))
            curriculum.add_episode_result(
                success=episode_success,
                final_distance=final_distance,
                steps=episode_steps,
                total_reward=episode_reward
            )
            
            # í†µê³„ ì €ì¥
            training_stats['episodes'].append(episode)
            training_stats['rewards'].append(episode_reward)
            training_stats['steps'].append(episode_steps)
            training_stats['levels'].append(curriculum.current_level)
            
            agent_stats = agent.get_statistics()
            training_stats['expert_counts'].append(agent_stats['num_experts'])
            training_stats['cluster_counts'].append(agent_stats['num_clusters'])
            
            # ì„±ê³µë¥  ê³„ì‚° (ìµœê·¼ 20ê°œ ì—í”¼ì†Œë“œ)
            recent_episodes = min(20, len(training_stats['episodes']))
            if recent_episodes > 0:
                recent_successes = sum(curriculum.episode_results[-recent_episodes:][i]['success'] 
                                     for i in range(recent_episodes))
                success_rate = recent_successes / recent_episodes
                training_stats['success_rates'].append(success_rate)
            else:
                training_stats['success_rates'].append(0.0)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if episode % 10 == 0 or episode_success:
                curriculum_stats = curriculum.get_statistics()
                print(f"\nğŸ“ˆ Episode {episode + 1}/{total_episodes}")
                print(f"  Level: {curriculum_stats['level_name']}")
                print(f"  Reward: {episode_reward:.2f}, Steps: {episode_steps}")
                print(f"  Success: {'âœ…' if episode_success else 'âŒ'}")
                print(f"  Recent Success Rate: {curriculum_stats['recent_success_rate']:.2%}")
                print(f"  Experts: {agent_stats['num_experts']}, Clusters: {agent_stats['num_clusters']}")
                
                if episode_info:
                    print(f"  Final Distance: {episode_info.get('final_cube_to_goal_distance', 0):.3f}")
                    print(f"  GLIP Error: {episode_info.get('glip_detection_error', 0):.3f}")
            
            # ì»¤ë¦¬í˜ëŸ¼ ë ˆë²¨ ì§„í–‰ í™•ì¸
            if curriculum.should_advance_level():
                curriculum.advance_level()
                print(f"ğŸ“ Advanced to level {curriculum.current_level + 1}")
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if (episode + 1) % args.checkpoint_freq == 0:
                checkpoint_path = os.path.join(save_dir, f"checkpoint_ep{episode + 1}.pth")
                agent.save_checkpoint(checkpoint_path)
                
                # í†µê³„ ì €ì¥
                stats_path = os.path.join(save_dir, f"stats_ep{episode + 1}.npz")
                np.savez(stats_path, **training_stats)
                print(f"ğŸ’¾ Saved checkpoint and stats at episode {episode + 1}")
            
            # í‰ê°€ ë° ì‹œê°í™”
            if (episode + 1) % args.eval_freq == 0:
                create_training_plots(training_stats, save_dir, episode + 1)
                
                # ìƒì„¸ í‰ê°€ ì‹¤í–‰
                eval_results = evaluate_agent(env, agent, lang_processor, instruction_templates)
                print(f"ğŸ” Evaluation Results:")
                for key, value in eval_results.items():
                    print(f"  {key}: {value}")
            
            episode += 1
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        final_checkpoint = os.path.join(save_dir, "final_model.pth")
        agent.save_checkpoint(final_checkpoint)
        
        final_stats = os.path.join(save_dir, "final_stats.npz")
        np.savez(final_stats, **training_stats)
        
        # ìµœì¢… ì‹œê°í™”
        create_training_plots(training_stats, save_dir, "final")
        
        # ìµœì¢… í‰ê°€
        print(f"\nğŸ† Final Evaluation:")
        final_eval = evaluate_agent(env, agent, lang_processor, instruction_templates, num_episodes=10)
        for key, value in final_eval.items():
            print(f"  {key}: {value}")
        
        print(f"\nâœ… Curriculum training completed!")
        print(f"ğŸ“ Results saved to: {save_dir}")
        
        env.close()
        
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        sim.close()

def run_episode(env, agent, instruction_emb, max_steps, episode_num):
    """ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰"""
    state, _ = env.reset()
    total_reward = 0
    steps = 0
    done = False
    
    instr_np = instruction_emb.squeeze(0).cpu().numpy()
    
    episode_experiences = []
    cluster_history = []
    
    while not done and steps < max_steps:
        # í–‰ë™ ì„ íƒ
        action, cluster_id = agent.select_action(
            state=state,
            instruction_emb=instr_np,
            deterministic=False
        )
        
        cluster_history.append(cluster_id)
        
        # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
        next_state, reward, done, info = env.step(action)
        total_reward += reward
        
        # ê²½í—˜ ì €ì¥
        agent.store_experience(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            instruction_emb=instr_np,
            cluster_id=cluster_id
        )
        
        episode_experiences.append({
            'state': state.copy(),
            'action': action.copy(),
            'reward': reward,
            'next_state': next_state.copy(),
            'done': done,
            'cluster_id': cluster_id
        })
        
        # ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸ (ë§¤ 5ìŠ¤í…ë§ˆë‹¤)
        if steps % 5 == 0 and steps > 0:
            agent.update_experts(batch_size=32)
        
        state = next_state
        steps += 1
    
    # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ì •ë³´
    success = info.get('success', False)
    final_distance = info.get('cube_to_goal_distance', float('inf'))
    
    episode_info = {
        'final_cube_to_goal_distance': final_distance,
        'glip_detection_error': info.get('glip_detection_error', 0),
        'unique_clusters': len(set(cluster_history)),
        'most_used_cluster': max(set(cluster_history), key=cluster_history.count) if cluster_history else 0,
        'reward_breakdown': info.get('reward_info', {})
    }
    
    return total_reward, steps, success, episode_info

def evaluate_agent(env, agent, lang_processor, instruction_templates, num_episodes=5):
    """ì—ì´ì „íŠ¸ í‰ê°€"""
    eval_results = {
        'success_rate': 0.0,
        'average_reward': 0.0,
        'average_steps': 0.0,
        'average_final_distance': 0.0
    }
    
    total_success = 0
    total_reward = 0
    total_steps = 0
    total_distance = 0
    
    for i in range(num_episodes):
        instruction = instruction_templates[i % len(instruction_templates)]
        instr_emb = lang_processor.encode(instruction)
        
        # ê²°ì •ì  í–‰ë™ìœ¼ë¡œ í‰ê°€
        episode_reward, episode_steps, episode_success, episode_info = run_episode(
            env=env,
            agent=agent,
            instruction_emb=instr_emb,
            max_steps=500,
            episode_num=f"eval_{i}"
        )
        
        total_success += int(episode_success)
        total_reward += episode_reward
        total_steps += episode_steps
        total_distance += episode_info.get('final_cube_to_goal_distance', 0)
    
    eval_results['success_rate'] = total_success / num_episodes
    eval_results['average_reward'] = total_reward / num_episodes
    eval_results['average_steps'] = total_steps / num_episodes
    eval_results['average_final_distance'] = total_distance / num_episodes
    
    return eval_results

def create_training_plots(stats, save_dir, episode_suffix):
    """í•™ìŠµ ì§„í–‰ ìƒí™© ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle(f'Training Progress - Episode {episode_suffix}', fontsize=16)
    
    episodes = stats['episodes']
    
    # 1. ë³´ìƒ ê·¸ë˜í”„
    axes[0, 0].plot(episodes, stats['rewards'])
    axes[0, 0].set_title('Episode Rewards')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Total Reward')
    axes[0, 0].grid(True)
    
    # 2. ì„±ê³µë¥  ê·¸ë˜í”„
    if len(stats['success_rates']) > 0:
        axes[0, 1].plot(episodes, stats['success_rates'])
        axes[0, 1].set_title('Success Rate (Recent 20 episodes)')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Success Rate')
        axes[0, 1].set_ylim(0, 1)
        axes[0, 1].grid(True)
    
    # 3. ìŠ¤í… ìˆ˜ ê·¸ë˜í”„
    axes[0, 2].plot(episodes, stats['steps'])
    axes[0, 2].set_title('Episode Steps')
    axes[0, 2].set_xlabel('Episode')
    axes[0, 2].set_ylabel('Steps')
    axes[0, 2].grid(True)
    
    # 4. ì»¤ë¦¬í˜ëŸ¼ ë ˆë²¨
    axes[1, 0].plot(episodes, stats['levels'])
    axes[1, 0].set_title('Curriculum Level')
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Level')
    axes[1, 0].grid(True)
    
    # 5. ì „ë¬¸ê°€ ìˆ˜
    axes[1, 1].plot(episodes, stats['expert_counts'])
    axes[1, 1].set_title('Number of Experts')
    axes[1, 1].set_xlabel('Episode')
    axes[1, 1].set_ylabel('Expert Count')
    axes[1, 1].grid(True)
    
    # 6. í´ëŸ¬ìŠ¤í„° ìˆ˜
    axes[1, 2].plot(episodes, stats['cluster_counts'])
    axes[1, 2].set_title('Number of Clusters')
    axes[1, 2].set_xlabel('Episode')
    axes[1, 2].set_ylabel('Cluster Count')
    axes[1, 2].grid(True)
    
    plt.tight_layout()
    
    # ì €ì¥
    plot_path = os.path.join(save_dir, f'training_progress_{episode_suffix}.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š Training plots saved to {plot_path}")

if __name__ == "__main__":
    main()