import gymnasium as gym
import numpy as np
import torch
from gymnasium.spaces import Box
from isaaclab.envs import ManagerBasedRLEnv
from isaaclab_tasks.manager_based.classic.ant.ant_env_cfg import AntEnvCfg, MySceneCfg
from config import CONFIG

class SimpleAntPushWrapper(gym.Wrapper):
    """SB3 í˜¸í™˜ Gym Wrapper - í…ì„œ/numpy ë³€í™˜ ë¬¸ì œ í•´ê²°"""
    def __init__(self, custom_cfg=None):
        # IsaacLab í™˜ê²½ ìƒì„±
        cfg = AntEnvCfg()
        cfg.scene = MySceneCfg(num_envs=1, env_spacing=cfg.scene.env_spacing)
        cfg.sim.device = str(CONFIG.device)
        cfg.seed = 42
        
        self.isaac_env = ManagerBasedRLEnv(cfg=cfg)
        
        # Gym í™˜ê²½ìœ¼ë¡œ ë˜í•‘
        super().__init__(self.isaac_env)
        
        # ì»¤ìŠ¤í…€ ì„¤ì •
        c = custom_cfg or {}
        self.cube_pos = np.array(c.get('cube_position', [2.0, 0.0, 0.1]), dtype=np.float32)
        self.goal_pos = np.array(c.get('goal_position', [3.0, 0.0, 0.1]), dtype=np.float32)
        
        # ì‹¤ì œ ê´€ì¸¡ê°’ í¬ê¸° í™•ì¸
        print("ğŸ” Checking actual observation dimensions...")
        try:
            dummy_obs_dict, _ = self.isaac_env.reset()
            dummy_base_obs = self._safe_tensor_to_numpy(dummy_obs_dict['policy'])
            
            if dummy_base_obs.ndim == 2:
                dummy_base_obs = dummy_base_obs[0]
            
            actual_base_dim = len(dummy_base_obs.flatten())
            print(f"  Actual base observation dimension: {actual_base_dim}")
            
        except Exception as e:
            print(f"  Warning: Could not determine actual obs dim: {e}")
            actual_base_dim = 60  # ê¸°ë³¸ê°’
        
        # ê´€ì¸¡ ê³µê°„ ì¬ì •ì˜
        total_obs_dim = actual_base_dim + 6  # +3 for cube, +3 for goal
        
        self.observation_space = Box(
            low=-10.0, high=10.0,
            shape=(total_obs_dim,),
            dtype=np.float32
        )
        
        # ì•¡ì…˜ ê³µê°„ ì¬ì •ì˜
        original_action_space = self.isaac_env.action_space
        if hasattr(original_action_space, 'shape'):
            if len(original_action_space.shape) == 2:
                action_dim = original_action_space.shape[1]
            else:
                action_dim = original_action_space.shape[0]
        else:
            action_dim = 8
        
        self.action_space = Box(
            low=-1.0, high=1.0,
            shape=(action_dim,),
            dtype=np.float32
        )
        
        print(f"ğŸ—ï¸ SB3 Compatible Wrapper created")
        print(f"  Obs space: {self.observation_space.shape}")
        print(f"  Action space: {self.action_space.shape}")
    
    def _safe_tensor_to_numpy(self, data):
        """ì•ˆì „í•œ í…ì„œ -> numpy ë³€í™˜"""
        if torch.is_tensor(data):
            return data.detach().cpu().numpy()
        elif isinstance(data, np.ndarray):
            return data
        else:
            return np.array(data, dtype=np.float32)
    
    def _safe_numpy_to_tensor(self, data, device=None):
        """ì•ˆì „í•œ numpy -> í…ì„œ ë³€í™˜"""
        if torch.is_tensor(data):
            return data
        else:
            tensor = torch.from_numpy(np.array(data, dtype=np.float32))
            if device:
                tensor = tensor.to(device)
            return tensor
    
    def reset(self, **kwargs):
        """ë¦¬ì…‹"""
        try:
            obs_dict, info = self.isaac_env.reset()
            obs = self._process_obs(obs_dict)
            return obs, info
        except Exception as e:
            print(f"âš ï¸ Reset error: {e}")
            dummy_obs = np.zeros(self.observation_space.shape[0], dtype=np.float32)
            return dummy_obs, {}
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… (Boolean Tensor ë¬¸ì œ ìˆ˜ì •)"""
        self.current_step += 1
    
        # ì•¡ì…˜ ì „ì²˜ë¦¬
        action = self._preprocess_action(action)
    
        try:
            if hasattr(self, 'isaac_env') and self.isaac_env is not None:
                # IsaacLab í™˜ê²½ ì‚¬ìš©
                obs_dict, reward, terminated, truncated, info = self.isaac_env.step(action)
            
                # Tensorë¥¼ ìŠ¤ì¹¼ë¼/numpyë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                if hasattr(reward, 'item'):
                    reward = reward.item()
                elif hasattr(reward, 'cpu'):
                    reward = reward.cpu().numpy().item()
            
                if hasattr(terminated, 'item'):
                    terminated = terminated.item()
                elif hasattr(terminated, 'cpu'):
                    terminated = terminated.cpu().numpy().item()
            
                if hasattr(truncated, 'item'):
                    truncated = truncated.item()
                elif hasattr(truncated, 'cpu'):
                    truncated = truncated.cpu().numpy().item()
            
                # ê´€ì¸¡ê°’ ì²˜ë¦¬
                next_obs = self._process_isaac_obs(obs_dict)
            
            else:
                # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
                next_obs, reward, terminated, truncated = self._simulate_step(action)
                info = {}
            
        except Exception as e:
            print(f"âš ï¸ Environment step error: {e}")
            # í´ë°±
            next_obs = self._generate_simulation_obs()
            reward = 0.0
            terminated = False
            truncated = False
            info = {}
    
        # ì—í”¼ì†Œë“œ ê¸¸ì´ ì œí•œ
        if self.current_step >= self.max_episode_steps:
            truncated = True
    
        # ë³´ìƒ ê³„ì‚°
        custom_reward = self._calculate_reward(next_obs)
        total_reward = float(reward) + custom_reward
    
        self._current_obs = next_obs
    
        info.update({
            'episode_step': self.current_step,
            'max_steps_reached': self.current_step >= self.max_episode_steps
        })
    
        # ëª¨ë“  ë°˜í™˜ê°’ì´ ì˜¬ë°”ë¥¸ íƒ€ì…ì¸ì§€ í™•ì¸
        return (
            next_obs.astype(np.float32), 
            float(total_reward), 
            bool(terminated), 
            bool(truncated), 
            info
        )
    
    def _process_obs(self, obs_dict):
        """ê´€ì¸¡ê°’ ì²˜ë¦¬"""
        try:
            base_obs = self._safe_tensor_to_numpy(obs_dict['policy'])
            
            # ë‹¤ì¤‘ í™˜ê²½ ì²˜ë¦¬
            if base_obs.ndim == 2:
                base_obs = base_obs[0]
            
            # 1ì°¨ì›ìœ¼ë¡œ í‰íƒ„í™”
            base_obs = base_obs.flatten()
            
            # cube/goal ìœ„ì¹˜ ì¶”ê°€
            cube_pos = self.cube_pos
            goal_pos = self.goal_pos
            
            # ì „ì²´ ê´€ì¸¡ê°’ êµ¬ì„±
            full_obs = np.concatenate([base_obs, cube_pos, goal_pos])
            
            # í¬ê¸° ì¡°ì •
            expected_size = self.observation_space.shape[0]
            actual_size = len(full_obs)
            
            if actual_size != expected_size:
                if actual_size < expected_size:
                    padding = np.zeros(expected_size - actual_size, dtype=np.float32)
                    full_obs = np.concatenate([full_obs, padding])
                else:
                    full_obs = full_obs[:expected_size]
            
            # ë²”ìœ„ ì œí•œ
            full_obs = np.clip(full_obs, -10.0, 10.0)
            
            return full_obs.astype(np.float32)
            
        except Exception as e:
            print(f"âš ï¸ Obs processing error: {e}")
            return np.zeros(self.observation_space.shape[0], dtype=np.float32)
    
    def _compute_reward(self, obs):
        """ë³´ìƒ ê³„ì‚°"""
        try:
            if len(obs) >= 6:
                cube_pos = obs[-6:-3]
                goal_pos = obs[-3:]
                
                distance = np.linalg.norm(cube_pos - goal_pos)
                reward = -distance
                
                # ì¶”ê°€ ë³´ìƒ: antê°€ cubeì— ê°€ê¹Œì´ ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
                if len(obs) >= 9:  # ant ìœ„ì¹˜ê°€ ìˆë‹¤ë©´
                    ant_pos = obs[:3]  # ì²« 3ê°œ ì°¨ì›ì„ ant ìœ„ì¹˜ë¡œ ê°€ì •
                    ant_to_cube_dist = np.linalg.norm(ant_pos - cube_pos)
                    reward += -0.1 * ant_to_cube_dist  # antê°€ cubeì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë³´ë„ˆìŠ¤
                
                reward = np.clip(reward, -10.0, 10.0)
                return reward
            else:
                return 0.0
                
        except Exception as e:
            print(f"âš ï¸ Reward computation error: {e}")
            return 0.0
    
    def close(self):
        """í™˜ê²½ ì¢…ë£Œ"""
        try:
            self.isaac_env.close()
            print("âœ… Environment closed")
        except Exception as e:
            print(f"âš ï¸ Close error: {e}")