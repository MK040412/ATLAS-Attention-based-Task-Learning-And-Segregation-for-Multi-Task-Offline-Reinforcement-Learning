import torch
import torch.nn as nn
import numpy as np

class DimensionAwareFusion(nn.Module):
    """ì°¨ì›ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ìœµí•© ëª¨ë“ˆ (ìˆ˜ì •ë¨)"""
    
    def __init__(self, instr_dim, state_dim, latent_dim):
        super().__init__()
        self.instr_dim = instr_dim
        self.state_dim = state_dim
        self.latent_dim = latent_dim
        
        print(f"ğŸ”— DimensionAwareFusion init:")
        print(f"  Instruction dim: {instr_dim}")
        print(f"  State dim: {state_dim}")
        print(f"  Latent dim: {latent_dim}")
        
        # ì§€ì‹œì‚¬í•­ í”„ë¡œì ì…˜
        self.instr_proj = nn.Sequential(
            nn.Linear(instr_dim, latent_dim // 2),
            nn.ReLU(),
            nn.Linear(latent_dim // 2, latent_dim // 2)
        )
        
        # ìƒíƒœ í”„ë¡œì ì…˜
        self.state_proj = nn.Sequential(
            nn.Linear(state_dim, latent_dim // 2),
            nn.ReLU(),
            nn.Linear(latent_dim // 2, latent_dim // 2)
        )
        
        # ìœµí•© ë ˆì´ì–´
        self.fusion = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim)
        )
    
    def _to_tensor_safe(self, data, target_device):
        """ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ í…ì„œë¡œ ë³€í™˜"""
        try:
            if torch.is_tensor(data):
                return data.to(target_device)
            elif isinstance(data, np.ndarray):
                return torch.from_numpy(data).float().to(target_device)
            else:
                return torch.tensor(data, dtype=torch.float32, device=target_device)
        except Exception as e:
            print(f"Warning: Tensor conversion failed: {e}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            return torch.zeros(1, device=target_device, dtype=torch.float32)
        
    def _safe_reshape(self, tensor, target_dim, name="tensor"):
        """í…ì„œë¥¼ ì•ˆì „í•˜ê²Œ ë¦¬ì…°ì´í”„"""
        try:
            if tensor.dim() == 1:
                # 1D í…ì„œì¸ ê²½ìš° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                tensor = tensor.unsqueeze(0)
            elif tensor.dim() == 3:
                # 3D í…ì„œì¸ ê²½ìš° ì‹œí€€ìŠ¤ ì°¨ì›ì—ì„œ í‰ê· 
                if tensor.size(0) == 1:
                    tensor = tensor.squeeze(0)  # (1, seq, dim) -> (seq, dim)
                tensor = tensor.mean(dim=0, keepdim=True)  # (seq, dim) -> (1, dim)
            elif tensor.dim() == 2:
                # 2D í…ì„œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                pass
            else:
                print(f"Warning: Unexpected {name} dimension: {tensor.shape}")
                tensor = tensor.view(1, -1)
            
            # ë§ˆì§€ë§‰ ì°¨ì›ì´ target_dimê³¼ ë§ëŠ”ì§€ í™•ì¸
            if tensor.size(-1) != target_dim:
                print(f"Warning: {name} last dim mismatch. Expected {target_dim}, got {tensor.size(-1)}")
                # ì ì‘ì  ì²˜ë¦¬
                if tensor.size(-1) > target_dim:
                    tensor = tensor[..., :target_dim]  # ì˜ë¼ë‚´ê¸°
                else:
                    # íŒ¨ë”©
                    pad_size = target_dim - tensor.size(-1)
                    padding = torch.zeros(*tensor.shape[:-1], pad_size, device=tensor.device)
                    tensor = torch.cat([tensor, padding], dim=-1)
            
            return tensor
            
        except Exception as e:
            print(f"Error in reshaping {name}: {e}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            device = tensor.device if torch.is_tensor(tensor) else next(self.parameters()).device
            return torch.zeros(1, target_dim, device=device)
    
    def forward(self, instr_emb, state_obs):
        """ìˆœì „íŒŒ"""
        try:
            # ë””ë°”ì´ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            device = next(self.parameters()).device
            
            # ì…ë ¥ì„ ì•ˆì „í•˜ê²Œ í…ì„œë¡œ ë³€í™˜
            instr_tensor = self._to_tensor_safe(instr_emb, device)
            state_tensor = self._to_tensor_safe(state_obs, device)
            
            # ì•ˆì „í•œ ë¦¬ì…°ì´í”„
            instr_tensor = self._safe_reshape(instr_tensor, self.instr_dim, "instruction")
            state_tensor = self._safe_reshape(state_tensor, self.state_dim, "state")
            
            # í”„ë¡œì ì…˜
            instr_proj = self.instr_proj(instr_tensor)  # (batch, latent_dim//2)
            state_proj = self.state_proj(state_tensor)  # (batch, latent_dim//2)
            
            # ì—°ê²°
            combined = torch.cat([instr_proj, state_proj], dim=-1)  # (batch, latent_dim)
            
            # ìœµí•©
            fused = self.fusion(combined)  # (batch, latent_dim)
            
            return fused.squeeze(0) if fused.size(0) == 1 else fused
            
        except Exception as e:
            print(f"Error in DimensionAwareFusion forward: {e}")
            import traceback
            traceback.print_exc()
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            device = next(self.parameters()).device
            return torch.zeros(self.latent_dim, device=device)