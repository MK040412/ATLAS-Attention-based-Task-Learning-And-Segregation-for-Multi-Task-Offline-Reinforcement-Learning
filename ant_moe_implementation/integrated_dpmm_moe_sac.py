import torch
import torch.nn as nn
import numpy as np
from typing import List, Dict, Any
from kl_divergence_dpmm import KLDivergenceDPMM
from stable_baselines3 import SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env
import gymnasium as gym
from gymnasium.spaces import Box

class FusionEncoder(nn.Module):
    def __init__(self, instr_dim, state_dim, latent_dim, hidden_dim=256):
        super().__init__()
        self.instr_dim = instr_dim
        self.state_dim = state_dim
        self.latent_dim = latent_dim
        
        input_dim = instr_dim + state_dim
        
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, latent_dim)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        for layer in self.mlp:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, instr_emb, state_obs):
        # ì°¨ì› ì •ë¦¬
        if instr_emb.dim() == 3:
            instr_emb = instr_emb.squeeze(0).mean(dim=0)
        elif instr_emb.dim() == 2:
            if instr_emb.size(0) == 1:
                instr_emb = instr_emb.squeeze(0)
            else:
                instr_emb = instr_emb.mean(dim=0)
        
        if state_obs.dim() == 2:
            if state_obs.size(0) == 1:
                state_obs = state_obs.squeeze(0)
            else:
                state_obs = state_obs[0]
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if instr_emb.dim() == 1:
            instr_emb = instr_emb.unsqueeze(0)
        if state_obs.dim() == 1:
            state_obs = state_obs.unsqueeze(0)
        
        # ì—°ê²° ë° ìœµí•©
        combined = torch.cat([instr_emb, state_obs], dim=-1)
        z = self.mlp(combined)
        
        return z.squeeze(0)

class DummyEnvWrapper(gym.Env):
    """SACë¥¼ ìœ„í•œ ë”ë¯¸ í™˜ê²½ ë˜í¼"""
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(state_dim,), dtype=np.float32)
        self.action_space = Box(low=-1, high=1, shape=(action_dim,), dtype=np.float32)
        self.state_dim = state_dim
        self.action_dim = action_dim
        
    def reset(self, seed=None, options=None):
        state = np.zeros(self.state_dim, dtype=np.float32)
        return state, {}
    
    def step(self, action):
        next_state = np.zeros(self.state_dim, dtype=np.float32)
        reward = 0.0
        done = False
        truncated = False
        info = {}
        return next_state, reward, done, truncated, info

class StableBaselines3SACExpert:
    """Stable Baselines3 SACë¥¼ ì‚¬ìš©í•œ ì „ë¬¸ê°€"""
    def __init__(self, state_dim, action_dim, expert_id=0):
        self.expert_id = expert_id
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # ë”ë¯¸ í™˜ê²½ ìƒì„± (SAC ì´ˆê¸°í™”ìš©)
        dummy_env = DummyEnvWrapper(state_dim, action_dim)
        
        # SAC ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.sac = SAC(
            "MlpPolicy",
            dummy_env,
            learning_rate=3e-4,
            buffer_size=10000,
            learning_starts=100,
            batch_size=64,
            tau=0.005,
            gamma=0.99,
            train_freq=1,
            gradient_steps=1,
            ent_coef='auto',
            target_update_interval=1,
            target_entropy='auto',
            use_sde=False,
            sde_sample_freq=-1,
            use_sde_at_warmup=False,
            tensorboard_log=None,
            policy_kwargs=dict(net_arch=[256, 256]),
            verbose=0,
            seed=None,
            device='auto',
            _init_setup_model=True
        )
        
        # ê²½í—˜ ì €ì¥ì„ ìœ„í•œ ë²„í¼
        self.experience_buffer = []
        
        print(f"ğŸ¤– SAC Expert {expert_id} initialized")
        print(f"  State dim: {state_dim}, Action dim: {action_dim}")
    
    def select_action(self, state, deterministic=False):
        """í–‰ë™ ì„ íƒ"""
        if isinstance(state, np.ndarray):
            state_tensor = state
        else:
            state_tensor = np.array(state, dtype=np.float32)
        
        # SACë¡œ í–‰ë™ ì˜ˆì¸¡
        action, _ = self.sac.predict(state_tensor, deterministic=deterministic)
        return action
    
    def store_experience(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.experience_buffer.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done
        })
        
        # ë²„í¼ê°€ ë„ˆë¬´ í¬ë©´ ì˜¤ë˜ëœ ê²½í—˜ ì œê±°
        if len(self.experience_buffer) > 10000:
            self.experience_buffer.pop(0)
    
    def update(self):
        """SAC ì—…ë°ì´íŠ¸"""
        if len(self.experience_buffer) < 100:  # ìµœì†Œ ê²½í—˜ ìˆ˜
            return {'loss': 0.0}
        
        # ìµœê·¼ ê²½í—˜ë“¤ì„ SAC replay bufferì— ì¶”ê°€
        for exp in self.experience_buffer[-64:]:  # ìµœê·¼ 64ê°œë§Œ
            self.sac.replay_buffer.add(
                exp['state'],
                exp['next_state'],
                exp['action'],
                exp['reward'],
                exp['done'],
                [{}]  # infos
            )
        
        # SAC í•™ìŠµ
        if self.sac.replay_buffer.size() >= self.sac.learning_starts:
            self.sac.train(gradient_steps=1, batch_size=self.sac.batch_size)
        
        return {'loss': 0.0}  # ì‹¤ì œ lossëŠ” SAC ë‚´ë¶€ì—ì„œ ì²˜ë¦¬

class IntegratedKLDPMMMoESAC:
    """KL Divergence DPMM + MoE-SAC í†µí•© ì—ì´ì „íŠ¸"""
    def __init__(self, state_dim, action_dim, instr_dim, latent_dim=32,
                 tau_birth=1e-4, tau_merge_kl=0.1, device='cpu'):
        self.device = torch.device(device)
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.instr_dim = instr_dim
        
        # KL-Divergence ê¸°ë°˜ DPMM
        self.dpmm = KLDivergenceDPMM(
            latent_dim=latent_dim,
            tau_birth=tau_birth,
            tau_merge_kl=tau_merge_kl
        )
        
        # Fusion Encoder
        self.fusion_encoder = FusionEncoder(
            instr_dim, state_dim, latent_dim
        ).to(self.device)
        
        # SAC ì „ë¬¸ê°€ë“¤
        self.experts: List[StableBaselines3SACExpert] = []
        
        # í†µê³„ ì¶”ì 
        self.assignment_history = []
        self.expert_usage_count = {}
        
        print(f"ğŸ¤– Integrated KL-DPMM-MoE-SAC Agent initialized")
        print(f"  State: {state_dim}, Action: {action_dim}, Instruction: {instr_dim}")
        print(f"  Latent: {latent_dim}, Device: {device}")
    
    def _create_new_expert(self) -> int:
        """ìƒˆë¡œìš´ SAC ì „ë¬¸ê°€ ìƒì„±"""
        expert_id = len(self.experts)
        expert = StableBaselines3SACExpert(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            expert_id=expert_id
        )
        self.experts.append(expert)
        self.expert_usage_count[expert_id] = 0
        
        print(f"ğŸ£ Created new SAC expert {expert_id}")
        return expert_id
    
    def select_action(self, state, instruction_emb, deterministic=False):
        """í–‰ë™ ì„ íƒ ë° í´ëŸ¬ìŠ¤í„° í• ë‹¹"""
        # ìƒíƒœì™€ ì§€ì‹œì‚¬í•­ì„ latent spaceë¡œ ë§¤í•‘
        with torch.no_grad():
            if isinstance(state, np.ndarray):
                state_tensor = torch.FloatTensor(state).to(self.device)
            else:
                state_tensor = state.to(self.device)
            
            if isinstance(instruction_emb, np.ndarray):
                instr_tensor = torch.FloatTensor(instruction_emb).to(self.device)
            else:
                instr_tensor = instruction_emb.to(self.device)
            
            # Fusion encoding
            z = self.fusion_encoder(instr_tensor, state_tensor)
            z_np = z.detach().cpu().numpy()
        
        # DPMM í´ëŸ¬ìŠ¤í„° í• ë‹¹
        cluster_id = self.dpmm.assign(z_np)
        
        # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ëŒ€ì‘í•˜ëŠ” ì „ë¬¸ê°€ê°€ ì—†ìœ¼ë©´ ìƒì„±
        if cluster_id >= len(self.experts):
            while len(self.experts) <= cluster_id:
                self._create_new_expert()
        
        # ì „ë¬¸ê°€ë¡œë¶€í„° í–‰ë™ ì„ íƒ
        expert = self.experts[cluster_id]
        action = expert.select_action(state, deterministic=deterministic)
        
        # ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
        self.expert_usage_count[cluster_id] += 1
        self.assignment_history.append({
            'cluster_id': cluster_id,
            'expert_id': cluster_id,
            'z': z_np.copy()
        })
        
        return action, cluster_id
    
    def store_experience(self, state, action, reward, next_state, done, cluster_id):
        """ê²½í—˜ ì €ì¥"""
        if cluster_id < len(self.experts):
            expert = self.experts[cluster_id]
            expert.store_experience(state, action, reward, next_state, done)
    
    def update_experts(self):
        """ëª¨ë“  ì „ë¬¸ê°€ ì—…ë°ì´íŠ¸"""
        total_loss = 0.0
        updated_count = 0
        
        for i, expert in enumerate(self.experts):
            if len(expert.experience_buffer) >= 10:  # ìµœì†Œ ê²½í—˜ ìˆ˜
                loss_info = expert.update()
                total_loss += loss_info.get('loss', 0.0)
                updated_count += 1
        
        return {
            'total_loss': total_loss,
            'updated_experts': updated_count,
            'total_experts': len(self.experts)
        }
    
    def merge_clusters(self, verbose=False):
        """KL divergence ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ë³‘í•©"""
        old_count = len(self.dpmm.clusters)
        self.dpmm.merge(verbose=verbose)
        new_count = len(self.dpmm.clusters)
        
        if new_count < old_count:
            if verbose:
                print(f"  ğŸ”— KL-Merge: {old_count} â†’ {new_count} clusters")
            return True
        return False
    
    def get_statistics(self):
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        cluster_info = self.dpmm.get_cluster_info()
        
        return {
            'dpmm': cluster_info,
            'experts': {
                'total_experts': len(self.experts),
                'expert_usage': self.expert_usage_count.copy(),
                'total_assignments': len(self.assignment_history)
            },
            'recent_assignments': self.assignment_history[-10:] if self.assignment_history else []
        }
    
    def save_checkpoint(self, filepath):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'fusion_encoder': self.fusion_encoder.state_dict(),
            'dpmm_clusters': {
                'clusters': self.dpmm.clusters,
                'next_cluster_id': self.dpmm.next_cluster_id,
                'birth_events': self.dpmm.birth_events,
                'merge_events': self.dpmm.merge_events
            },
            'expert_count': len(self.experts),
            'assignment_history': self.assignment_history,
            'expert_usage_count': self.expert_usage_count,
            'config': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'instr_dim': self.instr_dim,
                'latent_dim': self.dpmm.latent_dim,
                'tau_birth': self.dpmm.tau_birth,
                'tau_merge_kl': self.dpmm.tau_merge_kl
            }
        }
        
        # SAC ì „ë¬¸ê°€ë“¤ ì €ì¥
        expert_models = {}
        for i, expert in enumerate(self.experts):
            expert_models[f'expert_{i}'] = {
                'policy': expert.sac.policy.state_dict(),
                'experience_count': len(expert.experience_buffer)
            }
        checkpoint['experts'] = expert_models
        
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ Checkpoint saved to {filepath}")
    
    def load_checkpoint(self, filepath):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # Fusion encoder ë¡œë“œ
        self.fusion_encoder.load_state_dict(checkpoint['fusion_encoder'])
        
        # DPMM ìƒíƒœ ë³µì›
        dpmm_data = checkpoint['dpmm_clusters']
        self.dpmm.clusters = dpmm_data['clusters']
        self.dpmm.next_cluster_id = dpmm_data['next_cluster_id']
        self.dpmm.birth_events = dpmm_data['birth_events']
        self.dpmm.merge_events = dpmm_data['merge_events']
        
        # ì „ë¬¸ê°€ë“¤ ë³µì›
        expert_count = checkpoint['expert_count']
        self.experts = []
        expert_models = checkpoint['experts']
        
        for i in range(expert_count):
            expert = StableBaselines3SACExpert(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                expert_id=i
            )
            
            # SAC ì •ì±… ë¡œë“œ
            if f'expert_{i}' in expert_models:
                expert.sac.policy.load_state_dict(expert_models[f'expert_{i}']['policy'])
            
            self.experts.append(expert)
        
        # ê¸°íƒ€ ìƒíƒœ ë³µì›
        self.assignment_history = checkpoint['assignment_history']
        self.expert_usage_count = checkpoint['expert_usage_count']
        
        print(f"ğŸ“‚ Checkpoint loaded from {filepath}")
        print(f"  Restored {len(self.experts)} experts and {len(self.dpmm.clusters)} clusters")