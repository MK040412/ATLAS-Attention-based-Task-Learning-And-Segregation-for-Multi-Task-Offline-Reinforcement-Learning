#!/usr/bin/env python3
"""
SB3 ê¸°ë°˜ MoE SAC í†µí•© í•™ìŠµ (í™˜ê²½ ì¬ì‚¬ìš© ë²„ì „)
"""

import argparse
import os
import yaml
import numpy as np
import torch
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser(description="SB3 MoE SAC Training")
    parser.add_argument("--tasks", type=str, default="tasks.yaml")
    parser.add_argument("--save_dir", type=str, default="./sb3_moe_outputs")
    parser.add_argument("--episodes_per_task", type=int, default=50)
    parser.add_argument("--max_steps", type=int, default=500)
    parser.add_argument("--update_freq", type=int, default=10)
    parser.add_argument("--cluster_threshold", type=float, default=2.0)
    parser.add_argument("--latent_dim", type=int, default=32)
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    try:
        from fixed_simple_gym_wrapper import SimpleAntPushWrapper
        from sb3_moe_agent import SB3MoEAgent
        from language_processor import LanguageProcessor
        from config import CONFIG

        print(f"ğŸš€ SB3 MoE SAC Training (Single Environment)")
        print(f"  Tasks file: {args.tasks}")
        print(f"  Episodes per task: {args.episodes_per_task}")
        print(f"  Max steps per episode: {args.max_steps}")
        print(f"  Cluster threshold: {args.cluster_threshold}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(args.save_dir, exist_ok=True)
        
        # íƒœìŠ¤í¬ ë¡œë“œ
        if os.path.exists(args.tasks):
            tasks = yaml.safe_load(open(args.tasks))
        else:
            # ê¸°ë³¸ íƒœìŠ¤í¬ (ë™ì¼í•œ í™˜ê²½ ì„¤ì •ìœ¼ë¡œ í†µì¼)
            base_env_cfg = {
                'cube_position': [2.0, 0.0, 0.1],
                'goal_position': [3.0, 0.0, 0.1]
            }
            tasks = [
                {
                    'name': 'push_cube_basic',
                    'instruction': 'Push the blue cube to the goal position',
                    'env_cfg': base_env_cfg,
                    'episodes': args.episodes_per_task
                },
                {
                    'name': 'push_cube_far',
                    'instruction': 'Move the cube to the distant target',
                    'env_cfg': base_env_cfg,  # ê°™ì€ í™˜ê²½ ì‚¬ìš©
                    'episodes': args.episodes_per_task
                },
                {
                    'name': 'push_cube_left',
                    'instruction': 'Push the cube to the left side',
                    'env_cfg': base_env_cfg,  # ê°™ì€ í™˜ê²½ ì‚¬ìš©
                    'episodes': args.episodes_per_task
                },
                {
                    'name': 'push_cube_different',
                    'instruction': 'Move the object to the target location',
                    'env_cfg': base_env_cfg,  # ê°™ì€ í™˜ê²½ ì‚¬ìš©
                    'episodes': args.episodes_per_task
                }
            ]
        
        # ì–¸ì–´ ì²˜ë¦¬ê¸°
        lang_processor = LanguageProcessor(device=CONFIG.device)
        
        # ë‹¨ì¼ í™˜ê²½ìœ¼ë¡œ ëª¨ë“  íƒœìŠ¤í¬ ì²˜ë¦¬
        env = SimpleAntPushWrapper(custom_cfg=tasks[0]['env_cfg'])
        first_instr_emb = lang_processor.encode(tasks[0]['instruction'])
        instr_dim = first_instr_emb.shape[-1]
        
        # MoE ì—ì´ì „íŠ¸ ìƒì„±
        agent = SB3MoEAgent(
            env=env,
            instr_dim=instr_dim,
            latent_dim=args.latent_dim,
            cluster_threshold=args.cluster_threshold
        )
        
        # í•™ìŠµ í†µê³„
        all_stats = {
            'task_rewards': [],
            'cluster_evolution': [],
            'expert_counts': []
        }
        
        # íƒœìŠ¤í¬ë³„ í•™ìŠµ (ê°™ì€ í™˜ê²½ ì¬ì‚¬ìš©)
        for task_idx, task in enumerate(tasks):
            print(f"\n{'='*60}")
            print(f"ğŸ¯ Task {task_idx + 1}/{len(tasks)}: {task['name']}")
            print(f"ğŸ“ Instruction: {task['instruction']}")
            print(f"{'='*60}")
            
            # ì§€ì‹œì‚¬í•­ ì„ë² ë”©
            instr_emb = lang_processor.encode(task['instruction'])
            
            # íƒœìŠ¤í¬ í†µê³„
            task_rewards = []
            
            # ì—í”¼ì†Œë“œ í•™ìŠµ
            for episode in range(task['episodes']):
                print(f"\n--- Episode {episode + 1}/{task['episodes']} ---")
                
                # í™˜ê²½ ë¦¬ì…‹ (ê°™ì€ í™˜ê²½ ì¬ì‚¬ìš©)
                state, _ = env.reset()
                episode_reward = 0
                step_count = 0
                
                while step_count < args.max_steps:
                    # í–‰ë™ ì„ íƒ
                    action, cluster_id = agent.select_action(
                        state, 
                        instr_emb.squeeze(0).detach().cpu().numpy(),
                        deterministic=False
                    )
                    
                    if step_count == 0:
                        print(f"  Assigned to cluster/expert: {cluster_id}")
                    
                    # í™˜ê²½ ìŠ¤í…
                    next_state, reward, done, truncated, info = env.step(action)
                    episode_reward += reward
                    
                    # ê²½í—˜ ì €ì¥
                    agent.store_experience(state, action, reward, next_state, done, cluster_id)
                    
                    state = next_state
                    step_count += 1
                    
                    if done or truncated:
                        break
                
                # ì£¼ê¸°ì  ì—…ë°ì´íŠ¸
                if episode % args.update_freq == 0:
                    update_results = agent.update_experts(batch_size=32)
                    if update_results:
                        print(f"  ğŸ“ˆ Updated experts: {list(update_results.keys())}")
                
                # í†µê³„ ìˆ˜ì§‘
                task_rewards.append(episode_reward)
                agent_info = agent.get_info()
                
                # ì£¼ê¸°ì  ì¶œë ¥
                if episode % 10 == 0 or episode == task['episodes'] - 1:
                    print(f"  ğŸ“Š Episode {episode + 1} Summary:")
                    print(f"    Reward: {episode_reward:.2f}")
                    print(f"    Steps: {step_count}")
                    print(f"    Clusters: {agent_info['dpmm']['num_clusters']}")
                    print(f"    Experts: {agent_info['num_experts']}")
                    print(f"    Total buffer size: {agent_info['total_buffer_size']}")
                    
                    # ìµœê·¼ í‰ê·  ë³´ìƒ
                    recent_rewards = task_rewards[-10:]
                    if recent_rewards:
                        print(f"    Recent avg reward: {np.mean(recent_rewards):.2f}")
            
            # íƒœìŠ¤í¬ ì™„ë£Œ í›„ í†µê³„
            all_stats['task_rewards'].append(task_rewards)
            all_stats['cluster_evolution'].append(agent.get_info()['dpmm'])
            all_stats['expert_counts'].append(agent.get_info()['num_experts'])
            
            print(f"\nğŸ Task '{task['name']}' completed!")
            print(f"  Average reward: {np.mean(task_rewards):.2f}")
            print(f"  Best reward: {max(task_rewards):.2f}")
            print(f"  Final clusters: {agent.get_info()['dpmm']['num_clusters']}")
            print(f"  Final experts: {agent.get_info()['num_experts']}")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_save_path = os.path.join(args.save_dir, "final_sb3_moe_model")
        agent.save(final_save_path)
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        stats_path = os.path.join(args.save_dir, "training_stats.npz")
        np.savez(stats_path, **all_stats)
        
        # ìµœì¢… ìš”ì•½
        print(f"\nğŸ‰ All tasks completed!")
        print(f"  Total episodes: {sum(len(rewards) for rewards in all_stats['task_rewards'])}")
        all_rewards = [r for task_rewards in all_stats['task_rewards'] for r in task_rewards]
        print(f"  Overall average reward: {np.mean(all_rewards):.2f}")
        print(f"  Final clusters: {all_stats['cluster_evolution'][-1]['num_clusters']}")
        print(f"  Final experts: {all_stats['expert_counts'][-1]}")
        print(f"  Model saved to: {final_save_path}")
        
        env.close()
        
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        sim.close()

if __name__ == "__main__":
    main()