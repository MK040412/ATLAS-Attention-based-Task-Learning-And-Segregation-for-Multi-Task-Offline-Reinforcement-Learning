#!/usr/bin/env python3
"""
SB3 ê¸°ë°˜ ì—„ë°€í•œ DPMM + MoE-SAC í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ìˆ˜ì •ë¨)
"""

import argparse
import torch
import numpy as np
from isaaclab.app import AppLauncher

def main():
    parser = argparse.ArgumentParser(description="SB3-based Rigorous DPMM-MoE-SAC Training")
    parser.add_argument("--episodes", type=int, default=100)
    parser.add_argument("--max_steps", type=int, default=500)
    parser.add_argument("--latent_dim", type=int, default=32)
    parser.add_argument("--tau_birth", type=float, default=1e-4)
    parser.add_argument("--tau_merge", type=float, default=0.5)
    parser.add_argument("--save_checkpoint", type=str, default="rigorous_sb3_checkpoint.pth")
    AppLauncher.add_app_launcher_args(parser)
    args = parser.parse_args()

    app = AppLauncher(vars(args))
    sim = app.app

    try:
        # SB3 ê¸°ë°˜ ëª¨ë“ˆë“¤ import
        from fixed_simple_gym_wrapper import SimpleAntPushWrapper
        from sb3_moe_agent import SB3MoEAgent
        from language_processor import LanguageProcessor
        from config import CONFIG

        print(f"ğŸš€ Starting SB3-based Rigorous DPMM-MoE-SAC Training")
        print(f"  Episodes: {args.episodes}")
        print(f"  Max steps per episode: {args.max_steps}")
        print(f"  Latent dimension: {args.latent_dim}")
        print(f"  Birth threshold: {args.tau_birth}")
        print(f"  Merge threshold: {args.tau_merge}")

        # í™˜ê²½ ì„¤ì • (SB3 í˜¸í™˜)
        env_config = {
            'cube_position': [2.0, 0.0, 0.1],
            'goal_position': [3.0, 0.0, 0.1]
        }
        env = SimpleAntPushWrapper(custom_cfg=env_config)
        
        # ì–¸ì–´ ì²˜ë¦¬ê¸°
        lang_processor = LanguageProcessor(device=CONFIG.device)
        instruction = "Push the blue cube to the goal position"
        instruction_emb = lang_processor.encode(instruction)
        instr_dim = instruction_emb.shape[-1]
        
        print(f"ğŸ“Š Environment Dimensions:")
        print(f"  State dimension: {env.observation_space.shape[0]}")
        print(f"  Action dimension: {env.action_space.shape[0]}")
        print(f"  Instruction dimension: {instr_dim}")

        # SB3 ê¸°ë°˜ MoE ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©)
        agent = SB3MoEAgent(
            env=env,
            instr_dim=instr_dim,
            latent_dim=args.latent_dim,
            cluster_threshold=args.tau_merge  # SB3 ë²„ì „ì—ì„œ ì‚¬ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°ëª…
        )
        
        # ì—ì´ì „íŠ¸ì— ì¶”ê°€ íŒŒë¼ë¯¸í„° ì„¤ì • (tau_birthë¥¼ ë³„ë„ë¡œ ì„¤ì •)
        if hasattr(agent, 'set_dpmm_params'):
            agent.set_dpmm_params(tau_birth=args.tau_birth, tau_merge=args.tau_merge)
        elif hasattr(agent, 'dpmm'):
            agent.dpmm.tau_birth = args.tau_birth
            agent.dpmm.tau_merge_kl = args.tau_merge

        print(f"âœ… SB3 MoE Agent initialized successfully")
        print(f"  Agent type: {type(agent).__name__}")
        
        # ì—ì´ì „íŠ¸ ì •ë³´ í™•ì¸
        if hasattr(agent, 'get_info'):
            agent_info = agent.get_info()
            print(f"  Initial clusters: {agent_info.get('dpmm', {}).get('num_clusters', 'N/A')}")
            print(f"  Initial experts: {agent_info.get('num_experts', 'N/A')}")

        # ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„ (ë¨¼ì € ê¸°ë³¸ ë™ì‘ í™•ì¸)
        training_stats = simple_sb3_training_loop(
            env=env,
            agent=agent,
            instruction_emb=instruction_emb,
            max_episodes=args.episodes,
            max_steps_per_episode=args.max_steps,
            update_frequency=10
        )

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ‰ Training completed!")
        print(f"  Total episodes: {len(training_stats['episode_rewards'])}")
        print(f"  Average reward: {np.mean(training_stats['episode_rewards']):.2f}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if args.save_checkpoint:
            agent.save(args.save_checkpoint)
            print(f"ğŸ’¾ SB3 Checkpoint saved to {args.save_checkpoint}")

    except Exception as e:
        print(f"âŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        env.close()
        sim.close()

def simple_sb3_training_loop(env, agent, instruction_emb, max_episodes, max_steps_per_episode, update_frequency):
    """ê°„ë‹¨í•œ SB3 ê¸°ë°˜ í•™ìŠµ ë£¨í”„"""
    
    training_stats = {
        'episode_rewards': [],
        'cluster_evolution': [],
        'expert_counts': []
    }
    
    for episode in range(max_episodes):
        if episode % 10 == 0:
            print(f"\nğŸ¯ Episode {episode + 1}/{max_episodes}")
        
        state, _ = env.reset()
        episode_reward = 0
        step_count = 0
        
        while step_count < max_steps_per_episode:
            try:
                # SB3 ê¸°ë°˜ í–‰ë™ ì„ íƒ
                action, cluster_id = agent.select_action(
                    state, 
                    instruction_emb.squeeze(0).detach().cpu().numpy(),
                    deterministic=False
                )
                
                # í™˜ê²½ ìŠ¤í…
                next_state, reward, done, truncated, info = env.step(action)
                episode_reward += reward
                
                # ê²½í—˜ ì €ì¥
                agent.store_experience(state, action, reward, next_state, done, cluster_id)
                
                state = next_state
                step_count += 1
                
                if done or truncated:
                    break
                    
            except Exception as e:
                print(f"  âš ï¸ Step error at episode {episode}, step {step_count}: {e}")
                break
        
        # ì£¼ê¸°ì  ì—…ë°ì´íŠ¸
        if episode % update_frequency == 0 and episode > 0:
            try:
                update_results = agent.update_experts(batch_size=32)
                if update_results and episode % 20 == 0:
                    print(f"  ğŸ“ˆ Updated experts: {list(update_results.keys())}")
            except Exception as e:
                print(f"  âš ï¸ Update error at episode {episode}: {e}")
        
        # í†µê³„ ìˆ˜ì§‘
        training_stats['episode_rewards'].append(episode_reward)
        
        try:
            agent_info = agent.get_info()
            training_stats['cluster_evolution'].append(agent_info.get('dpmm', {}))
            training_stats['expert_counts'].append(agent_info.get('num_experts', 0))
        except:
            training_stats['cluster_evolution'].append({})
            training_stats['expert_counts'].append(0)
        
        # ì£¼ê¸°ì  ì¶œë ¥
        if episode % 20 == 0 and episode > 0:
            recent_rewards = training_stats['episode_rewards'][-10:]
            print(f"  ğŸ“Š Episode {episode} Summary:")
            print(f"    Recent avg reward: {np.mean(recent_rewards):.2f}")
            print(f"    Episode reward: {episode_reward:.2f}")
            print(f"    Steps: {step_count}")
            
            try:
                agent_info = agent.get_info()
                print(f"    Clusters: {agent_info.get('dpmm', {}).get('num_clusters', 'N/A')}")
                print(f"    Experts: {agent_info.get('num_experts', 'N/A')}")
            except:
                print(f"    Clusters: N/A, Experts: N/A")
    
    return training_stats

if __name__ == "__main__":
    main()