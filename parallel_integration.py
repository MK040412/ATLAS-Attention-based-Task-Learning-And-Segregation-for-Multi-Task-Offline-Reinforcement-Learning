def train_with_parallel_environments():
    """ë³‘ë ¬ í™˜ê²½ì„ í™œìš©í•œ í•™ìŠµ"""
    
    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (64ê°œ í™˜ê²½ ì‚¬ìš©)
    env = RotatingVectorizedWrapper(num_envs=64, rotation_frequency=20)
    
    # ì–¸ì–´ ì²˜ë¦¬ê¸°
    from language_processor import LanguageProcessor
    lang_processor = LanguageProcessor(device=CONFIG.device)
    
    # MoE ì—ì´ì „íŠ¸ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)
    from sb3_moe_agent import SB3MoEAgent
    
    # ì²« ë²ˆì§¸ ë¦¬ì…‹ìœ¼ë¡œ ì°¨ì› í™•ì¸
    state, _ = env.reset()
    instruction = "Push the blue cube to the goal position"
    instruction_emb = lang_processor.encode(instruction)
    
    agent = SB3MoEAgent(
        env=env,
        instr_dim=instruction_emb.shape[-1],
        latent_dim=32
    )
    
    print(f"ğŸš€ Parallel MoE Training Started:")
    print(f"  â€¢ Parallel environments: {env.num_envs}")
    print(f"  â€¢ State dimension: {state.shape}")
    print(f"  â€¢ Instruction dimension: {instruction_emb.shape}")
    
    # ë³‘ë ¬ ê²½í—˜ ìˆ˜ì§‘ê¸°
    parallel_collector = ParallelExperienceCollector(env, agent)
    
    # í•™ìŠµ ë£¨í”„
    for episode in range(100):
        print(f"\n--- Episode {episode + 1}/100 ---")
        
        # ğŸ”„ í™˜ê²½ ë¡œí…Œì´ì…˜ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
        if episode % 10 == 0:
            print(f"ğŸ”„ Using environment {env.active_env_idx}/{env.num_envs}")
        
        # ë‹¨ì¼ ì—í”¼ì†Œë“œ ì‹¤í–‰ (ì„ íƒëœ í™˜ê²½ì—ì„œ)
        episode_reward = 0
        step_count = 0
        max_steps = 500
        
        state, _ = env.reset()
        
        while step_count < max_steps:
            # í–‰ë™ ì„ íƒ
            action, cluster_id = agent.select_action(
                state, 
                instruction_emb.squeeze(0).detach().cpu().numpy(),
                deterministic=False
            )
            
            if step_count == 0:
                print(f"  ğŸ¯ Assigned to cluster/expert: {cluster_id}")
                print(f"  ğŸŒ Active environment: {env.active_env_idx}")
            
            # í™˜ê²½ ìŠ¤í… (ë‚´ë¶€ì ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬)
            next_state, reward, done, truncated, info = env.step(action)
            episode_reward += reward
            
            # ê²½í—˜ ì €ì¥
            agent.store_experience(state, action, reward, next_state, done, cluster_id)
            
            state = next_state
            step_count += 1
            
            if done or truncated:
                break
        
        # ğŸš€ ë³‘ë ¬ ë°°ì¹˜ ê²½í—˜ ìˆ˜ì§‘ (ì£¼ê¸°ì ìœ¼ë¡œ)
        if episode % 5 == 0 and episode > 0:
            print(f"  ğŸ”„ Collecting parallel batch experiences...")
            batch_experiences = parallel_collector.collect_batch(
                instruction_emb, 
                batch_size=32,  # 32ê°œ í™˜ê²½ì—ì„œ ë™ì‹œ ìˆ˜ì§‘
                steps_per_env=50
            )
            
            # ë°°ì¹˜ ê²½í—˜ì„ ê° ì „ë¬¸ê°€ì—ê²Œ ë¶„ë°°
            agent.store_batch_experiences(batch_experiences)
            print(f"  ğŸ“¦ Collected {len(batch_experiences)} parallel experiences")
        
        # ì „ë¬¸ê°€ ì—…ë°ì´íŠ¸
        if episode % 10 == 0 and episode > 0:
            update_results = agent.update_experts(batch_size=256)
            if update_results:
                updated_count = len([k for k, v in update_results.items() if v is not None])
                print(f"  ğŸ“ˆ Updated {updated_count} experts")
        
        # í´ëŸ¬ìŠ¤í„° ë³‘í•©
        if episode % 15 == 0 and episode > 0:
            merged = agent.merge_clusters(verbose=(episode % 30 == 0))
            if merged:
                print(f"  ğŸ”— Clusters merged")
        
        # í†µê³„ ì¶œë ¥
        if episode % 20 == 0 or episode == 99:
            agent_info = agent.get_info()
            print(f"  ğŸ“Š Episode {episode + 1} Summary:")
            print(f"    Episode reward: {episode_reward:.2f}")
            print(f"    Steps: {step_count}")
            print(f"    Clusters: {agent_info['dpmm']['num_clusters']}")
            print(f"    Experts: {agent_info['num_experts']}")
            print(f"    Total buffer size: {agent_info['total_buffer_size']}")
            print(f"    Parallel env utilization: {info.get('parallel_rewards_mean', 0):.2f}")
    
    env.close()
    return agent

class ParallelExperienceCollector:
    """ë³‘ë ¬ í™˜ê²½ì—ì„œ ê²½í—˜ ìˆ˜ì§‘"""
    
    def __init__(self, vectorized_env, agent):
        self.env = vectorized_env
        self.agent = agent
        self.num_envs = vectorized_env.num_envs
    
    def collect_batch(self, instruction_emb, batch_size: int = 32, steps_per_env: int = 50):
        """ë°°ì¹˜ë¡œ ë³‘ë ¬ ê²½í—˜ ìˆ˜ì§‘"""
        experiences = []
        
        # ì‚¬ìš©í•  í™˜ê²½ ì¸ë±ìŠ¤ë“¤ ì„ íƒ
        env_indices = np.random.choice(self.num_envs, size=min(batch_size, self.num_envs), replace=False)
        
        for env_idx in env_indices:
            # í™˜ê²½ ì „í™˜
            original_idx = self.env.active_env_idx
            self.env.active_env_idx = env_idx
            
            try:
                # ì§§ì€ ì—í”¼ì†Œë“œ ì‹¤í–‰
                state, _ = self.env.reset()
                
                for step in range(steps_per_env):
                    action, cluster_id = self.agent.select_action(
                        state, 
                        instruction_emb.squeeze(0).detach().cpu().numpy(),
                        deterministic=True  # íƒí—˜ë³´ë‹¤ëŠ” í™œìš©
                    )
                    
                    next_state, reward, done, truncated, info = self.env.step(action)
                    
                    experiences.append({
                        'state': state.copy(),
                        'action': action.copy(),
                        'reward': reward,
                        'next_state': next_state.copy(),
                        'done': done or truncated,
                        'cluster_id': cluster_id,
                        'env_idx': env_idx,
                        'instruction_emb': instruction_emb.squeeze(0).detach().cpu().numpy()
                    })
                    
                    state = next_state
                    
                    if done or truncated:
                        break
                        
            except Exception as e:
                print(f"âš ï¸ Parallel collection error in env {env_idx}: {e}")
            
            # ì›ë˜ í™˜ê²½ìœ¼ë¡œ ë³µì›
            self.env.active_env_idx = original_idx
        
        return experiences

# ğŸ”§ SB3MoEAgent í™•ì¥ (ë°°ì¹˜ ê²½í—˜ ì²˜ë¦¬)
class EnhancedSB3MoEAgent(SB3MoEAgent):
    """ë³‘ë ¬ ê²½í—˜ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í™•ì¥ëœ MoE ì—ì´ì „íŠ¸"""
    
    def store_batch_experiences(self, batch_experiences):
        """ë°°ì¹˜ ê²½í—˜ë“¤ì„ ê° ì „ë¬¸ê°€ì—ê²Œ ë¶„ë°°"""
        cluster_experiences = {}
        
        # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ê²½í—˜ ë¶„ë¥˜
        for exp in batch_experiences:
            cluster_id = exp['cluster_id']
            if cluster_id not in cluster_experiences:
                cluster_experiences[cluster_id] = []
            cluster_experiences[cluster_id].append(exp)
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì „ë¬¸ê°€ì—ê²Œ ê²½í—˜ ì €ì¥
        for cluster_id, experiences in cluster_experiences.items():
            if cluster_id in self.experts:
                expert = self.experts[cluster_id]
                
                for exp in experiences:
                    if hasattr(expert, 'replay_buffer'):
                        expert.replay_buffer.add(
                            exp['state'], exp['action'], exp['reward'],
                            exp['next_state'], exp['done']
                        )
        
        print(f"  ğŸ“¦ Distributed {len(batch_experiences)} experiences to {len(cluster_experiences)} clusters")
    
    def get_parallel_statistics(self):
        """ë³‘ë ¬ í™˜ê²½ ê´€ë ¨ í†µê³„"""
        base_stats = self.get_info()
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ê²½í—˜ ë¶„í¬
        cluster_experience_counts = {}
        for cluster_id, expert in self.experts.items():
            if hasattr(expert, 'replay_buffer'):
                cluster_experience_counts[cluster_id] = len(expert.replay_buffer)
        
        base_stats['parallel_stats'] = {
            'cluster_experience_distribution': cluster_experience_counts,
            'total_parallel_experiences': sum(cluster_experience_counts.values()),
            'experience_balance': np.std(list(cluster_experience_counts.values())) if cluster_experience_counts else 0
        }
        
        return base_stats

# ğŸ¯ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main_parallel_training():
    """ë³‘ë ¬ í™˜ê²½ì„ í™œìš©í•œ ë©”ì¸ í•™ìŠµ"""
    print("ğŸš€ Starting Parallel MoE Training with IsaacLab")
    
    try:
        # ë³‘ë ¬ í•™ìŠµ ì‹¤í–‰
        trained_agent = train_with_parallel_environments()
        
        # ìµœì¢… í†µê³„
        final_stats = trained_agent.get_parallel_statistics()
        print(f"\nğŸ‰ Parallel Training Completed!")
        print(f"  â€¢ Final clusters: {final_stats['dpmm']['num_clusters']}")
        print(f"  â€¢ Final experts: {final_stats['num_experts']}")
        print(f"  â€¢ Total experiences: {final_stats['parallel_stats']['total_parallel_experiences']}")
        print(f"  â€¢ Experience balance (std): {final_stats['parallel_stats']['experience_balance']:.2f}")
        
        return trained_agent
        
    except Exception as e:
        print(f"âŒ Parallel training failed: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main_parallel_training()