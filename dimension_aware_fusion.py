import torch
import torch.nn as nn
import numpy as np

class DimensionAwareFusion(nn.Module):
    """ì°¨ì›ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ìœµí•© ëª¨ë“ˆ"""
    
    def __init__(self, instr_dim, state_dim, latent_dim):
        super().__init__()
        self.instr_dim = instr_dim
        self.state_dim = state_dim
        self.latent_dim = latent_dim
        
        print(f"ğŸ”— DimensionAwareFusion init:")
        print(f"  Instruction dim: {instr_dim}")
        print(f"  State dim: {state_dim}")
        print(f"  Latent dim: {latent_dim}")
        
        # ì§€ì‹œì‚¬í•­ í”„ë¡œì ì…˜
        self.instr_proj = nn.Sequential(
            nn.Linear(instr_dim, latent_dim // 2),
            nn.ReLU(),
            nn.Linear(latent_dim // 2, latent_dim // 2)
        )
        
        # ìƒíƒœ í”„ë¡œì ì…˜
        self.state_proj = nn.Sequential(
            nn.Linear(state_dim, latent_dim // 2),
            nn.ReLU(),
            nn.Linear(latent_dim // 2, latent_dim // 2)
        )
        
        # ìœµí•© ë ˆì´ì–´
        self.fusion = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim)
        )
        
    def _safe_reshape(self, tensor, target_dim, name="tensor"):
        """í…ì„œë¥¼ ì•ˆì „í•˜ê²Œ ë¦¬ì…°ì´í”„"""
        try:
            if tensor.dim() == 1:
                # 1D í…ì„œì¸ ê²½ìš° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                tensor = tensor.unsqueeze(0)
            elif tensor.dim() == 3:
                # 3D í…ì„œì¸ ê²½ìš° ì‹œí€€ìŠ¤ ì°¨ì›ì—ì„œ í‰ê· 
                if tensor.size(0) == 1:
                    tensor = tensor.squeeze(0)  # (1, seq, dim) -> (seq, dim)
                tensor = tensor.mean(dim=0, keepdim=True)  # (seq, dim) -> (1, dim)
            elif tensor.dim() == 2:
                # 2D í…ì„œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                pass
            else:
                print(f"Warning: Unexpected {name} dimension: {tensor.shape}")
                tensor = tensor.view(1, -1)
            
            # ë§ˆì§€ë§‰ ì°¨ì›ì´ target_dimê³¼ ë§ëŠ”ì§€ í™•ì¸
            if tensor.size(-1) != target_dim:
                print(f"Warning: {name} last dim mismatch. Expected {target_dim}, got {tensor.size(-1)}")
                # ì ì‘ì  ì²˜ë¦¬
                if tensor.size(-1) > target_dim:
                    tensor = tensor[..., :target_dim]  # ì˜ë¼ë‚´ê¸°
                else:
                    # íŒ¨ë”©
                    pad_size = target_dim - tensor.size(-1)
                    padding = torch.zeros(*tensor.shape[:-1], pad_size, device=tensor.device)
                    tensor = torch.cat([tensor, padding], dim=-1)
            
            return tensor
            
        except Exception as e:
            print(f"Error in reshaping {name}: {e}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            return torch.zeros(1, target_dim, device=tensor.device if torch.is_tensor(tensor) else 'cpu')
    
    def forward(self, instr_emb, state_obs):
        """ìˆœì „íŒŒ"""
        try:
            # ì…ë ¥ ì°¨ì› í™•ì¸
            if torch.is_tensor(instr_emb):
                instr_tensor = instr_emb
            else:
                instr_tensor = torch.FloatTensor(instr_emb).to(next(self.parameters()).device)
            
            if torch.is_tensor(state_obs):
                state_tensor = state_obs
            else:
                state_tensor = torch.FloatTensor(state_obs).to(next(self.parameters()).device)
            
            # ì•ˆì „í•œ ë¦¬ì…°ì´í”„
            instr_tensor = self._safe_reshape(instr_tensor, self.instr_dim, "instruction")
            state_tensor = self._safe_reshape(state_tensor, self.state_dim, "state")
            
            # í”„ë¡œì ì…˜
            instr_proj = self.instr_proj(instr_tensor)  # (batch, latent_dim//2)
            state_proj = self.state_proj(state_tensor)  # (batch, latent_dim//2)
            
            # ì—°ê²°
            combined = torch.cat([instr_proj, state_proj], dim=-1)  # (batch, latent_dim)
            
            # ìœµí•©
            fused = self.fusion(combined)  # (batch, latent_dim)
            
            return fused.squeeze(0) if fused.size(0) == 1 else fused
            
        except Exception as e:
            print(f"Error in DimensionAwareFusion forward: {e}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            device = next(self.parameters()).device
            return torch.zeros(self.latent_dim, device=device)
    
    def save_checkpoint(self, filepath):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        try:
            torch.save({
                'state_dict': self.state_dict(),
                'instr_dim': self.instr_dim,
                'state_dim': self.state_dim,
                'latent_dim': self.latent_dim,
                'hidden_dim': self.hidden_dim
            }, filepath)
            print(f"âœ“ Fusion model saved to {filepath}")
        except Exception as e:
            print(f"Warning: Failed to save fusion model: {e}")
    
    def load_checkpoint(self, filepath):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(filepath, map_location=self.instr_proj[0].weight.device)
            
            # ì°¨ì› ê²€ì¦
            if (checkpoint['instr_dim'] != self.instr_dim or 
                checkpoint['state_dim'] != self.state_dim or
                checkpoint['latent_dim'] != self.latent_dim):
                raise ValueError("Dimension mismatch in checkpoint")
            
            self.load_state_dict(checkpoint['state_dict'])
            print(f"âœ“ Fusion model loaded from {filepath}")
        except Exception as e:
            print(f"Warning: Failed to load fusion model: {e}")

def create_dimension_aware_fusion(instr_dim, state_dim, latent_dim, hidden_dim=128):
    """ì°¨ì› ì¸ì‹ ìœµí•© ëª¨ë¸ ìƒì„±"""
    return DimensionAwareFusion(instr_dim, state_dim, latent_dim, hidden_dim)