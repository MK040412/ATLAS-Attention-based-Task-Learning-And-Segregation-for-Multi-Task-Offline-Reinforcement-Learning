import numpy as np
import torch

class ImprovedRewardSystem:
    """ê°œì„ ëœ ë³´ìƒ ì‹œìŠ¤í…œ - Dense Reward + Curriculum"""
    
    def __init__(self):
        # ë³´ìƒ ê°€ì¤‘ì¹˜
        self.w_distance = 1.0      # ëª©í‘œê¹Œì§€ ê±°ë¦¬
        self.w_approach = 0.5      # íë¸Œì— ì ‘ê·¼
        self.w_contact = 2.0       # íë¸Œì™€ ì ‘ì´‰
        self.w_push = 3.0          # íë¸Œ ë°€ê¸°
        self.w_goal = 10.0         # ëª©í‘œ ë‹¬ì„±
        self.w_efficiency = 0.1    # íš¨ìœ¨ì„± (ì—ë„ˆì§€ ì ˆì•½)
        
        # ì´ì „ ìƒíƒœ ì €ì¥
        self.prev_cube_to_goal_dist = None
        self.prev_ant_to_cube_dist = None
        self.contact_steps = 0
        self.total_steps = 0
        
    def reset(self):
        """ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œ ë¦¬ì…‹"""
        self.prev_cube_to_goal_dist = None
        self.prev_ant_to_cube_dist = None
        self.contact_steps = 0
        self.total_steps = 0
    
    def compute_reward(self, ant_pos, cube_pos, goal_pos, action, info=None):
        """ê°œì„ ëœ ë³´ìƒ ê³„ì‚°"""
        self.total_steps += 1
        
        # ê¸°ë³¸ ê±°ë¦¬ë“¤
        cube_to_goal_dist = np.linalg.norm(cube_pos - goal_pos)
        ant_to_cube_dist = np.linalg.norm(ant_pos - cube_pos)
        ant_to_goal_dist = np.linalg.norm(ant_pos - goal_pos)
        
        reward = 0.0
        reward_info = {}
        
        # 1. ëª©í‘œ ë‹¬ì„± ë³´ìƒ (ê°€ì¥ ì¤‘ìš”)
        if cube_to_goal_dist < 0.3:  # ëª©í‘œ ê·¼ì²˜
            goal_reward = self.w_goal * (1.0 - cube_to_goal_dist / 0.3)
            reward += goal_reward
            reward_info['goal_reward'] = goal_reward
        
        # 2. íë¸Œë¥¼ ëª©í‘œë¡œ ë°€ê¸° ë³´ìƒ (ì§„í–‰ ìƒí™© ê¸°ë°˜)
        if self.prev_cube_to_goal_dist is not None:
            progress = self.prev_cube_to_goal_dist - cube_to_goal_dist
            push_reward = self.w_push * progress * 10  # ì§„í–‰ë„ì— ë”°ë¥¸ ë³´ìƒ
            reward += push_reward
            reward_info['push_reward'] = push_reward
        
        # 3. íë¸Œì— ì ‘ê·¼ ë³´ìƒ
        if self.prev_ant_to_cube_dist is not None:
            approach_progress = self.prev_ant_to_cube_dist - ant_to_cube_dist
            approach_reward = self.w_approach * approach_progress * 5
            reward += approach_reward
            reward_info['approach_reward'] = approach_reward
        
        # 4. ì ‘ì´‰ ë³´ìƒ (íë¸Œ ê·¼ì²˜ì— ìˆì„ ë•Œ)
        if ant_to_cube_dist < 0.5:  # ì ‘ì´‰ ë²”ìœ„
            contact_reward = self.w_contact * (1.0 - ant_to_cube_dist / 0.5)
            reward += contact_reward
            reward_info['contact_reward'] = contact_reward
            self.contact_steps += 1
        
        # 5. ê±°ë¦¬ ê¸°ë°˜ ê¸°ë³¸ ë³´ìƒ (í•­ìƒ ì œê³µ)
        distance_reward = -self.w_distance * cube_to_goal_dist / 10.0
        reward += distance_reward
        reward_info['distance_reward'] = distance_reward
        
        # 6. íš¨ìœ¨ì„± ë³´ìƒ (ì—ë„ˆì§€ ì ˆì•½)
        action_penalty = -self.w_efficiency * np.sum(np.abs(action))
        reward += action_penalty
        reward_info['efficiency_reward'] = action_penalty
        
        # 7. íƒí—˜ ë³´ë„ˆìŠ¤ (ì´ˆê¸° ë‹¨ê³„)
        if self.total_steps < 100:
            exploration_bonus = 0.1 * (100 - self.total_steps) / 100
            reward += exploration_bonus
            reward_info['exploration_bonus'] = exploration_bonus
        
        # 8. ì‹œê°„ í˜ë„í‹° (ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë©´)
        if self.total_steps > 500:
            time_penalty = -0.01 * (self.total_steps - 500)
            reward += time_penalty
            reward_info['time_penalty'] = time_penalty
        
        # ì´ì „ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.prev_cube_to_goal_dist = cube_to_goal_dist
        self.prev_ant_to_cube_dist = ant_to_cube_dist
        
        # ì´ ë³´ìƒ ì •ë³´
        reward_info['total_reward'] = reward
        reward_info['cube_to_goal_dist'] = cube_to_goal_dist
        reward_info['ant_to_cube_dist'] = ant_to_cube_dist
        reward_info['contact_ratio'] = self.contact_steps / max(1, self.total_steps)
        
        return reward, reward_info

class CurriculumManager:
    """ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.current_level = 0
        self.success_threshold = 0.7  # 70% ì„±ê³µë¥ 
        self.min_episodes = 20        # ìµœì†Œ ì—í”¼ì†Œë“œ ìˆ˜
        self.episode_results = []
        
        # ì»¤ë¦¬í˜ëŸ¼ ë ˆë²¨ ì •ì˜
        self.curriculum_levels = [
            {
                'name': 'Level 1: Close Push',
                'cube_position': [1.5, 0.0, 0.1],
                'goal_position': [2.0, 0.0, 0.1],
                'success_distance': 0.3,
                'max_steps': 300,
                'description': 'Push cube a short distance'
            },
            {
                'name': 'Level 2: Medium Push', 
                'cube_position': [2.0, 0.0, 0.1],
                'goal_position': [3.0, 0.0, 0.1],
                'success_distance': 0.3,
                'max_steps': 500,
                'description': 'Push cube medium distance'
            },
            {
                'name': 'Level 3: Long Push',
                'cube_position': [1.0, 0.0, 0.1],
                'goal_position': [4.0, 0.0, 0.1], 
                'success_distance': 0.3,
                'max_steps': 800,
                'description': 'Push cube long distance'
            },
            {
                'name': 'Level 4: Diagonal Push',
                'cube_position': [1.5, 1.0, 0.1],
                'goal_position': [2.5, 2.0, 0.1],
                'success_distance': 0.3,
                'max_steps': 600,
                'description': 'Push cube diagonally'
            },
            {
                'name': 'Level 5: Complex Navigation',
                'cube_position': [1.0, 1.5, 0.1],
                'goal_position': [4.0, -1.5, 0.1],
                'success_distance': 0.3,
                'max_steps': 1000,
                'description': 'Complex push with navigation'
            }
        ]
    
    def get_current_level(self):
        """í˜„ì¬ ë ˆë²¨ ì •ë³´ ë°˜í™˜"""
        if self.current_level < len(self.curriculum_levels):
            return self.curriculum_levels[self.current_level]
        else:
            # ë§ˆì§€ë§‰ ë ˆë²¨ ë°˜ë³µ
            return self.curriculum_levels[-1]
    
    def add_episode_result(self, success, final_distance, steps, total_reward):
        """ì—í”¼ì†Œë“œ ê²°ê³¼ ì¶”ê°€"""
        result = {
            'success': success,
            'final_distance': final_distance,
            'steps': steps,
            'total_reward': total_reward,
            'level': self.current_level
        }
        self.episode_results.append(result)
        
        # ìµœê·¼ ê²°ê³¼ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if len(self.episode_results) > 100:
            self.episode_results = self.episode_results[-100:]
    
    def should_advance_level(self):
        """ë‹¤ìŒ ë ˆë²¨ë¡œ ì§„í–‰í• ì§€ ê²°ì •"""
        if len(self.episode_results) < self.min_episodes:
            return False
        
        # í˜„ì¬ ë ˆë²¨ì˜ ìµœê·¼ ê²°ê³¼ë“¤ë§Œ í™•ì¸
        current_level_results = [r for r in self.episode_results[-self.min_episodes:] 
                               if r['level'] == self.current_level]
        
        if len(current_level_results) < self.min_episodes:
            return False
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rate = sum(r['success'] for r in current_level_results) / len(current_level_results)
        
        return success_rate >= self.success_threshold
    
    def advance_level(self):
        """ë‹¤ìŒ ë ˆë²¨ë¡œ ì§„í–‰"""
        if self.current_level < len(self.curriculum_levels) - 1:
            self.current_level += 1
            print(f"ğŸ“ Advanced to {self.curriculum_levels[self.current_level]['name']}")
            return True
        else:
            print("ğŸ† Curriculum completed! Continuing with final level.")
            return False
    
    def get_statistics(self):
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        if not self.episode_results:
            return {}
        
        recent_results = self.episode_results[-20:]  # ìµœê·¼ 20ê°œ ì—í”¼ì†Œë“œ
        current_level_results = [r for r in recent_results if r['level'] == self.current_level]
        
        stats = {
            'current_level': self.current_level,
            'level_name': self.get_current_level()['name'],
            'total_episodes': len(self.episode_results),
            'recent_success_rate': sum(r['success'] for r in recent_results) / max(1, len(recent_results)),
            'current_level_episodes': len(current_level_results),
            'current_level_success_rate': sum(r['success'] for r in current_level_results) / max(1, len(current_level_results)),
            'average_steps': np.mean([r['steps'] for r in recent_results]) if recent_results else 0,
            'average_reward': np.mean([r['total_reward'] for r in recent_results]) if recent_results else 0
        }
        
        return stats